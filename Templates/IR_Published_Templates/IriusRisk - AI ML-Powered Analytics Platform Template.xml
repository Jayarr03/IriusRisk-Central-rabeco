<?xml version="1.0" encoding="UTF-8"?>
<template ref="iriusrisk-ai-ml-powered-analytics-platform-template" name="IriusRisk - AI ML-Powered Analytics Platform Template" tags="" modelUpdated="2024-04-17 10:57:33">
  <desc>Incorporates machine learning models for predictive analytics, data processing, and decision-making, necessitating secure data handling, model training, and inference processes to protect sensitive information and ensure the integrity of the AI/ML pipeline.</desc>
  <diagram draft="false">
    <schema><mxGraphModel dx="2346" dy="1172" grid="0" gridSize="10" guides="1" tooltips="1" connect="1" arrows="1" fold="1" page="0" pageScale="1" pageWidth="850" pageHeight="1100" math="0" shadow="0" irDrawioVersion="6.4.0" irIriusRiskVersion="4.27.2"><root><mxCell id="0"/><mxCell id="1" parent="0"/><mxCell id="2" value="Private Secured" style="ir.ref=2ab4effa-40b7-4cd2-ba81-8247d29a6f2d;rounded=1;whiteSpace=wrap;recursiveResize=0;html=1;verticalAlign=top;align=left;dashed=1;strokeWidth=1;arcSize=3;absoluteArcSize=1;spacingTop=1;spacingLeft=32;strokeColor=#7575EB;fillColor=#F0F0FF;fillOpacity=30;fontColor=#5651E0;connectable=0;container=1;source=iriusrisk;ir.type=TRUSTZONE;" parent="1" vertex="1" isThumb="0"><mxGeometry x="-260" y="176" width="863" height="224" as="geometry"/></mxCell><mxCell id="3" value="Trained Model" style="ir.ref=09f3011d-d0ef-4690-8b9a-dca5132e3d21;rounded=1;whiteSpace=wrap;html=1;align=center;imageAlign=center;imageWidth=82;imageHeight=82;arcSize=90;absoluteArcSize=1;strokeColor=#20C9E3;fillColor=#E3FCFC;fontColor=#0084AD;fontSize=12;source=iriusrisk;ir.type=COMPONENT;ir.componentDefinition.ref=CD-ML-AI-TRAINED-MODEL;image=data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 1 1" height="1" width="1">&#xa;<mask height="1" width="1" y="0" x="0" maskUnits="userSpaceOnUse" style="mask-type:alpha" id="mask0_3051_23740">&#xa;<circle fill="black" r="0.5" cy="0.5" cx="0.5"/>&#xa;</mask>&#xa;<g mask="url(#mask0_3051_23740)">&#xa;<path fill="url(#paint0_linear_3051_23740)" d="M1 0H0V1H1V0Z"/>&#xa;<path fill="#E0FBFF" d="M0.700625 0.245403C0.715941 0.245403 0.728355 0.232988 0.728355 0.217672C0.728355 0.202357 0.715941 0.189941 0.700625 0.189941C0.688005 0.189941 0.677353 0.198372 0.673995 0.209908H0.62186C0.605319 0.209908 0.59191 0.223316 0.59191 0.239857V0.395151C0.59191 0.403115 0.585454 0.409572 0.57749 0.409572H0.506504V0.293133C0.506545 0.252037 0.47781 0.223281 0.436569 0.223219H0.436444C0.405795 0.223219 0.380451 0.239012 0.369049 0.264834H0.359061H0.358874C0.332428 0.264834 0.307313 0.273677 0.28817 0.289783C0.267861 0.306845 0.256667 0.330378 0.256625 0.355992C0.256605 0.376696 0.267571 0.399626 0.277953 0.414066C0.231698 0.430088 0.200258 0.468852 0.200195 0.511612C0.200133 0.554184 0.231552 0.59297 0.277891 0.609138C0.267363 0.623536 0.256168 0.646341 0.256147 0.666691C0.256085 0.71717 0.302236 0.758307 0.359082 0.75841H0.36907C0.379514 0.782222 0.402012 0.797535 0.429679 0.799748V0.819992C0.429679 0.836533 0.443088 0.849942 0.459628 0.849942H0.575277C0.591818 0.849942 0.605227 0.836533 0.605227 0.819992V0.664698C0.605227 0.656734 0.611683 0.650278 0.619647 0.650278H0.673996C0.677355 0.661814 0.688006 0.670243 0.700625 0.670243C0.715941 0.670243 0.728355 0.657828 0.728355 0.642512C0.728355 0.627197 0.715941 0.614781 0.700625 0.614781C0.688005 0.614781 0.677353 0.623212 0.673995 0.634748H0.619647C0.603107 0.634748 0.589698 0.648157 0.589698 0.664698V0.819992C0.589698 0.827956 0.583242 0.834413 0.575277 0.834413H0.459628C0.451664 0.834413 0.445208 0.827956 0.445208 0.819992V0.799603C0.481653 0.79591 0.506446 0.768387 0.506504 0.730112V0.589259C0.525607 0.588929 0.542899 0.581171 0.555616 0.568748H0.633743C0.636917 0.580568 0.647707 0.58927 0.660528 0.58927C0.675844 0.58927 0.688259 0.576855 0.688259 0.561539C0.688259 0.546224 0.675844 0.533808 0.660528 0.533808C0.647908 0.533808 0.637257 0.542238 0.633899 0.553774H0.567368C0.573703 0.543044 0.577337 0.530531 0.577337 0.517169L0.577234 0.513287H0.733575C0.736749 0.525106 0.747539 0.533808 0.76036 0.533808C0.775676 0.533808 0.788091 0.521393 0.788091 0.506077C0.788091 0.490762 0.775676 0.478346 0.76036 0.478346C0.74774 0.478346 0.737088 0.486776 0.73373 0.498312H0.574846C0.573144 0.492013 0.570609 0.486055 0.567368 0.480565H0.57789H0.637234C0.653775 0.480565 0.667184 0.467155 0.667184 0.450615V0.364094C0.667184 0.356131 0.67364 0.349674 0.681604 0.349674H0.736698C0.740706 0.360057 0.750782 0.367422 0.762578 0.367422C0.777894 0.367422 0.790309 0.355007 0.790309 0.339691C0.790309 0.324376 0.777894 0.31196 0.762578 0.31196C0.749163 0.31196 0.737971 0.321487 0.735402 0.334145H0.681604C0.665063 0.334145 0.651654 0.347553 0.651654 0.364094V0.450615C0.651654 0.458579 0.645198 0.465035 0.637234 0.465035H0.555042C0.542382 0.452937 0.525321 0.445403 0.506504 0.445079V0.425101H0.57749C0.59403 0.425101 0.60744 0.411692 0.60744 0.395151V0.239857C0.60744 0.231894 0.613896 0.225437 0.62186 0.225437H0.673995C0.677353 0.236974 0.688005 0.245403 0.700625 0.245403ZM0.433135 0.517169C0.433135 0.506737 0.435351 0.496822 0.439337 0.48787H0.394955H0.394934C0.386611 0.48787 0.381659 0.492865 0.381638 0.501208V0.650898C0.393602 0.654206 0.402445 0.665068 0.402445 0.678073C0.402445 0.693678 0.389752 0.706371 0.374147 0.706371C0.358542 0.706371 0.345848 0.693678 0.345848 0.678073C0.345848 0.665068 0.354692 0.654206 0.366656 0.650898V0.529507H0.318071C0.314783 0.541451 0.303921 0.550314 0.290917 0.550314C0.275311 0.550314 0.262618 0.537621 0.262618 0.522016C0.262618 0.50641 0.275311 0.493717 0.290917 0.493717C0.303901 0.493717 0.314762 0.502561 0.318071 0.514525H0.366656V0.501208C0.366677 0.484541 0.378308 0.47291 0.394913 0.47291H0.394955H0.429079H0.44406H0.448314C0.458829 0.459406 0.474044 0.449741 0.491502 0.446375V0.425101H0.4588C0.44226 0.425101 0.428851 0.411692 0.428851 0.395151V0.354585H0.429079V0.341094C0.417114 0.337786 0.408271 0.326944 0.408271 0.31394C0.408271 0.298334 0.420964 0.285642 0.436569 0.285642C0.452175 0.285642 0.464868 0.298334 0.464868 0.31394C0.464868 0.326924 0.456024 0.337786 0.44406 0.341094V0.354585H0.444381V0.395151C0.444381 0.403115 0.450837 0.409572 0.4588 0.409572H0.491502V0.293133C0.491564 0.260319 0.469487 0.238263 0.436549 0.2382H0.436444C0.409998 0.2382 0.389358 0.251892 0.381201 0.274842C0.380139 0.277818 0.37731 0.279816 0.374147 0.279816H0.359082H0.358915C0.336298 0.279816 0.314034 0.287619 0.297825 0.301248C0.280929 0.315438 0.271628 0.334872 0.271587 0.356013C0.271566 0.375301 0.284362 0.39923 0.294288 0.410487H0.346993C0.350302 0.398522 0.361142 0.38968 0.374147 0.38968C0.389752 0.38968 0.402445 0.402351 0.402445 0.417957C0.402445 0.433562 0.389752 0.446256 0.374147 0.446256C0.361163 0.446256 0.350302 0.437412 0.346993 0.425447H0.292061C0.246846 0.437266 0.21524 0.472577 0.215177 0.511612C0.215115 0.550689 0.247033 0.586207 0.29279 0.597984C0.29539 0.59865 0.297429 0.600668 0.298137 0.60327C0.298844 0.60587 0.298116 0.608659 0.296202 0.610552C0.285964 0.620748 0.27115 0.646611 0.271108 0.666712C0.271066 0.714007 0.316718 0.743346 0.359082 0.743408H0.374147C0.377331 0.743408 0.38016 0.745406 0.381222 0.748403C0.388483 0.769064 0.406128 0.782131 0.429079 0.784503V0.657339C0.417114 0.654031 0.408271 0.643169 0.408271 0.630165C0.408271 0.614559 0.420964 0.601867 0.436569 0.601867C0.452175 0.601867 0.464868 0.614559 0.464868 0.630165C0.464868 0.643169 0.456024 0.654031 0.44406 0.657339V0.784358C0.472609 0.781237 0.491439 0.760345 0.491502 0.73005V0.587963C0.458251 0.581551 0.433135 0.552293 0.433135 0.517169ZM0.374147 0.664756C0.381492 0.664756 0.387464 0.670728 0.387464 0.678073C0.387464 0.685418 0.381492 0.691389 0.374147 0.691389C0.366801 0.691389 0.36083 0.685418 0.36083 0.678073C0.36083 0.670728 0.366801 0.664756 0.374147 0.664756ZM0.36083 0.417978C0.36083 0.410633 0.366801 0.404661 0.374147 0.404661C0.381492 0.404661 0.387464 0.410633 0.387464 0.417978C0.387464 0.425323 0.381492 0.431294 0.374147 0.431294C0.366801 0.431294 0.36083 0.425323 0.36083 0.417978ZM0.505236 0.467254C0.477668 0.467254 0.455319 0.489602 0.455319 0.51717C0.455319 0.544738 0.477668 0.567085 0.505236 0.567085C0.532804 0.567085 0.555151 0.544738 0.555151 0.51717C0.555151 0.489602 0.532804 0.467254 0.505236 0.467254Z" clip-rule="evenodd" fill-rule="evenodd"/>&#xa;<rect fill="#E0FBFF" rx="0.035" height="0.07" width="0.07" y="0.47998" x="0.469727"/>&#xa;</g>&#xa;<defs>&#xa;<linearGradient gradientUnits="userSpaceOnUse" y2="-0.207067" x2="1.20707" y1="1.2072" x1="-0.2072" id="paint0_linear_3051_23740">&#xa;<stop stop-color="#087997"/>&#xa;<stop stop-color="#0BA0B7" offset="1"/>&#xa;</linearGradient>&#xa;</defs>&#xa;</svg>;verticalLabelPosition=bottom;verticalAlign=top;ir.synchronized=1;strokeWidth=1;" parent="2" vertex="1" isThumb="0"><mxGeometry x="73" y="53" width="90" height="90" as="geometry"/></mxCell><mxCell id="4" value="Deploy an Artificial Intelligence model" style="ir.ref=d7a72b37-4a1d-454d-b209-0698e506c1e2;rounded=1;whiteSpace=wrap;html=1;align=center;imageAlign=center;imageWidth=82;imageHeight=82;arcSize=90;absoluteArcSize=1;strokeColor=#20C9E3;fillColor=#E3FCFC;fontColor=#0084AD;fontSize=12;source=iriusrisk;ir.type=COMPONENT;ir.componentDefinition.ref=CD-ML-AI-MODEL-DEPLOY;image=data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 1 1" height="1" width="1">&#xa;<mask height="1" width="1" y="0" x="0" maskUnits="userSpaceOnUse" style="mask-type:alpha" id="mask0_3051_23740">&#xa;<circle fill="black" r="0.5" cy="0.5" cx="0.5"/>&#xa;</mask>&#xa;<g mask="url(#mask0_3051_23740)">&#xa;<path fill="url(#paint0_linear_3051_23740)" d="M1 0H0V1H1V0Z"/>&#xa;<path fill="#E0FBFF" d="M0.700625 0.245403C0.715941 0.245403 0.728355 0.232988 0.728355 0.217672C0.728355 0.202357 0.715941 0.189941 0.700625 0.189941C0.688005 0.189941 0.677353 0.198372 0.673995 0.209908H0.62186C0.605319 0.209908 0.59191 0.223316 0.59191 0.239857V0.395151C0.59191 0.403115 0.585454 0.409572 0.57749 0.409572H0.506504V0.293133C0.506545 0.252037 0.47781 0.223281 0.436569 0.223219H0.436444C0.405795 0.223219 0.380451 0.239012 0.369049 0.264834H0.359061H0.358874C0.332428 0.264834 0.307313 0.273677 0.28817 0.289783C0.267861 0.306845 0.256667 0.330378 0.256625 0.355992C0.256605 0.376696 0.267571 0.399626 0.277953 0.414066C0.231698 0.430088 0.200258 0.468852 0.200195 0.511612C0.200133 0.554184 0.231552 0.59297 0.277891 0.609138C0.267363 0.623536 0.256168 0.646341 0.256147 0.666691C0.256085 0.71717 0.302236 0.758307 0.359082 0.75841H0.36907C0.379514 0.782222 0.402012 0.797535 0.429679 0.799748V0.819992C0.429679 0.836533 0.443088 0.849942 0.459628 0.849942H0.575277C0.591818 0.849942 0.605227 0.836533 0.605227 0.819992V0.664698C0.605227 0.656734 0.611683 0.650278 0.619647 0.650278H0.673996C0.677355 0.661814 0.688006 0.670243 0.700625 0.670243C0.715941 0.670243 0.728355 0.657828 0.728355 0.642512C0.728355 0.627197 0.715941 0.614781 0.700625 0.614781C0.688005 0.614781 0.677353 0.623212 0.673995 0.634748H0.619647C0.603107 0.634748 0.589698 0.648157 0.589698 0.664698V0.819992C0.589698 0.827956 0.583242 0.834413 0.575277 0.834413H0.459628C0.451664 0.834413 0.445208 0.827956 0.445208 0.819992V0.799603C0.481653 0.79591 0.506446 0.768387 0.506504 0.730112V0.589259C0.525607 0.588929 0.542899 0.581171 0.555616 0.568748H0.633743C0.636917 0.580568 0.647707 0.58927 0.660528 0.58927C0.675844 0.58927 0.688259 0.576855 0.688259 0.561539C0.688259 0.546224 0.675844 0.533808 0.660528 0.533808C0.647908 0.533808 0.637257 0.542238 0.633899 0.553774H0.567368C0.573703 0.543044 0.577337 0.530531 0.577337 0.517169L0.577234 0.513287H0.733575C0.736749 0.525106 0.747539 0.533808 0.76036 0.533808C0.775676 0.533808 0.788091 0.521393 0.788091 0.506077C0.788091 0.490762 0.775676 0.478346 0.76036 0.478346C0.74774 0.478346 0.737088 0.486776 0.73373 0.498312H0.574846C0.573144 0.492013 0.570609 0.486055 0.567368 0.480565H0.57789H0.637234C0.653775 0.480565 0.667184 0.467155 0.667184 0.450615V0.364094C0.667184 0.356131 0.67364 0.349674 0.681604 0.349674H0.736698C0.740706 0.360057 0.750782 0.367422 0.762578 0.367422C0.777894 0.367422 0.790309 0.355007 0.790309 0.339691C0.790309 0.324376 0.777894 0.31196 0.762578 0.31196C0.749163 0.31196 0.737971 0.321487 0.735402 0.334145H0.681604C0.665063 0.334145 0.651654 0.347553 0.651654 0.364094V0.450615C0.651654 0.458579 0.645198 0.465035 0.637234 0.465035H0.555042C0.542382 0.452937 0.525321 0.445403 0.506504 0.445079V0.425101H0.57749C0.59403 0.425101 0.60744 0.411692 0.60744 0.395151V0.239857C0.60744 0.231894 0.613896 0.225437 0.62186 0.225437H0.673995C0.677353 0.236974 0.688005 0.245403 0.700625 0.245403ZM0.433135 0.517169C0.433135 0.506737 0.435351 0.496822 0.439337 0.48787H0.394955H0.394934C0.386611 0.48787 0.381659 0.492865 0.381638 0.501208V0.650898C0.393602 0.654206 0.402445 0.665068 0.402445 0.678073C0.402445 0.693678 0.389752 0.706371 0.374147 0.706371C0.358542 0.706371 0.345848 0.693678 0.345848 0.678073C0.345848 0.665068 0.354692 0.654206 0.366656 0.650898V0.529507H0.318071C0.314783 0.541451 0.303921 0.550314 0.290917 0.550314C0.275311 0.550314 0.262618 0.537621 0.262618 0.522016C0.262618 0.50641 0.275311 0.493717 0.290917 0.493717C0.303901 0.493717 0.314762 0.502561 0.318071 0.514525H0.366656V0.501208C0.366677 0.484541 0.378308 0.47291 0.394913 0.47291H0.394955H0.429079H0.44406H0.448314C0.458829 0.459406 0.474044 0.449741 0.491502 0.446375V0.425101H0.4588C0.44226 0.425101 0.428851 0.411692 0.428851 0.395151V0.354585H0.429079V0.341094C0.417114 0.337786 0.408271 0.326944 0.408271 0.31394C0.408271 0.298334 0.420964 0.285642 0.436569 0.285642C0.452175 0.285642 0.464868 0.298334 0.464868 0.31394C0.464868 0.326924 0.456024 0.337786 0.44406 0.341094V0.354585H0.444381V0.395151C0.444381 0.403115 0.450837 0.409572 0.4588 0.409572H0.491502V0.293133C0.491564 0.260319 0.469487 0.238263 0.436549 0.2382H0.436444C0.409998 0.2382 0.389358 0.251892 0.381201 0.274842C0.380139 0.277818 0.37731 0.279816 0.374147 0.279816H0.359082H0.358915C0.336298 0.279816 0.314034 0.287619 0.297825 0.301248C0.280929 0.315438 0.271628 0.334872 0.271587 0.356013C0.271566 0.375301 0.284362 0.39923 0.294288 0.410487H0.346993C0.350302 0.398522 0.361142 0.38968 0.374147 0.38968C0.389752 0.38968 0.402445 0.402351 0.402445 0.417957C0.402445 0.433562 0.389752 0.446256 0.374147 0.446256C0.361163 0.446256 0.350302 0.437412 0.346993 0.425447H0.292061C0.246846 0.437266 0.21524 0.472577 0.215177 0.511612C0.215115 0.550689 0.247033 0.586207 0.29279 0.597984C0.29539 0.59865 0.297429 0.600668 0.298137 0.60327C0.298844 0.60587 0.298116 0.608659 0.296202 0.610552C0.285964 0.620748 0.27115 0.646611 0.271108 0.666712C0.271066 0.714007 0.316718 0.743346 0.359082 0.743408H0.374147C0.377331 0.743408 0.38016 0.745406 0.381222 0.748403C0.388483 0.769064 0.406128 0.782131 0.429079 0.784503V0.657339C0.417114 0.654031 0.408271 0.643169 0.408271 0.630165C0.408271 0.614559 0.420964 0.601867 0.436569 0.601867C0.452175 0.601867 0.464868 0.614559 0.464868 0.630165C0.464868 0.643169 0.456024 0.654031 0.44406 0.657339V0.784358C0.472609 0.781237 0.491439 0.760345 0.491502 0.73005V0.587963C0.458251 0.581551 0.433135 0.552293 0.433135 0.517169ZM0.374147 0.664756C0.381492 0.664756 0.387464 0.670728 0.387464 0.678073C0.387464 0.685418 0.381492 0.691389 0.374147 0.691389C0.366801 0.691389 0.36083 0.685418 0.36083 0.678073C0.36083 0.670728 0.366801 0.664756 0.374147 0.664756ZM0.36083 0.417978C0.36083 0.410633 0.366801 0.404661 0.374147 0.404661C0.381492 0.404661 0.387464 0.410633 0.387464 0.417978C0.387464 0.425323 0.381492 0.431294 0.374147 0.431294C0.366801 0.431294 0.36083 0.425323 0.36083 0.417978ZM0.505236 0.467254C0.477668 0.467254 0.455319 0.489602 0.455319 0.51717C0.455319 0.544738 0.477668 0.567085 0.505236 0.567085C0.532804 0.567085 0.555151 0.544738 0.555151 0.51717C0.555151 0.489602 0.532804 0.467254 0.505236 0.467254Z" clip-rule="evenodd" fill-rule="evenodd"/>&#xa;<rect fill="#E0FBFF" rx="0.035" height="0.07" width="0.07" y="0.47998" x="0.469727"/>&#xa;</g>&#xa;<defs>&#xa;<linearGradient gradientUnits="userSpaceOnUse" y2="-0.207067" x2="1.20707" y1="1.2072" x1="-0.2072" id="paint0_linear_3051_23740">&#xa;<stop stop-color="#087997"/>&#xa;<stop stop-color="#0BA0B7" offset="1"/>&#xa;</linearGradient>&#xa;</defs>&#xa;</svg>;verticalLabelPosition=bottom;verticalAlign=top;ir.synchronized=1;strokeWidth=1;" parent="2" vertex="1" isThumb="0"><mxGeometry x="266" y="53" width="90" height="90" as="geometry"/></mxCell><mxCell id="5" value="Deploy an Artificial Intelligence model -> Trained Model" style="ir.ref=316e5bcb-4575-458f-adb3-080f44cf40f4;edgeStyle=none;curved=1;html=1;ir.synchronized=1;strokeColor=#BFBFBF;strokeWidth=2;endArrow=open;endFill=0;endSize=4;sourcePerimeterSpacing=9;" parent="2" source="4" target="3" edge="1"><mxGeometry relative="1" as="geometry"/></mxCell><mxCell id="6" value="Build an Artificial Intelligence model" style="ir.ref=f443aa9e-03ce-4dd4-b970-1c42c722c16c;rounded=1;whiteSpace=wrap;html=1;align=center;imageAlign=center;imageWidth=82;imageHeight=82;arcSize=90;absoluteArcSize=1;strokeColor=#20C9E3;fillColor=#E3FCFC;fontColor=#0084AD;fontSize=12;source=iriusrisk;ir.type=COMPONENT;ir.componentDefinition.ref=CD-ML-AI-MODEL-BUILD;image=data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 1 1" height="1" width="1">&#xa;<mask height="1" width="1" y="0" x="0" maskUnits="userSpaceOnUse" style="mask-type:alpha" id="mask0_3051_23740">&#xa;<circle fill="black" r="0.5" cy="0.5" cx="0.5"/>&#xa;</mask>&#xa;<g mask="url(#mask0_3051_23740)">&#xa;<path fill="url(#paint0_linear_3051_23740)" d="M1 0H0V1H1V0Z"/>&#xa;<path fill="#E0FBFF" d="M0.700625 0.245403C0.715941 0.245403 0.728355 0.232988 0.728355 0.217672C0.728355 0.202357 0.715941 0.189941 0.700625 0.189941C0.688005 0.189941 0.677353 0.198372 0.673995 0.209908H0.62186C0.605319 0.209908 0.59191 0.223316 0.59191 0.239857V0.395151C0.59191 0.403115 0.585454 0.409572 0.57749 0.409572H0.506504V0.293133C0.506545 0.252037 0.47781 0.223281 0.436569 0.223219H0.436444C0.405795 0.223219 0.380451 0.239012 0.369049 0.264834H0.359061H0.358874C0.332428 0.264834 0.307313 0.273677 0.28817 0.289783C0.267861 0.306845 0.256667 0.330378 0.256625 0.355992C0.256605 0.376696 0.267571 0.399626 0.277953 0.414066C0.231698 0.430088 0.200258 0.468852 0.200195 0.511612C0.200133 0.554184 0.231552 0.59297 0.277891 0.609138C0.267363 0.623536 0.256168 0.646341 0.256147 0.666691C0.256085 0.71717 0.302236 0.758307 0.359082 0.75841H0.36907C0.379514 0.782222 0.402012 0.797535 0.429679 0.799748V0.819992C0.429679 0.836533 0.443088 0.849942 0.459628 0.849942H0.575277C0.591818 0.849942 0.605227 0.836533 0.605227 0.819992V0.664698C0.605227 0.656734 0.611683 0.650278 0.619647 0.650278H0.673996C0.677355 0.661814 0.688006 0.670243 0.700625 0.670243C0.715941 0.670243 0.728355 0.657828 0.728355 0.642512C0.728355 0.627197 0.715941 0.614781 0.700625 0.614781C0.688005 0.614781 0.677353 0.623212 0.673995 0.634748H0.619647C0.603107 0.634748 0.589698 0.648157 0.589698 0.664698V0.819992C0.589698 0.827956 0.583242 0.834413 0.575277 0.834413H0.459628C0.451664 0.834413 0.445208 0.827956 0.445208 0.819992V0.799603C0.481653 0.79591 0.506446 0.768387 0.506504 0.730112V0.589259C0.525607 0.588929 0.542899 0.581171 0.555616 0.568748H0.633743C0.636917 0.580568 0.647707 0.58927 0.660528 0.58927C0.675844 0.58927 0.688259 0.576855 0.688259 0.561539C0.688259 0.546224 0.675844 0.533808 0.660528 0.533808C0.647908 0.533808 0.637257 0.542238 0.633899 0.553774H0.567368C0.573703 0.543044 0.577337 0.530531 0.577337 0.517169L0.577234 0.513287H0.733575C0.736749 0.525106 0.747539 0.533808 0.76036 0.533808C0.775676 0.533808 0.788091 0.521393 0.788091 0.506077C0.788091 0.490762 0.775676 0.478346 0.76036 0.478346C0.74774 0.478346 0.737088 0.486776 0.73373 0.498312H0.574846C0.573144 0.492013 0.570609 0.486055 0.567368 0.480565H0.57789H0.637234C0.653775 0.480565 0.667184 0.467155 0.667184 0.450615V0.364094C0.667184 0.356131 0.67364 0.349674 0.681604 0.349674H0.736698C0.740706 0.360057 0.750782 0.367422 0.762578 0.367422C0.777894 0.367422 0.790309 0.355007 0.790309 0.339691C0.790309 0.324376 0.777894 0.31196 0.762578 0.31196C0.749163 0.31196 0.737971 0.321487 0.735402 0.334145H0.681604C0.665063 0.334145 0.651654 0.347553 0.651654 0.364094V0.450615C0.651654 0.458579 0.645198 0.465035 0.637234 0.465035H0.555042C0.542382 0.452937 0.525321 0.445403 0.506504 0.445079V0.425101H0.57749C0.59403 0.425101 0.60744 0.411692 0.60744 0.395151V0.239857C0.60744 0.231894 0.613896 0.225437 0.62186 0.225437H0.673995C0.677353 0.236974 0.688005 0.245403 0.700625 0.245403ZM0.433135 0.517169C0.433135 0.506737 0.435351 0.496822 0.439337 0.48787H0.394955H0.394934C0.386611 0.48787 0.381659 0.492865 0.381638 0.501208V0.650898C0.393602 0.654206 0.402445 0.665068 0.402445 0.678073C0.402445 0.693678 0.389752 0.706371 0.374147 0.706371C0.358542 0.706371 0.345848 0.693678 0.345848 0.678073C0.345848 0.665068 0.354692 0.654206 0.366656 0.650898V0.529507H0.318071C0.314783 0.541451 0.303921 0.550314 0.290917 0.550314C0.275311 0.550314 0.262618 0.537621 0.262618 0.522016C0.262618 0.50641 0.275311 0.493717 0.290917 0.493717C0.303901 0.493717 0.314762 0.502561 0.318071 0.514525H0.366656V0.501208C0.366677 0.484541 0.378308 0.47291 0.394913 0.47291H0.394955H0.429079H0.44406H0.448314C0.458829 0.459406 0.474044 0.449741 0.491502 0.446375V0.425101H0.4588C0.44226 0.425101 0.428851 0.411692 0.428851 0.395151V0.354585H0.429079V0.341094C0.417114 0.337786 0.408271 0.326944 0.408271 0.31394C0.408271 0.298334 0.420964 0.285642 0.436569 0.285642C0.452175 0.285642 0.464868 0.298334 0.464868 0.31394C0.464868 0.326924 0.456024 0.337786 0.44406 0.341094V0.354585H0.444381V0.395151C0.444381 0.403115 0.450837 0.409572 0.4588 0.409572H0.491502V0.293133C0.491564 0.260319 0.469487 0.238263 0.436549 0.2382H0.436444C0.409998 0.2382 0.389358 0.251892 0.381201 0.274842C0.380139 0.277818 0.37731 0.279816 0.374147 0.279816H0.359082H0.358915C0.336298 0.279816 0.314034 0.287619 0.297825 0.301248C0.280929 0.315438 0.271628 0.334872 0.271587 0.356013C0.271566 0.375301 0.284362 0.39923 0.294288 0.410487H0.346993C0.350302 0.398522 0.361142 0.38968 0.374147 0.38968C0.389752 0.38968 0.402445 0.402351 0.402445 0.417957C0.402445 0.433562 0.389752 0.446256 0.374147 0.446256C0.361163 0.446256 0.350302 0.437412 0.346993 0.425447H0.292061C0.246846 0.437266 0.21524 0.472577 0.215177 0.511612C0.215115 0.550689 0.247033 0.586207 0.29279 0.597984C0.29539 0.59865 0.297429 0.600668 0.298137 0.60327C0.298844 0.60587 0.298116 0.608659 0.296202 0.610552C0.285964 0.620748 0.27115 0.646611 0.271108 0.666712C0.271066 0.714007 0.316718 0.743346 0.359082 0.743408H0.374147C0.377331 0.743408 0.38016 0.745406 0.381222 0.748403C0.388483 0.769064 0.406128 0.782131 0.429079 0.784503V0.657339C0.417114 0.654031 0.408271 0.643169 0.408271 0.630165C0.408271 0.614559 0.420964 0.601867 0.436569 0.601867C0.452175 0.601867 0.464868 0.614559 0.464868 0.630165C0.464868 0.643169 0.456024 0.654031 0.44406 0.657339V0.784358C0.472609 0.781237 0.491439 0.760345 0.491502 0.73005V0.587963C0.458251 0.581551 0.433135 0.552293 0.433135 0.517169ZM0.374147 0.664756C0.381492 0.664756 0.387464 0.670728 0.387464 0.678073C0.387464 0.685418 0.381492 0.691389 0.374147 0.691389C0.366801 0.691389 0.36083 0.685418 0.36083 0.678073C0.36083 0.670728 0.366801 0.664756 0.374147 0.664756ZM0.36083 0.417978C0.36083 0.410633 0.366801 0.404661 0.374147 0.404661C0.381492 0.404661 0.387464 0.410633 0.387464 0.417978C0.387464 0.425323 0.381492 0.431294 0.374147 0.431294C0.366801 0.431294 0.36083 0.425323 0.36083 0.417978ZM0.505236 0.467254C0.477668 0.467254 0.455319 0.489602 0.455319 0.51717C0.455319 0.544738 0.477668 0.567085 0.505236 0.567085C0.532804 0.567085 0.555151 0.544738 0.555151 0.51717C0.555151 0.489602 0.532804 0.467254 0.505236 0.467254Z" clip-rule="evenodd" fill-rule="evenodd"/>&#xa;<rect fill="#E0FBFF" rx="0.035" height="0.07" width="0.07" y="0.47998" x="0.469727"/>&#xa;</g>&#xa;<defs>&#xa;<linearGradient gradientUnits="userSpaceOnUse" y2="-0.207067" x2="1.20707" y1="1.2072" x1="-0.2072" id="paint0_linear_3051_23740">&#xa;<stop stop-color="#087997"/>&#xa;<stop stop-color="#0BA0B7" offset="1"/>&#xa;</linearGradient>&#xa;</defs>&#xa;</svg>;verticalLabelPosition=bottom;verticalAlign=top;ir.synchronized=1;strokeWidth=1;" parent="2" vertex="1" isThumb="0"><mxGeometry x="484" y="53" width="90" height="90" as="geometry"/></mxCell><mxCell id="7" value="Build an Artificial Intelligence model -> Deploy an Artificial Intelligence model" style="ir.ref=4e45b93b-a976-4b97-83dc-017f8ff713ef;edgeStyle=none;curved=1;html=1;ir.synchronized=1;strokeColor=#BFBFBF;strokeWidth=2;endArrow=open;endFill=0;endSize=4;sourcePerimeterSpacing=9;" parent="2" source="6" target="4" edge="1"><mxGeometry relative="1" as="geometry"/></mxCell><mxCell id="8" value="Data Preparation for Learning" style="ir.ref=51b9f19f-7d06-4be1-b5f4-5a22f88f36d5;rounded=1;whiteSpace=wrap;html=1;align=center;imageAlign=center;imageWidth=82;imageHeight=82;arcSize=90;absoluteArcSize=1;strokeColor=#20C9E3;fillColor=#E3FCFC;fontColor=#0084AD;fontSize=12;source=iriusrisk;ir.type=COMPONENT;ir.componentDefinition.ref=CD-ML-AI-DATA-PREPARATION-FOR-LEARNING;image=data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 1 1" height="1" width="1">&#xa;<mask height="1" width="1" y="0" x="0" maskUnits="userSpaceOnUse" style="mask-type:alpha" id="mask0_3051_23740">&#xa;<circle fill="black" r="0.5" cy="0.5" cx="0.5"/>&#xa;</mask>&#xa;<g mask="url(#mask0_3051_23740)">&#xa;<path fill="url(#paint0_linear_3051_23740)" d="M1 0H0V1H1V0Z"/>&#xa;<path fill="#E0FBFF" d="M0.700625 0.245403C0.715941 0.245403 0.728355 0.232988 0.728355 0.217672C0.728355 0.202357 0.715941 0.189941 0.700625 0.189941C0.688005 0.189941 0.677353 0.198372 0.673995 0.209908H0.62186C0.605319 0.209908 0.59191 0.223316 0.59191 0.239857V0.395151C0.59191 0.403115 0.585454 0.409572 0.57749 0.409572H0.506504V0.293133C0.506545 0.252037 0.47781 0.223281 0.436569 0.223219H0.436444C0.405795 0.223219 0.380451 0.239012 0.369049 0.264834H0.359061H0.358874C0.332428 0.264834 0.307313 0.273677 0.28817 0.289783C0.267861 0.306845 0.256667 0.330378 0.256625 0.355992C0.256605 0.376696 0.267571 0.399626 0.277953 0.414066C0.231698 0.430088 0.200258 0.468852 0.200195 0.511612C0.200133 0.554184 0.231552 0.59297 0.277891 0.609138C0.267363 0.623536 0.256168 0.646341 0.256147 0.666691C0.256085 0.71717 0.302236 0.758307 0.359082 0.75841H0.36907C0.379514 0.782222 0.402012 0.797535 0.429679 0.799748V0.819992C0.429679 0.836533 0.443088 0.849942 0.459628 0.849942H0.575277C0.591818 0.849942 0.605227 0.836533 0.605227 0.819992V0.664698C0.605227 0.656734 0.611683 0.650278 0.619647 0.650278H0.673996C0.677355 0.661814 0.688006 0.670243 0.700625 0.670243C0.715941 0.670243 0.728355 0.657828 0.728355 0.642512C0.728355 0.627197 0.715941 0.614781 0.700625 0.614781C0.688005 0.614781 0.677353 0.623212 0.673995 0.634748H0.619647C0.603107 0.634748 0.589698 0.648157 0.589698 0.664698V0.819992C0.589698 0.827956 0.583242 0.834413 0.575277 0.834413H0.459628C0.451664 0.834413 0.445208 0.827956 0.445208 0.819992V0.799603C0.481653 0.79591 0.506446 0.768387 0.506504 0.730112V0.589259C0.525607 0.588929 0.542899 0.581171 0.555616 0.568748H0.633743C0.636917 0.580568 0.647707 0.58927 0.660528 0.58927C0.675844 0.58927 0.688259 0.576855 0.688259 0.561539C0.688259 0.546224 0.675844 0.533808 0.660528 0.533808C0.647908 0.533808 0.637257 0.542238 0.633899 0.553774H0.567368C0.573703 0.543044 0.577337 0.530531 0.577337 0.517169L0.577234 0.513287H0.733575C0.736749 0.525106 0.747539 0.533808 0.76036 0.533808C0.775676 0.533808 0.788091 0.521393 0.788091 0.506077C0.788091 0.490762 0.775676 0.478346 0.76036 0.478346C0.74774 0.478346 0.737088 0.486776 0.73373 0.498312H0.574846C0.573144 0.492013 0.570609 0.486055 0.567368 0.480565H0.57789H0.637234C0.653775 0.480565 0.667184 0.467155 0.667184 0.450615V0.364094C0.667184 0.356131 0.67364 0.349674 0.681604 0.349674H0.736698C0.740706 0.360057 0.750782 0.367422 0.762578 0.367422C0.777894 0.367422 0.790309 0.355007 0.790309 0.339691C0.790309 0.324376 0.777894 0.31196 0.762578 0.31196C0.749163 0.31196 0.737971 0.321487 0.735402 0.334145H0.681604C0.665063 0.334145 0.651654 0.347553 0.651654 0.364094V0.450615C0.651654 0.458579 0.645198 0.465035 0.637234 0.465035H0.555042C0.542382 0.452937 0.525321 0.445403 0.506504 0.445079V0.425101H0.57749C0.59403 0.425101 0.60744 0.411692 0.60744 0.395151V0.239857C0.60744 0.231894 0.613896 0.225437 0.62186 0.225437H0.673995C0.677353 0.236974 0.688005 0.245403 0.700625 0.245403ZM0.433135 0.517169C0.433135 0.506737 0.435351 0.496822 0.439337 0.48787H0.394955H0.394934C0.386611 0.48787 0.381659 0.492865 0.381638 0.501208V0.650898C0.393602 0.654206 0.402445 0.665068 0.402445 0.678073C0.402445 0.693678 0.389752 0.706371 0.374147 0.706371C0.358542 0.706371 0.345848 0.693678 0.345848 0.678073C0.345848 0.665068 0.354692 0.654206 0.366656 0.650898V0.529507H0.318071C0.314783 0.541451 0.303921 0.550314 0.290917 0.550314C0.275311 0.550314 0.262618 0.537621 0.262618 0.522016C0.262618 0.50641 0.275311 0.493717 0.290917 0.493717C0.303901 0.493717 0.314762 0.502561 0.318071 0.514525H0.366656V0.501208C0.366677 0.484541 0.378308 0.47291 0.394913 0.47291H0.394955H0.429079H0.44406H0.448314C0.458829 0.459406 0.474044 0.449741 0.491502 0.446375V0.425101H0.4588C0.44226 0.425101 0.428851 0.411692 0.428851 0.395151V0.354585H0.429079V0.341094C0.417114 0.337786 0.408271 0.326944 0.408271 0.31394C0.408271 0.298334 0.420964 0.285642 0.436569 0.285642C0.452175 0.285642 0.464868 0.298334 0.464868 0.31394C0.464868 0.326924 0.456024 0.337786 0.44406 0.341094V0.354585H0.444381V0.395151C0.444381 0.403115 0.450837 0.409572 0.4588 0.409572H0.491502V0.293133C0.491564 0.260319 0.469487 0.238263 0.436549 0.2382H0.436444C0.409998 0.2382 0.389358 0.251892 0.381201 0.274842C0.380139 0.277818 0.37731 0.279816 0.374147 0.279816H0.359082H0.358915C0.336298 0.279816 0.314034 0.287619 0.297825 0.301248C0.280929 0.315438 0.271628 0.334872 0.271587 0.356013C0.271566 0.375301 0.284362 0.39923 0.294288 0.410487H0.346993C0.350302 0.398522 0.361142 0.38968 0.374147 0.38968C0.389752 0.38968 0.402445 0.402351 0.402445 0.417957C0.402445 0.433562 0.389752 0.446256 0.374147 0.446256C0.361163 0.446256 0.350302 0.437412 0.346993 0.425447H0.292061C0.246846 0.437266 0.21524 0.472577 0.215177 0.511612C0.215115 0.550689 0.247033 0.586207 0.29279 0.597984C0.29539 0.59865 0.297429 0.600668 0.298137 0.60327C0.298844 0.60587 0.298116 0.608659 0.296202 0.610552C0.285964 0.620748 0.27115 0.646611 0.271108 0.666712C0.271066 0.714007 0.316718 0.743346 0.359082 0.743408H0.374147C0.377331 0.743408 0.38016 0.745406 0.381222 0.748403C0.388483 0.769064 0.406128 0.782131 0.429079 0.784503V0.657339C0.417114 0.654031 0.408271 0.643169 0.408271 0.630165C0.408271 0.614559 0.420964 0.601867 0.436569 0.601867C0.452175 0.601867 0.464868 0.614559 0.464868 0.630165C0.464868 0.643169 0.456024 0.654031 0.44406 0.657339V0.784358C0.472609 0.781237 0.491439 0.760345 0.491502 0.73005V0.587963C0.458251 0.581551 0.433135 0.552293 0.433135 0.517169ZM0.374147 0.664756C0.381492 0.664756 0.387464 0.670728 0.387464 0.678073C0.387464 0.685418 0.381492 0.691389 0.374147 0.691389C0.366801 0.691389 0.36083 0.685418 0.36083 0.678073C0.36083 0.670728 0.366801 0.664756 0.374147 0.664756ZM0.36083 0.417978C0.36083 0.410633 0.366801 0.404661 0.374147 0.404661C0.381492 0.404661 0.387464 0.410633 0.387464 0.417978C0.387464 0.425323 0.381492 0.431294 0.374147 0.431294C0.366801 0.431294 0.36083 0.425323 0.36083 0.417978ZM0.505236 0.467254C0.477668 0.467254 0.455319 0.489602 0.455319 0.51717C0.455319 0.544738 0.477668 0.567085 0.505236 0.567085C0.532804 0.567085 0.555151 0.544738 0.555151 0.51717C0.555151 0.489602 0.532804 0.467254 0.505236 0.467254Z" clip-rule="evenodd" fill-rule="evenodd"/>&#xa;<rect fill="#E0FBFF" rx="0.035" height="0.07" width="0.07" y="0.47998" x="0.469727"/>&#xa;</g>&#xa;<defs>&#xa;<linearGradient gradientUnits="userSpaceOnUse" y2="-0.207067" x2="1.20707" y1="1.2072" x1="-0.2072" id="paint0_linear_3051_23740">&#xa;<stop stop-color="#087997"/>&#xa;<stop stop-color="#0BA0B7" offset="1"/>&#xa;</linearGradient>&#xa;</defs>&#xa;</svg>;verticalLabelPosition=bottom;verticalAlign=top;ir.synchronized=1;strokeWidth=1;" parent="2" vertex="1" isThumb="0"><mxGeometry x="694" y="53" width="90" height="90" as="geometry"/></mxCell><mxCell id="9" value="Data Preparation for Learning -> Build an Artificial Intelligence model" style="ir.ref=7a86177c-37c3-4e7b-8a42-221e0df13478;edgeStyle=none;curved=1;html=1;ir.synchronized=1;strokeColor=#BFBFBF;strokeWidth=2;endArrow=open;endFill=0;endSize=4;sourcePerimeterSpacing=9;" parent="2" source="8" target="6" edge="1"><mxGeometry relative="1" as="geometry"/></mxCell><mxCell id="10" value="Private Secured" style="ir.ref=2ab4effa-40b7-4cd2-ba81-8247d29a6f2d;rounded=1;whiteSpace=wrap;recursiveResize=0;html=1;verticalAlign=top;align=left;dashed=1;strokeWidth=1;arcSize=3;absoluteArcSize=1;spacingTop=1;spacingLeft=32;strokeColor=#7575EB;fillColor=#F0F0FF;fillOpacity=30;fontColor=#5651E0;connectable=0;container=1;source=iriusrisk;ir.type=TRUSTZONE;" parent="1" vertex="1" isThumb="0"><mxGeometry x="1" y="-71" width="602" height="222" as="geometry"/></mxCell><mxCell id="11" value="Other data store" style="ir.ref=933dca8b-9277-4a0a-97ea-5d4fd542fbde;rounded=1;whiteSpace=wrap;html=1;align=center;imageAlign=center;imageWidth=82;imageHeight=82;arcSize=90;absoluteArcSize=1;strokeColor=#20C9E3;fillColor=#E3FCFC;fontColor=#0084AD;fontSize=12;source=iriusrisk;ir.type=COMPONENT;ir.componentDefinition.ref=other-database;image=data:image/svg+xml,PHN2ZyB4bWxucz0iaHR0cDovL3d3dy53My5vcmcvMjAwMC9zdmciIHdpZHRoPSIxIiBoZWlnaHQ9IjEiIHZpZXdCb3g9IjAgMCAxIDEiIGZpbGw9Im5vbmUiPiYjeGE7PHN0eWxlPiYjeGE7ICAgIC5pY29uLWZpbGwgeyYjeGE7ICAgICAgICBmaWxsOiAjMTFBOENGOyYjeGE7ICAgIH0mI3hhOzwvc3R5bGU+JiN4YTs8ZyBjbGlwLXBhdGg9InVybCgjY2xpcDApIj4mI3hhOzxwYXRoIGZpbGwtcnVsZT0iZXZlbm9kZCIgY2xpcC1ydWxlPSJldmVub2RkIiBkPSJNMC41MDA2NDUgMC40MTYwNzhDMC42ODg0MSAwLjQxNjA3OCAwLjg0MDY0NSAwLjM0MDg5OCAwLjg0MDY0NSAwLjI0ODA3OEMwLjg0MDY0NSAwLjE1NTI1OCAwLjY4ODQxIDAuMDgwMDc4MSAwLjUwMDY0NSAwLjA4MDA3ODFDMC4zMTI4OCAwLjA4MDA3ODEgMC4xNjA2NDUgMC4xNTUyNTggMC4xNjA2NDUgMC4yNDgwNzhDMC4xNjA2NDUgMC4zNDA4OTggMC4zMTI4OCAwLjQxNjA3OCAwLjUwMDY0NSAwLjQxNjA3OFpNMC43Njk1ODUgMC40MjMyMThDMC43OTUyNjcgMC40MTA3NTUgMC44MTkxNjIgMC4zOTQ5NzkgMC44NDA2NDUgMC4zNzYzMDRWMC41MDAwNjhDMC44MTkxNjEgMC41MTg3NDQgMC43OTUyNjcgMC41MzQ1MTkgMC43Njk1ODUgMC41NDY5ODJDMC42OTc0MTkgMC41ODI1OTggMC42MDIzMDQgMC42MDI4NDIgMC41MDA2NDUgMC42MDI4NDJDMC4zOTg5ODUgMC42MDI4NDIgMC4zMDM4NjkgMC41ODI1OTggMC4yMzE3MDUgMC41NDY5ODJDMC4yMDYwMjIgMC41MzQ1MTkgMC4xODIxMjcgMC41MTg3NDQgMC4xNjA2NDUgMC41MDAwNjhWMC4zNzYzMDRDMC4xODIxMjcgMC4zOTQ5NzkgMC4yMDYwMjIgMC40MTA3NTUgMC4yMzE3MDUgMC40MjMyMThDMC4zMDM4NjkgMC40NTg4MzQgMC4zOTg5ODUgMC40NzkwNzggMC41MDA2NDUgMC40NzkwNzhDMC42MDIzMDQgMC40NzkwNzggMC42OTc0MTkgMC40NTg4MzQgMC43Njk1ODUgMC40MjMyMThaTTAuNzY5NTg1IDAuNTg4NzU3QzAuNzk1MjY3IDAuNTc2MjkzIDAuODE5MTYxIDAuNTYwNTE4IDAuODQwNjQ1IDAuNTQxODQyVjAuNjUwMDY5QzAuODE5MTYxIDAuNjY4NzQ1IDAuNzk1MjY3IDAuNjg0NTIgMC43Njk1ODUgMC42OTY5ODRDMC42OTc0MTkgMC43MzI1OTkgMC42MDIzMDQgMC43NTI4NDMgMC41MDA2NDUgMC43NTI4NDNDMC4zOTg5ODUgMC43NTI4NDMgMC4zMDM4NjkgMC43MzI1OTkgMC4yMzE3MDUgMC42OTY5ODRDMC4yMDYwMjIgMC42ODQ1MiAwLjE4MjEyNyAwLjY2ODc0NSAwLjE2MDY0NSAwLjY1MDA2OVYwLjU0MTg0MkMwLjE4MjEyNyAwLjU2MDUxOCAwLjIwNjAyMiAwLjU3NjI5MyAwLjIzMTcwNSAwLjU4ODc1N0MwLjMwMzg2OSAwLjYyNDM3MiAwLjM5ODk4NSAwLjY0NDYxNiAwLjUwMDY0NSAwLjY0NDYxNkMwLjYwMjMwNCAwLjY0NDYxNiAwLjY5NzQxOSAwLjYyNDM3MiAwLjc2OTU4NSAwLjU4ODc1N1pNMC4xNjA2NDUgMC42OTE4NDRWMC43NTIwNzhDMC4xNjA2NDUgMC44NDQ4OTggMC4zMTI4OCAwLjkyMDA3OCAwLjUwMDY0NSAwLjkyMDA3OEMwLjY4ODQxIDAuOTIwMDc4IDAuODQwNjQ1IDAuODQ0ODk4IDAuODQwNjQ1IDAuNzUyMDc4VjAuNjkxODQ0QzAuODE5MTYxIDAuNzEwNTE5IDAuNzk1MjY3IDAuNzI2Mjk1IDAuNzY5NTg1IDAuNzM4NzU4QzAuNjk3NDE5IDAuNzc0Mzc0IDAuNjAyMzA0IDAuNzk0NjE4IDAuNTAwNjQ1IDAuNzk0NjE4QzAuMzk4OTg1IDAuNzk0NjE4IDAuMzAzODY5IDAuNzc0Mzc0IDAuMjMxNzA1IDAuNzM4NzU4QzAuMjA2MDIyIDAuNzI2Mjk1IDAuMTgyMTI3IDAuNzEwNTE5IDAuMTYwNjQ1IDAuNjkxODQ0WiIgY2xhc3M9Imljb24tZmlsbCIvPiYjeGE7PC9nPiYjeGE7PGRlZnM+JiN4YTs8Y2xpcFBhdGggaWQ9ImNsaXAwIj4mI3hhOzxyZWN0IHdpZHRoPSIxIiBoZWlnaHQ9IjEiIGZpbGw9IndoaXRlIi8+JiN4YTs8L2NsaXBQYXRoPiYjeGE7PC9kZWZzPiYjeGE7PC9zdmc+;verticalLabelPosition=bottom;verticalAlign=top;ir.synchronized=1;strokeWidth=1;" parent="10" vertex="1" isThumb="0"><mxGeometry x="433" y="64" width="90" height="90" as="geometry"/></mxCell><mxCell id="12" value="Data Pre-processing" style="ir.ref=dbebfd8f-4b75-4b12-8d82-c4a6d3b22137;rounded=1;whiteSpace=wrap;html=1;align=center;imageAlign=center;imageWidth=82;imageHeight=82;arcSize=90;absoluteArcSize=1;strokeColor=#20C9E3;fillColor=#E3FCFC;fontColor=#0084AD;fontSize=12;source=iriusrisk;ir.type=COMPONENT;ir.componentDefinition.ref=CD-ML-AI-DATA-PRE-PROCESSING;image=data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 1 1" height="1" width="1">&#xa;<mask height="1" width="1" y="0" x="0" maskUnits="userSpaceOnUse" style="mask-type:alpha" id="mask0_3051_23740">&#xa;<circle fill="black" r="0.5" cy="0.5" cx="0.5"/>&#xa;</mask>&#xa;<g mask="url(#mask0_3051_23740)">&#xa;<path fill="url(#paint0_linear_3051_23740)" d="M1 0H0V1H1V0Z"/>&#xa;<path fill="#E0FBFF" d="M0.700625 0.245403C0.715941 0.245403 0.728355 0.232988 0.728355 0.217672C0.728355 0.202357 0.715941 0.189941 0.700625 0.189941C0.688005 0.189941 0.677353 0.198372 0.673995 0.209908H0.62186C0.605319 0.209908 0.59191 0.223316 0.59191 0.239857V0.395151C0.59191 0.403115 0.585454 0.409572 0.57749 0.409572H0.506504V0.293133C0.506545 0.252037 0.47781 0.223281 0.436569 0.223219H0.436444C0.405795 0.223219 0.380451 0.239012 0.369049 0.264834H0.359061H0.358874C0.332428 0.264834 0.307313 0.273677 0.28817 0.289783C0.267861 0.306845 0.256667 0.330378 0.256625 0.355992C0.256605 0.376696 0.267571 0.399626 0.277953 0.414066C0.231698 0.430088 0.200258 0.468852 0.200195 0.511612C0.200133 0.554184 0.231552 0.59297 0.277891 0.609138C0.267363 0.623536 0.256168 0.646341 0.256147 0.666691C0.256085 0.71717 0.302236 0.758307 0.359082 0.75841H0.36907C0.379514 0.782222 0.402012 0.797535 0.429679 0.799748V0.819992C0.429679 0.836533 0.443088 0.849942 0.459628 0.849942H0.575277C0.591818 0.849942 0.605227 0.836533 0.605227 0.819992V0.664698C0.605227 0.656734 0.611683 0.650278 0.619647 0.650278H0.673996C0.677355 0.661814 0.688006 0.670243 0.700625 0.670243C0.715941 0.670243 0.728355 0.657828 0.728355 0.642512C0.728355 0.627197 0.715941 0.614781 0.700625 0.614781C0.688005 0.614781 0.677353 0.623212 0.673995 0.634748H0.619647C0.603107 0.634748 0.589698 0.648157 0.589698 0.664698V0.819992C0.589698 0.827956 0.583242 0.834413 0.575277 0.834413H0.459628C0.451664 0.834413 0.445208 0.827956 0.445208 0.819992V0.799603C0.481653 0.79591 0.506446 0.768387 0.506504 0.730112V0.589259C0.525607 0.588929 0.542899 0.581171 0.555616 0.568748H0.633743C0.636917 0.580568 0.647707 0.58927 0.660528 0.58927C0.675844 0.58927 0.688259 0.576855 0.688259 0.561539C0.688259 0.546224 0.675844 0.533808 0.660528 0.533808C0.647908 0.533808 0.637257 0.542238 0.633899 0.553774H0.567368C0.573703 0.543044 0.577337 0.530531 0.577337 0.517169L0.577234 0.513287H0.733575C0.736749 0.525106 0.747539 0.533808 0.76036 0.533808C0.775676 0.533808 0.788091 0.521393 0.788091 0.506077C0.788091 0.490762 0.775676 0.478346 0.76036 0.478346C0.74774 0.478346 0.737088 0.486776 0.73373 0.498312H0.574846C0.573144 0.492013 0.570609 0.486055 0.567368 0.480565H0.57789H0.637234C0.653775 0.480565 0.667184 0.467155 0.667184 0.450615V0.364094C0.667184 0.356131 0.67364 0.349674 0.681604 0.349674H0.736698C0.740706 0.360057 0.750782 0.367422 0.762578 0.367422C0.777894 0.367422 0.790309 0.355007 0.790309 0.339691C0.790309 0.324376 0.777894 0.31196 0.762578 0.31196C0.749163 0.31196 0.737971 0.321487 0.735402 0.334145H0.681604C0.665063 0.334145 0.651654 0.347553 0.651654 0.364094V0.450615C0.651654 0.458579 0.645198 0.465035 0.637234 0.465035H0.555042C0.542382 0.452937 0.525321 0.445403 0.506504 0.445079V0.425101H0.57749C0.59403 0.425101 0.60744 0.411692 0.60744 0.395151V0.239857C0.60744 0.231894 0.613896 0.225437 0.62186 0.225437H0.673995C0.677353 0.236974 0.688005 0.245403 0.700625 0.245403ZM0.433135 0.517169C0.433135 0.506737 0.435351 0.496822 0.439337 0.48787H0.394955H0.394934C0.386611 0.48787 0.381659 0.492865 0.381638 0.501208V0.650898C0.393602 0.654206 0.402445 0.665068 0.402445 0.678073C0.402445 0.693678 0.389752 0.706371 0.374147 0.706371C0.358542 0.706371 0.345848 0.693678 0.345848 0.678073C0.345848 0.665068 0.354692 0.654206 0.366656 0.650898V0.529507H0.318071C0.314783 0.541451 0.303921 0.550314 0.290917 0.550314C0.275311 0.550314 0.262618 0.537621 0.262618 0.522016C0.262618 0.50641 0.275311 0.493717 0.290917 0.493717C0.303901 0.493717 0.314762 0.502561 0.318071 0.514525H0.366656V0.501208C0.366677 0.484541 0.378308 0.47291 0.394913 0.47291H0.394955H0.429079H0.44406H0.448314C0.458829 0.459406 0.474044 0.449741 0.491502 0.446375V0.425101H0.4588C0.44226 0.425101 0.428851 0.411692 0.428851 0.395151V0.354585H0.429079V0.341094C0.417114 0.337786 0.408271 0.326944 0.408271 0.31394C0.408271 0.298334 0.420964 0.285642 0.436569 0.285642C0.452175 0.285642 0.464868 0.298334 0.464868 0.31394C0.464868 0.326924 0.456024 0.337786 0.44406 0.341094V0.354585H0.444381V0.395151C0.444381 0.403115 0.450837 0.409572 0.4588 0.409572H0.491502V0.293133C0.491564 0.260319 0.469487 0.238263 0.436549 0.2382H0.436444C0.409998 0.2382 0.389358 0.251892 0.381201 0.274842C0.380139 0.277818 0.37731 0.279816 0.374147 0.279816H0.359082H0.358915C0.336298 0.279816 0.314034 0.287619 0.297825 0.301248C0.280929 0.315438 0.271628 0.334872 0.271587 0.356013C0.271566 0.375301 0.284362 0.39923 0.294288 0.410487H0.346993C0.350302 0.398522 0.361142 0.38968 0.374147 0.38968C0.389752 0.38968 0.402445 0.402351 0.402445 0.417957C0.402445 0.433562 0.389752 0.446256 0.374147 0.446256C0.361163 0.446256 0.350302 0.437412 0.346993 0.425447H0.292061C0.246846 0.437266 0.21524 0.472577 0.215177 0.511612C0.215115 0.550689 0.247033 0.586207 0.29279 0.597984C0.29539 0.59865 0.297429 0.600668 0.298137 0.60327C0.298844 0.60587 0.298116 0.608659 0.296202 0.610552C0.285964 0.620748 0.27115 0.646611 0.271108 0.666712C0.271066 0.714007 0.316718 0.743346 0.359082 0.743408H0.374147C0.377331 0.743408 0.38016 0.745406 0.381222 0.748403C0.388483 0.769064 0.406128 0.782131 0.429079 0.784503V0.657339C0.417114 0.654031 0.408271 0.643169 0.408271 0.630165C0.408271 0.614559 0.420964 0.601867 0.436569 0.601867C0.452175 0.601867 0.464868 0.614559 0.464868 0.630165C0.464868 0.643169 0.456024 0.654031 0.44406 0.657339V0.784358C0.472609 0.781237 0.491439 0.760345 0.491502 0.73005V0.587963C0.458251 0.581551 0.433135 0.552293 0.433135 0.517169ZM0.374147 0.664756C0.381492 0.664756 0.387464 0.670728 0.387464 0.678073C0.387464 0.685418 0.381492 0.691389 0.374147 0.691389C0.366801 0.691389 0.36083 0.685418 0.36083 0.678073C0.36083 0.670728 0.366801 0.664756 0.374147 0.664756ZM0.36083 0.417978C0.36083 0.410633 0.366801 0.404661 0.374147 0.404661C0.381492 0.404661 0.387464 0.410633 0.387464 0.417978C0.387464 0.425323 0.381492 0.431294 0.374147 0.431294C0.366801 0.431294 0.36083 0.425323 0.36083 0.417978ZM0.505236 0.467254C0.477668 0.467254 0.455319 0.489602 0.455319 0.51717C0.455319 0.544738 0.477668 0.567085 0.505236 0.567085C0.532804 0.567085 0.555151 0.544738 0.555151 0.51717C0.555151 0.489602 0.532804 0.467254 0.505236 0.467254Z" clip-rule="evenodd" fill-rule="evenodd"/>&#xa;<rect fill="#E0FBFF" rx="0.035" height="0.07" width="0.07" y="0.47998" x="0.469727"/>&#xa;</g>&#xa;<defs>&#xa;<linearGradient gradientUnits="userSpaceOnUse" y2="-0.207067" x2="1.20707" y1="1.2072" x1="-0.2072" id="paint0_linear_3051_23740">&#xa;<stop stop-color="#087997"/>&#xa;<stop stop-color="#0BA0B7" offset="1"/>&#xa;</linearGradient>&#xa;</defs>&#xa;</svg>;verticalLabelPosition=bottom;verticalAlign=top;ir.synchronized=1;strokeWidth=1;" parent="10" vertex="1" isThumb="0"><mxGeometry x="253" y="66" width="90" height="90" as="geometry"/></mxCell><mxCell id="13" value="Data Pre-processing -> Other data store" style="ir.ref=8500ce57-1c3e-47a2-88ca-bcdc36862cb4;edgeStyle=none;curved=1;html=1;ir.synchronized=1;strokeColor=#BFBFBF;strokeWidth=2;endArrow=open;endFill=0;endSize=4;sourcePerimeterSpacing=9;" parent="10" source="12" target="11" edge="1"><mxGeometry relative="1" as="geometry"/></mxCell><mxCell id="14" value="Raw Data Collection" style="ir.ref=bd3eee1b-37cb-4b9a-bb34-cd16a716588f;rounded=1;whiteSpace=wrap;html=1;align=center;imageAlign=center;imageWidth=82;imageHeight=82;arcSize=90;absoluteArcSize=1;strokeColor=#20C9E3;fillColor=#E3FCFC;fontColor=#0084AD;fontSize=12;source=iriusrisk;ir.type=COMPONENT;ir.componentDefinition.ref=CD-ML-AI-RAW-DATA-COLLECTION;image=data:image/svg+xml,<svg xmlns="http://www.w3.org/2000/svg" fill="none" viewBox="0 0 1 1" height="1" width="1">&#xa;<mask height="1" width="1" y="0" x="0" maskUnits="userSpaceOnUse" style="mask-type:alpha" id="mask0_3051_23740">&#xa;<circle fill="black" r="0.5" cy="0.5" cx="0.5"/>&#xa;</mask>&#xa;<g mask="url(#mask0_3051_23740)">&#xa;<path fill="url(#paint0_linear_3051_23740)" d="M1 0H0V1H1V0Z"/>&#xa;<path fill="#E0FBFF" d="M0.700625 0.245403C0.715941 0.245403 0.728355 0.232988 0.728355 0.217672C0.728355 0.202357 0.715941 0.189941 0.700625 0.189941C0.688005 0.189941 0.677353 0.198372 0.673995 0.209908H0.62186C0.605319 0.209908 0.59191 0.223316 0.59191 0.239857V0.395151C0.59191 0.403115 0.585454 0.409572 0.57749 0.409572H0.506504V0.293133C0.506545 0.252037 0.47781 0.223281 0.436569 0.223219H0.436444C0.405795 0.223219 0.380451 0.239012 0.369049 0.264834H0.359061H0.358874C0.332428 0.264834 0.307313 0.273677 0.28817 0.289783C0.267861 0.306845 0.256667 0.330378 0.256625 0.355992C0.256605 0.376696 0.267571 0.399626 0.277953 0.414066C0.231698 0.430088 0.200258 0.468852 0.200195 0.511612C0.200133 0.554184 0.231552 0.59297 0.277891 0.609138C0.267363 0.623536 0.256168 0.646341 0.256147 0.666691C0.256085 0.71717 0.302236 0.758307 0.359082 0.75841H0.36907C0.379514 0.782222 0.402012 0.797535 0.429679 0.799748V0.819992C0.429679 0.836533 0.443088 0.849942 0.459628 0.849942H0.575277C0.591818 0.849942 0.605227 0.836533 0.605227 0.819992V0.664698C0.605227 0.656734 0.611683 0.650278 0.619647 0.650278H0.673996C0.677355 0.661814 0.688006 0.670243 0.700625 0.670243C0.715941 0.670243 0.728355 0.657828 0.728355 0.642512C0.728355 0.627197 0.715941 0.614781 0.700625 0.614781C0.688005 0.614781 0.677353 0.623212 0.673995 0.634748H0.619647C0.603107 0.634748 0.589698 0.648157 0.589698 0.664698V0.819992C0.589698 0.827956 0.583242 0.834413 0.575277 0.834413H0.459628C0.451664 0.834413 0.445208 0.827956 0.445208 0.819992V0.799603C0.481653 0.79591 0.506446 0.768387 0.506504 0.730112V0.589259C0.525607 0.588929 0.542899 0.581171 0.555616 0.568748H0.633743C0.636917 0.580568 0.647707 0.58927 0.660528 0.58927C0.675844 0.58927 0.688259 0.576855 0.688259 0.561539C0.688259 0.546224 0.675844 0.533808 0.660528 0.533808C0.647908 0.533808 0.637257 0.542238 0.633899 0.553774H0.567368C0.573703 0.543044 0.577337 0.530531 0.577337 0.517169L0.577234 0.513287H0.733575C0.736749 0.525106 0.747539 0.533808 0.76036 0.533808C0.775676 0.533808 0.788091 0.521393 0.788091 0.506077C0.788091 0.490762 0.775676 0.478346 0.76036 0.478346C0.74774 0.478346 0.737088 0.486776 0.73373 0.498312H0.574846C0.573144 0.492013 0.570609 0.486055 0.567368 0.480565H0.57789H0.637234C0.653775 0.480565 0.667184 0.467155 0.667184 0.450615V0.364094C0.667184 0.356131 0.67364 0.349674 0.681604 0.349674H0.736698C0.740706 0.360057 0.750782 0.367422 0.762578 0.367422C0.777894 0.367422 0.790309 0.355007 0.790309 0.339691C0.790309 0.324376 0.777894 0.31196 0.762578 0.31196C0.749163 0.31196 0.737971 0.321487 0.735402 0.334145H0.681604C0.665063 0.334145 0.651654 0.347553 0.651654 0.364094V0.450615C0.651654 0.458579 0.645198 0.465035 0.637234 0.465035H0.555042C0.542382 0.452937 0.525321 0.445403 0.506504 0.445079V0.425101H0.57749C0.59403 0.425101 0.60744 0.411692 0.60744 0.395151V0.239857C0.60744 0.231894 0.613896 0.225437 0.62186 0.225437H0.673995C0.677353 0.236974 0.688005 0.245403 0.700625 0.245403ZM0.433135 0.517169C0.433135 0.506737 0.435351 0.496822 0.439337 0.48787H0.394955H0.394934C0.386611 0.48787 0.381659 0.492865 0.381638 0.501208V0.650898C0.393602 0.654206 0.402445 0.665068 0.402445 0.678073C0.402445 0.693678 0.389752 0.706371 0.374147 0.706371C0.358542 0.706371 0.345848 0.693678 0.345848 0.678073C0.345848 0.665068 0.354692 0.654206 0.366656 0.650898V0.529507H0.318071C0.314783 0.541451 0.303921 0.550314 0.290917 0.550314C0.275311 0.550314 0.262618 0.537621 0.262618 0.522016C0.262618 0.50641 0.275311 0.493717 0.290917 0.493717C0.303901 0.493717 0.314762 0.502561 0.318071 0.514525H0.366656V0.501208C0.366677 0.484541 0.378308 0.47291 0.394913 0.47291H0.394955H0.429079H0.44406H0.448314C0.458829 0.459406 0.474044 0.449741 0.491502 0.446375V0.425101H0.4588C0.44226 0.425101 0.428851 0.411692 0.428851 0.395151V0.354585H0.429079V0.341094C0.417114 0.337786 0.408271 0.326944 0.408271 0.31394C0.408271 0.298334 0.420964 0.285642 0.436569 0.285642C0.452175 0.285642 0.464868 0.298334 0.464868 0.31394C0.464868 0.326924 0.456024 0.337786 0.44406 0.341094V0.354585H0.444381V0.395151C0.444381 0.403115 0.450837 0.409572 0.4588 0.409572H0.491502V0.293133C0.491564 0.260319 0.469487 0.238263 0.436549 0.2382H0.436444C0.409998 0.2382 0.389358 0.251892 0.381201 0.274842C0.380139 0.277818 0.37731 0.279816 0.374147 0.279816H0.359082H0.358915C0.336298 0.279816 0.314034 0.287619 0.297825 0.301248C0.280929 0.315438 0.271628 0.334872 0.271587 0.356013C0.271566 0.375301 0.284362 0.39923 0.294288 0.410487H0.346993C0.350302 0.398522 0.361142 0.38968 0.374147 0.38968C0.389752 0.38968 0.402445 0.402351 0.402445 0.417957C0.402445 0.433562 0.389752 0.446256 0.374147 0.446256C0.361163 0.446256 0.350302 0.437412 0.346993 0.425447H0.292061C0.246846 0.437266 0.21524 0.472577 0.215177 0.511612C0.215115 0.550689 0.247033 0.586207 0.29279 0.597984C0.29539 0.59865 0.297429 0.600668 0.298137 0.60327C0.298844 0.60587 0.298116 0.608659 0.296202 0.610552C0.285964 0.620748 0.27115 0.646611 0.271108 0.666712C0.271066 0.714007 0.316718 0.743346 0.359082 0.743408H0.374147C0.377331 0.743408 0.38016 0.745406 0.381222 0.748403C0.388483 0.769064 0.406128 0.782131 0.429079 0.784503V0.657339C0.417114 0.654031 0.408271 0.643169 0.408271 0.630165C0.408271 0.614559 0.420964 0.601867 0.436569 0.601867C0.452175 0.601867 0.464868 0.614559 0.464868 0.630165C0.464868 0.643169 0.456024 0.654031 0.44406 0.657339V0.784358C0.472609 0.781237 0.491439 0.760345 0.491502 0.73005V0.587963C0.458251 0.581551 0.433135 0.552293 0.433135 0.517169ZM0.374147 0.664756C0.381492 0.664756 0.387464 0.670728 0.387464 0.678073C0.387464 0.685418 0.381492 0.691389 0.374147 0.691389C0.366801 0.691389 0.36083 0.685418 0.36083 0.678073C0.36083 0.670728 0.366801 0.664756 0.374147 0.664756ZM0.36083 0.417978C0.36083 0.410633 0.366801 0.404661 0.374147 0.404661C0.381492 0.404661 0.387464 0.410633 0.387464 0.417978C0.387464 0.425323 0.381492 0.431294 0.374147 0.431294C0.366801 0.431294 0.36083 0.425323 0.36083 0.417978ZM0.505236 0.467254C0.477668 0.467254 0.455319 0.489602 0.455319 0.51717C0.455319 0.544738 0.477668 0.567085 0.505236 0.567085C0.532804 0.567085 0.555151 0.544738 0.555151 0.51717C0.555151 0.489602 0.532804 0.467254 0.505236 0.467254Z" clip-rule="evenodd" fill-rule="evenodd"/>&#xa;<rect fill="#E0FBFF" rx="0.035" height="0.07" width="0.07" y="0.47998" x="0.469727"/>&#xa;</g>&#xa;<defs>&#xa;<linearGradient gradientUnits="userSpaceOnUse" y2="-0.207067" x2="1.20707" y1="1.2072" x1="-0.2072" id="paint0_linear_3051_23740">&#xa;<stop stop-color="#087997"/>&#xa;<stop stop-color="#0BA0B7" offset="1"/>&#xa;</linearGradient>&#xa;</defs>&#xa;</svg>;verticalLabelPosition=bottom;verticalAlign=top;ir.synchronized=1;strokeWidth=1;" parent="10" vertex="1" isThumb="0"><mxGeometry x="64" y="66" width="90" height="90" as="geometry"/></mxCell><mxCell id="15" value="Raw Data Collection -> Data Pre-processing" style="ir.ref=b245d2fe-91d5-4572-8535-ae19d9a43a0b;edgeStyle=none;curved=1;html=1;ir.synchronized=1;strokeColor=#BFBFBF;strokeWidth=2;endArrow=open;endFill=0;endSize=4;sourcePerimeterSpacing=9;" parent="10" source="14" target="12" edge="1"><mxGeometry relative="1" as="geometry"/></mxCell><mxCell id="16" value="Data Preparation for Learning -> Other data store" style="ir.ref=ac629dc9-36c3-46a0-8df3-1710059bb935;edgeStyle=none;curved=1;html=1;ir.synchronized=1;strokeColor=#BFBFBF;strokeWidth=2;endArrow=open;endFill=0;endSize=4;sourcePerimeterSpacing=9;" parent="1" source="8" target="11" edge="1"><mxGeometry relative="1" as="geometry"/></mxCell><mxCell id="17" value="Private Secured" style="ir.ref=2ab4effa-40b7-4cd2-ba81-8247d29a6f2d;rounded=1;whiteSpace=wrap;recursiveResize=0;html=1;verticalAlign=top;align=left;dashed=1;strokeWidth=1;arcSize=3;absoluteArcSize=1;spacingTop=1;spacingLeft=32;strokeColor=#7575EB;fillColor=#F0F0FF;fillOpacity=30;fontColor=#5651E0;connectable=0;container=1;source=iriusrisk;ir.type=TRUSTZONE;" parent="1" vertex="1" isThumb="0"><mxGeometry x="-260" y="-71" width="230" height="222" as="geometry"/></mxCell><mxCell id="18" value="Administration interface" style="ir.ref=5d4ed7c6-d6b4-4e24-8707-41d13ae746fd;rounded=1;whiteSpace=wrap;html=1;align=center;imageAlign=center;imageWidth=82;imageHeight=82;arcSize=90;absoluteArcSize=1;strokeColor=#20C9E3;fillColor=#E3FCFC;fontColor=#0084AD;fontSize=12;source=iriusrisk;ir.type=COMPONENT;ir.componentDefinition.ref=CD-ADMINISTRATION-INTERFACE;image=data:image/svg+xml,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;verticalLabelPosition=bottom;verticalAlign=top;ir.synchronized=1;strokeWidth=1;" parent="17" vertex="1" isThumb="0"><mxGeometry x="73" y="66" width="90" height="90" as="geometry"/></mxCell><mxCell id="19" value="Administration interface -> Trained Model" style="ir.ref=ee4cb995-d3d0-4bd6-b271-26d6545b30d8;edgeStyle=none;curved=1;html=1;ir.synchronized=1;strokeColor=#BFBFBF;strokeWidth=2;endArrow=open;endFill=0;endSize=4;sourcePerimeterSpacing=9;" parent="1" source="18" target="3" edge="1"><mxGeometry relative="1" as="geometry"/></mxCell><mxCell id="20" value="&lt;h1 style=&quot;&quot;>&lt;span style=&quot;background-color: initial;&quot;>&lt;font color=&quot;#0365ab&quot;>How to use this template&lt;/font>&lt;/span>&lt;/h1>&lt;div style=&quot;&quot;>&lt;span style=&quot;color: rgb(26, 26, 26); background-color: initial;&quot;>This template shows an AI/ML powered analytics platform.&amp;nbsp;&lt;/span>&lt;/div>&lt;font color=&quot;#1a1a1a&quot;>&lt;div style=&quot;&quot;>&lt;br>&lt;/div>&lt;/font>&lt;blockquote style=&quot;margin: 0px 0px 0px 40px; border: none; padding: 0px;&quot;>&lt;/blockquote>&lt;span style=&quot;color: rgb(26, 26, 26); background-color: initial;&quot;>&lt;div style=&quot;&quot;>&lt;span style=&quot;background-color: initial;&quot;>Separate trust zones are used to segment the data acquisition and storage pipeline, the model creation process, and the administration interface.&lt;/span>&lt;/div>&lt;/span>&lt;font color=&quot;#1a1a1a&quot;>&lt;div style=&quot;&quot;>&lt;br>&lt;/div>&lt;/font>&lt;blockquote style=&quot;margin: 0px 0px 0px 40px; border: none; padding: 0px;&quot;>&lt;/blockquote>&lt;span style=&quot;background-color: initial;&quot;>&lt;div style=&quot;color: rgb(26, 26, 26);&quot;>&lt;span style=&quot;background-color: initial;&quot;>Components come primarily from the &quot;ML/AI - IriusRisk&quot; category. Add or change components by dragging them from the menu on the left.&lt;/span>&lt;/div>&lt;div style=&quot;color: rgb(26, 26, 26);&quot;>&lt;span style=&quot;background-color: initial;&quot;>&lt;br>&lt;/span>&lt;/div>&lt;h2 style=&quot;&quot;>&lt;span style=&quot;background-color: initial;&quot;>&lt;font color=&quot;#0365ab&quot;>What to do next&lt;/font>&lt;/span>&lt;/h2>&lt;div style=&quot;color: rgb(26, 26, 26);&quot;>&lt;span style=&quot;background-color: initial;&quot;>Click the orange &quot;Update model&quot; button if it is showing, then navigate over to the Threats and Countermeasures view to take a look at what could go wrong, and what to do about it.&lt;/span>&lt;br>&lt;/div>&lt;div style=&quot;color: rgb(26, 26, 26);&quot;>&lt;span style=&quot;background-color: initial;&quot;>&lt;br>&lt;/span>&lt;/div>&lt;h2 style=&quot;&quot;>&lt;span style=&quot;background-color: initial;&quot;>&lt;font color=&quot;#0365ab&quot;>Tips&lt;/font>&lt;/span>&lt;/h2>&lt;div style=&quot;color: rgb(26, 26, 26);&quot;>&lt;span style=&quot;background-color: initial;&quot;>Right-click on the components and complete the questionnaires to provide additional context to components.&lt;/span>&lt;/div>&lt;div style=&quot;color: rgb(26, 26, 26);&quot;>&lt;span style=&quot;background-color: initial;&quot;>&lt;br>&lt;/span>&lt;/div>&lt;div style=&quot;color: rgb(26, 26, 26);&quot;>&lt;span style=&quot;background-color: initial;&quot;>Add tags and assets to data flows to provide more context.&lt;/span>&lt;/div>&lt;div style=&quot;color: rgb(26, 26, 26);&quot;>&lt;br>&lt;/div>&lt;div style=&quot;color: rgb(26, 26, 26);&quot;>(and delete these notes when you're done with them)&lt;/div>&lt;/span>" style="rounded=1;whiteSpace=wrap;html=1;strokeColor=none;strokeWidth=1;fontColor=#8C8C8C;fillColor=#F5F5F5;arcSize=3;align=left;spacingLeft=20;spacingRight=20;spacing=0;spacingTop=0;verticalAlign=top;" parent="1" vertex="1"><mxGeometry x="-709" y="-178" width="324" height="544" as="geometry"/></mxCell><mxCell id="23" style="edgeStyle=none;curved=1;html=1;ir.synchronized=0;strokeColor=#059BF5;strokeWidth=2;endArrow=open;endFill=0;endSize=4;sourcePerimeterSpacing=9;fontColor=#059BF5;exitX=0;exitY=0.5;exitDx=0;exitDy=0;" parent="1" source="22" edge="1"><mxGeometry relative="1" as="geometry"><mxPoint x="-188" y="-77" as="targetPoint"/><Array as="points"><mxPoint x="-193" y="-120"/></Array></mxGeometry></mxCell><mxCell id="22" value="&lt;font color=&quot;#0365ab&quot; style=&quot;font-size: 15px;&quot;>&lt;b>A trust zone&lt;/b>&lt;/font>" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontColor=#8C8C8C;" parent="1" vertex="1"><mxGeometry x="-140" y="-150" width="96" height="30" as="geometry"/></mxCell><mxCell id="27" style="edgeStyle=none;curved=1;html=1;ir.synchronized=0;strokeColor=#059BF5;strokeWidth=2;endArrow=open;endFill=0;endSize=4;sourcePerimeterSpacing=9;" parent="1" source="24" edge="1"><mxGeometry relative="1" as="geometry"><mxPoint x="-157" y="352" as="targetPoint"/><Array as="points"><mxPoint x="-189" y="387"/></Array></mxGeometry></mxCell><mxCell id="24" value="&lt;font color=&quot;#0365ab&quot; style=&quot;font-size: 15px;&quot;>&lt;b>AI components&lt;/b>&lt;/font>" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontColor=#8C8C8C;" parent="1" vertex="1"><mxGeometry x="-251" y="440" width="111" height="36" as="geometry"/></mxCell><mxCell id="26" style="edgeStyle=none;curved=1;html=1;ir.synchronized=0;strokeColor=#059BF5;strokeWidth=2;endArrow=open;endFill=0;endSize=4;sourcePerimeterSpacing=9;exitX=0.75;exitY=1;exitDx=0;exitDy=0;" parent="1" source="25" edge="1"><mxGeometry relative="1" as="geometry"><mxPoint x="478.5" y="-33" as="targetPoint"/><Array as="points"><mxPoint x="512" y="-76"/></Array></mxGeometry></mxCell><mxCell id="25" value="&lt;font color=&quot;#0365ab&quot; style=&quot;font-size: 15px;&quot;>&lt;b>Replace this with your data store&lt;/b>&lt;/font>" style="text;html=1;strokeColor=none;fillColor=none;align=center;verticalAlign=middle;whiteSpace=wrap;rounded=0;fontColor=#8C8C8C;" parent="1" vertex="1"><mxGeometry x="413" y="-178" width="121" height="53" as="geometry"/></mxCell></root></mxGraphModel></schema>
  </diagram>
  <trustZones>
    <trustZone ref="private-secured" uuid="2ab4effa-40b7-4cd2-ba81-8247d29a6f2d" name="Private Secured" desc="A secured zone within a trusted private zone" trustRating="100"/>
  </trustZones>
  <questions/>
  <assets/>
  <settings/>
  <dataflows>
    <dataflow name="Deploy an Artificial Intelligence model -&gt; Trained Model" ref="316e5bcb-4575-458f-adb3-080f44cf40f4" source="299077f7-64f3-48ed-9f2f-ea92ea0fed27" target="6d8c6a95-3951-4011-b931-fffdf5f1923b">
      <assets/>
      <tags/>
    </dataflow>
    <dataflow name="Build an Artificial Intelligence model -&gt; Deploy an Artificial Intelligence model" ref="4e45b93b-a976-4b97-83dc-017f8ff713ef" source="08c19600-28fd-420d-aaa1-5890eacc707f" target="299077f7-64f3-48ed-9f2f-ea92ea0fed27">
      <assets/>
      <tags/>
    </dataflow>
    <dataflow name="Data Preparation for Learning -&gt; Build an Artificial Intelligence model" ref="7a86177c-37c3-4e7b-8a42-221e0df13478" source="4783c65b-6ce7-4433-bfba-ebaba30b97ff" target="08c19600-28fd-420d-aaa1-5890eacc707f">
      <assets/>
      <tags/>
    </dataflow>
    <dataflow name="Data Pre-processing -&gt; Other data store" ref="8500ce57-1c3e-47a2-88ca-bcdc36862cb4" source="0f80f801-b5a4-4b8f-8687-bdb7ef3f9799" target="97348d1e-36ad-4b51-85fd-e5ed8ed6907b">
      <assets/>
      <tags/>
    </dataflow>
    <dataflow name="Data Preparation for Learning -&gt; Other data store" ref="ac629dc9-36c3-46a0-8df3-1710059bb935" source="4783c65b-6ce7-4433-bfba-ebaba30b97ff" target="97348d1e-36ad-4b51-85fd-e5ed8ed6907b">
      <assets/>
      <tags/>
    </dataflow>
    <dataflow name="Raw Data Collection -&gt; Data Pre-processing" ref="b245d2fe-91d5-4572-8535-ae19d9a43a0b" source="a84f8551-e8e8-474c-9b0e-f8e39393dcc4" target="0f80f801-b5a4-4b8f-8687-bdb7ef3f9799">
      <assets/>
      <tags/>
    </dataflow>
    <dataflow name="Administration interface -&gt; Trained Model" ref="ee4cb995-d3d0-4bd6-b271-26d6545b30d8" source="3cef9f40-72b7-4cb1-af1d-1d9ef01bdd6b" target="6d8c6a95-3951-4011-b931-fffdf5f1923b">
      <assets/>
      <tags/>
    </dataflow>
  </dataflows>
  <customFields>
    <customField ref="status" value="Design"/>
  </customFields>
  <components>
    <component uuid="9d34308f-f78f-4a81-8401-c090678af257" diagramComponentId="5d4ed7c6-d6b4-4e24-8707-41d13ae746fd" ref="3cef9f40-72b7-4cb1-af1d-1d9ef01bdd6b" name="Administration interface" desc="" library="IR-Functional-Components" parentComponentRef="" componentDefinitionRef="CD-ADMINISTRATION-INTERFACE">
      <tags/>
      <questions/>
      <trustZones>
        <trustZone ref="private-secured"/>
      </trustZones>
      <assets/>
      <settings/>
      <weaknesses>
        <weakness ref="WEB-SRV-ADM" name="Access to management functions is not limited to web service administrators" state="0" impact="100" issueId="" issueLink="">
          <desc>The software does not perform any authentication for functionality that requires a provable
                        user identity or consumes a significant amount of resources, gives access to sensitive data or
                        sensitive business logic.
                        &lt;br /&gt;
                    </desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:47.475">
              <output/>
            </source>
            <references>
              <reference name="CWE-306: Missing Authentication for Critical Function" url="https://cwe.mitre.org/data/definitions/306.html"/>
            </references>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="CSD-ADM-OPEN" name="Access to the administration interfaces is open to untrusted parties" state="0" impact="100" issueId="" issueLink="">
          <desc>The application does not restrict access to administration interfaces to untrusted parties. 
This fact could allow an attacker to chain vulnerabilities in order to gain unauthorized access to the system. </desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:47.474">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </weakness>
      </weaknesses>
      <countermeasures>
        <countermeasure ref="WEB-SRV-ADM-AUTH" name="Restrict access to administrative functionality" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="CS-Default" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;div&gt;
                        If inadequate controls are in place, lower privileged users may be able to access higher
                        privilege or administrative functionality to subvert security within the application.
                        &lt;/div&gt;
                        &lt;div&gt;
                        &lt;ul&gt;
                        &lt;li&gt;Restrict administration functions to designated administrators only through robust
                        access controls.&lt;/li&gt;
                        &lt;li&gt;Ensure this restriction is applied at the server-side; do not rely on 'secret' areas
                        of the application, menu hiding, or other client-side techniques to protect the functionality.&lt;/li&gt;
                        &lt;li&gt;Measures to prevent cross-site request forgery must be present on administrative
                        functions.&lt;/li&gt;
                        &lt;/ul&gt;
                        &lt;/div&gt;
                    </desc>
          <implementations/>
          <references>
            <reference name="OSA AC-01 Access Control Policies and Procedures" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/23-08_02_AC-01"/>
            <reference name="OSA AC-02 Account Management" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/24-08_02_AC-02"/>
            <reference name="OSA AC-03 Access Enforcement" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/25-08_02_AC-03"/>
            <reference name="OSA AC-05 Separation Of Duties" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/27-08_02_AC-05"/>
            <reference name="OSA AC-06 Least Privilege" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/28-08_02_AC-06"/>
            <reference name="OSA SC-03 Security Function Isolation" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/63-08_02_SC-03"/>
            <reference name="OSA SI-10 Information Accuracy, Completeness, Validity, And Authenticity" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/48-08_02_SI-10"/>
          </references>
          <standards>
            <standard ref="1.2" supportedStandardRef="swift-cscf"/>
            <standard ref="12.1.2" supportedStandardRef="ISO/IEC 27002:2013"/>
            <standard ref="2.11A" supportedStandardRef="swift-cscf"/>
            <standard ref="2.9" supportedStandardRef="swift-cscf"/>
            <standard ref="4.3.1" supportedStandardRef="owasp-asvs4-level-2"/>
            <standard ref="4.3.1" supportedStandardRef="owasp-asvs4-level-3"/>
            <standard ref="4.3.1" supportedStandardRef="owasp-asvs4-level-1"/>
            <standard ref="5.1" supportedStandardRef="swift-cscf"/>
            <standard ref="8.32" supportedStandardRef="iso-27002-2022"/>
            <standard ref="AC-2" supportedStandardRef="fedramp-low-baseline"/>
            <standard ref="AC-2" supportedStandardRef="fedramp-moderate-baseline"/>
            <standard ref="AC-2" supportedStandardRef="NIST 800-53"/>
            <standard ref="AC-2" supportedStandardRef="fedramp-high-baseline"/>
            <standard ref="DE.CM-3" supportedStandardRef="nist-csf"/>
            <standard ref="PR.AC-4" supportedStandardRef="nist-csf"/>
          </standards>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps>&lt;ol&gt;
                            &lt;li&gt;Try to access administration resources with a user who is not an administrator.&amp;nbsp;&lt;/li&gt;
                            &lt;li&gt;Check the access is forbidden.&amp;nbsp;&lt;/li&gt;
                            &lt;/ol&gt;
                        </steps>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:47.338">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="CSD-ADM-TRUST" name="Restrict access to administrative interfaces" issueId="" issueLink="" platform="" cost="1" risk="56" state="Recommended" owner="fscott-admin" library="CS-Default" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Restrict access to administrative interfaces to trusted actors from trusted locations to reduce the application attack surface and likelihood of compromise.&lt;/p&gt; 
&lt;p&gt;Restrict administrative access to specific networks or hosts.&lt;/p&gt; 
&lt;div&gt;
  Use strong authentication for privileged access, for example a 2FA. 
&lt;/div&gt;</desc>
          <implementations/>
          <references>
            <reference name="OSA AC-01 Access Control Policies and Procedures" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/23-08_02_AC-01"/>
          </references>
          <standards>
            <standard ref="2.4.10.13" supportedStandardRef="iotsf-class-2"/>
            <standard ref="2.4.10.13" supportedStandardRef="iotsf-class-1"/>
            <standard ref="2.4.11.8" supportedStandardRef="iotsf-class-2"/>
            <standard ref="2.4.11.8" supportedStandardRef="iotsf-class-1"/>
            <standard ref="4.3.1" supportedStandardRef="owasp-asvs4-level-2"/>
            <standard ref="4.3.1" supportedStandardRef="owasp-asvs4-level-3"/>
            <standard ref="4.3.1" supportedStandardRef="owasp-asvs4-level-1"/>
          </standards>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps>&lt;p&gt;Try to access the administrative interface from an untrusted location. For example, if you are using a Wordpress &lt;span class="caps"&gt;CMS&lt;/span&gt; you should restrict access to /wp-admin which should not be available from the internet but only from trusted networks or IP addresses.&lt;/p&gt;</steps>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:47.337">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
      </countermeasures>
      <usecases>
        <usecase ref="ADMINISTER SERVICE" name="Administer service" desc="" library="CS-Default">
          <threats>
            <threat ref="CSD-ADM-IFACE" name="Attackers gain access to the system through an unprotected administration interface" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="CS-Default" editable="true">
              <desc>Administrative interfaces grant access to sensitive operations that can typically
                                affect key security mechanisms, like modifying credentials and adding/removing user
                                accounts.
                            </desc>
              <riskRating confidentiality="100" integrity="100" availability="100" easeOfExploitation="100"/>
              <references/>
              <weaknesses>
                <weakness ref="CSD-ADM-OPEN">
                  <countermeasures>
                    <countermeasure ref="CSD-ADM-TRUST" mitigation="50"/>
                  </countermeasures>
                </weakness>
                <weakness ref="WEB-SRV-ADM">
                  <countermeasures>
                    <countermeasure ref="WEB-SRV-ADM-AUTH" mitigation="50"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="CSD-ADM-TRUST" mitigation="50"/>
                <countermeasure ref="WEB-SRV-ADM-AUTH" mitigation="50"/>
              </countermeasures>
              <customFields/>
            </threat>
          </threats>
        </usecase>
      </usecases>
    </component>
    <component uuid="270e54a0-f2f8-467c-b76c-804007bfbc82" diagramComponentId="f443aa9e-03ce-4dd4-b970-1c42c722c16c" ref="08c19600-28fd-420d-aaa1-5890eacc707f" name="Build an Artificial Intelligence model" desc="" library="IR-Machine-Learning-Artificial-Intelligence" parentComponentRef="" componentDefinitionRef="CD-ML-AI-MODEL-BUILD">
      <tags/>
      <questions/>
      <trustZones>
        <trustZone ref="private-secured"/>
      </trustZones>
      <assets/>
      <settings/>
      <weaknesses/>
      <countermeasures>
        <countermeasure ref="C-ML-AI-MODEL-CHOICE" name="Carefully consider the model choice" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Models can be built using different approaches or algorithms. Overall, choosing the right algorithm depends on several factors, including business considerations, data size, accuracy, training time, parameters, and inference performance, among others.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;In general, the right choice is typically a combination of these aspects. In terms of security, however, the main algorithmic concern is about the nature of the data, where certain algorithms are more vulnerable to data leaks than others.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Depending on your privacy concerns, and/or compliance considerations, e.g., GPDR, &amp;nbsp;consider auditing the data privacy of the models under consideration, e.g., using tools like Privacy Meter.&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Privacy Meter" url="https://github.com/privacytrustlab/ml_privacy_meter"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.003">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-ROBUST-LEARNING" name="Consider robust learning and defenses against poisoning attacks for online models" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Robustness verification, robust learning techniques, and other defenses against adversarial perturbations have become important aspects when developing an ML/AI model. Techniques for achieving robustness include adversarial training and learning techniques in the presence of outliers or noise. Robust statistics such as stochastic approximation approaches, which have shown to be robust against data poisoning attacks, e.g., training with stochastic gradient descent, is an example.&lt;/p&gt;
&lt;p&gt;Depending on the domain area and the underlying ML/AI framework used, there are a number of tools that can help assess models against adversarial threats, including adversarial tampering. These include the Adversarial Robustness Toolbox, DeepRobust, or AutoAttack.&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Adversarial Robustness Toolbox (ART)" url="https://adversarial-robustness-toolbox.readthedocs.io/en/latest/"/>
            <reference name="Adversarial Robustness Tools" url="https://github.com/EthicalML/awesome-production-machine-learning#adversarial-robustness"/>
            <reference name="Robust Machine Learning Models and Their Applications" url="https://dspace.mit.edu/bitstream/handle/1721.1/130760/1252059420-MIT.pdf?sequence=1&amp;isAllowed=y"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.004">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-ENV-SETUP" name="Consider the right framework, model, and language for an application" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Setting up the right tool stack for your ML/AI projects is very important to work efficiently and be able to deliver results.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The best ML/AI toolbox would include:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Various flavors of Jupyter Notebooks which allow for quick experiments and visualization, and exchanging ideas between team members,&lt;/li&gt;
 &lt;li&gt;Appropriate IDEs with ease of use and possible connectivity with third-party platforms/clouds or useful extensions, e.g., Pycharm or Visual Studio,&lt;/li&gt;
 &lt;li&gt;Code and data versioning, e.g., Github, or Data Version Control (DVC),&lt;/li&gt;
 &lt;li&gt;The framework that provides the right functionality. Scikit-learn, R, TensorFlow, and PyTorch have been some of the favorites of ML/AI teams,&amp;nbsp; 
  &lt;ul&gt;
   &lt;li&gt;For more specialized areas, such as NLP or Computer Vision, more specialized frameworks and libraries exist such as Huggingface, Spacy, NLTK, Gensim, or OpenCV.&lt;/li&gt;
  &lt;/ul&gt;&lt;/li&gt;
 &lt;li&gt;For operationalizing models and managing their lifecycle, tools such as MLflow or Kubeflow are being used to manage ML/AI workflows and pipelines end-to-end.&lt;/li&gt;
 &lt;li&gt;In terms of larger and cloud-based platforms, Amazon SageMaker and Azure ML are the two biggest names. They provide fully-managed cloud services for predictive analytics&lt;/li&gt;
 &lt;li&gt;Running models in production and at scale also involves the language, the programming paradigm, and hosts: 
  &lt;ul&gt;
   &lt;li&gt;While most models are written in Python or R, these might be too slow for production models! Using faster languages such as C/C++ or Java might be a challenge.&amp;nbsp;&lt;/li&gt;
   &lt;li&gt;Use powerful hosts and parallel processing, e.g., Apache Spark, to perform iterative and heavy computational operations.&lt;/li&gt;
  &lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;The above only scratches the surface regarding what tools, libraries, and frameworks are available to build your model. &amp;nbsp;The landscape is also rapidly evolving. Make sure to consider the pros and cons of several alternatives before landing on a final solution.&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Awesome Production Machine Learning - a curated list of tools to help setup ML/AI projects" url="https://github.com/EthicalML/awesome-production-machine-learning"/>
            <reference name="Guidance and resources on building the right ML/AI platform - Neptune.ai's blog" url="https://neptune.ai/blog"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:48.853">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-EVALUATION-METRICS" name="Consider using data sets with well-known error rates for the model evaluation, in addition to well-established performance evaluation techniques" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;When evaluating an ML/AI model for real-world uses, two fundamental questions arise: 1) Is the model good enough? and 2) Is it better than other alternatives?&amp;nbsp;&lt;/p&gt;
&lt;p&gt;To obtain reliable answers to these questions, special care should be taken in the process of selecting data, selecting and building the model, as well as the evaluation metrics used.&lt;/p&gt;
&lt;p&gt;The validation/evaluation strategy of ML/AI models involves the following two main aspects:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Resampling methods, which are techniques of rearranging data samples to inspect if the model performs well. These include: 
  &lt;ul&gt;
   &lt;li&gt;Random Split,&lt;/li&gt;
   &lt;li&gt;Time-Based Split,&lt;/li&gt;
   &lt;li&gt;K-Fold Cross-Validation,&lt;/li&gt;
   &lt;li&gt;Stratified K-Fold, and&lt;/li&gt;
   &lt;li&gt;Bootstrap.&lt;/li&gt;
  &lt;/ul&gt;&lt;/li&gt;
 &lt;li&gt;Classification metrics, &amp;nbsp;including: 
  &lt;ul&gt;
   &lt;li&gt;Accuracy: sensitive to the sum of true positives+true negatives and to the total number of data points. Not sensitive to class imbalance.&lt;/li&gt;
   &lt;li&gt;Precision: sensitive to true positives and false positives. Not sensitive to false negatives or true negatives.&lt;/li&gt;
   &lt;li&gt;Recall: sensitive to true positives and false negatives. Not sensitive to false positives or true negatives.&lt;/li&gt;
   &lt;li&gt;F-score: sensitive to true positives, false positives, and false negatives. Not sensitive to true negatives.&lt;/li&gt;
   &lt;li&gt;Overlap: sensitive to the intersection and overlap of predicted and actual.&lt;/li&gt;
   &lt;li&gt;Likelihood: sensitive to the probability that the model assigns to the test data.&lt;/li&gt;
   &lt;li&gt;Distance: sensitive to the distance between the prediction and the actual value.&lt;/li&gt;
   &lt;li&gt;Correlation: sensitive to each of true positives, true negatives, false positives, and false negatives, but unlike Accuracy metrics they factor in the degree of agreement that would be expected by chance.&lt;/li&gt;
   &lt;li&gt;AUC: Does not rely on a specific classification threshold, but instead calculates the area under a curve parameterized by different thresholds.&lt;/li&gt;
  &lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other measures include:&amp;nbsp;&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Probabilistic measures, e.g., Akaike Information Criterion (AIC),&lt;/li&gt;
 &lt;li&gt;Regression metrics, e.g., Mean Squared Error or MSE, or&lt;/li&gt;
 &lt;li&gt;Clustering metrics, e.g., Elbow method.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="The Ultimate Guide to Evaluation and Selection of Models in Machine Learning" url="https://neptune.ai/blog/ml-model-evaluation-and-selection"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:48.854">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-PARAMETERS-PROTECTION" name="Perform sensitivity analyses and lock in the set of model parameters and hyperparameters" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Consider the following to assess and protect parameters and hyperparameters:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Mechanisms in place to protect and lock parameters and hyperparameters.&lt;/li&gt;
 &lt;li&gt;Sensitivity analysis to show how changes in model parameters affect model outputs.&lt;/li&gt;
&lt;/ul&gt;</desc>
          <implementations/>
          <references>
            <reference name="Sensitivity Analysis Library" url="https://github.com/SALib/SALib"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.004">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-REPRESENTATION-ROBUSTNESS" name="Review the representation robustness" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Depending on the model used, randomization and encoding techniques have been proposed to potentially increase the resilience of models to adversarial attacks.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;For instance, assigning different encoded labels for each classifier in an ensemble structure has been shown to improve the classification performance on adversarial attacks. It has also been demonstrated that randomized defenses against adversarial attacks are more efficient than deterministic ones.&lt;/p&gt;
&lt;p&gt;Representational robustness can manifest in decisions as basic as using word2vec embedding in an NLP system versus one-hot vector encodings, which can help combat some blind spot risks.&lt;/p&gt;
&lt;p&gt;On the purely performance side, initialization strategies based on the properties of the statistical distribution of the data on which the models are trained have also been proposed.&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Data Driven Initialization for Machine Learning Classification Models" url="https://repository.urosario.edu.co/server/api/core/bitstreams/bb7de9d7-1410-49f2-a27f-70aaf4824dbe/content"/>
            <reference name="Ensemble of Random Binary Output Encoding for Adversarial Robustness" url="https://ieeexplore.ieee.org/document/8813035"/>
            <reference name="Randomization matters - How to defend against strong adversarial attacks" url="https://dl.acm.org/doi/pdf/10.5555/3524938.3525653"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.004">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-CATASTROPHIC-FORGETTING" name="Use strategies to counter potential catastrophic forgetting" issueId="" issueLink="" platform="" cost="0" risk="39" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Consider using mechanisms to counter potential catastrophic forgetting, such as:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Complement the model with a memory storing previous information, for example, by using memory replay to consolidate internal representations,&lt;/li&gt;
 &lt;li&gt;Train the network while regularizing, and&lt;/li&gt;
 &lt;li&gt;Expand the model if necessary to represent new tasks.&lt;/li&gt;
&lt;/ul&gt;</desc>
          <implementations/>
          <references>
            <reference name="Learn, don’t forget: constructive methods for effective continual learning" url="https://upcommons.upc.edu/bitstream/handle/2117/374177/thesis%20Aitor%20Ganuza.pdf?sequence=1&amp;isAllowed=y"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:48.853">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
      </countermeasures>
      <usecases>
        <usecase ref="general" name="General" desc="" library="IR-Machine-Learning-Artificial-Intelligence">
          <threats>
            <threat ref="T-ML-AI-CATASTROPHIC-INTERFERENCE" name="Models may be subject to catastrophic interference" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;A model is subject to abruptly and drastically forgetting previously learned information upon learning new information. Nowadays, this is more of an issue with autonomous and continuous learning, i.e., without any humans in the loop.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="100" availability="75" easeOfExploitation="50"/>
              <references>
                <reference name="Continual Learning of Natural Language Processing Tasks: A Survey" url="https://arxiv.org/pdf/2211.12701.pdf"/>
                <reference name="Overcoming catastrophic forgetting in neural networks" url="https://www.pnas.org/doi/10.1073/pnas.1611835114"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-CATASTROPHIC-FORGETTING" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-EVALUATION-ISSUES" name="Models may be subject to overfitting, or bad evaluation sets" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;A number of risks must be considered when evaluating models' performance, including:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Overfitting: one of the most prevalent issues in ML/AI systems. This means the model is excellent with training data but does not generalize well.&lt;/li&gt;
 &lt;li&gt;Bad evaluation data: does test data reflect the data expected in production?&lt;/li&gt;
 &lt;li&gt;Bad upstream data: as the expression goes, "garbage in, garbage out". Issues in data upstream make both training and subsequent evaluation difficult.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Note that evaluation might be motivated by various stakeholder perspectives and interests. Always optimizing for the most common static tests would fail to account for the changes in societal, linguistic, and other relevant factors of the ecosystem and the application at hand. Make sure you validate that the metrics you selected properly measure what you are trying to achieve.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="100" availability="50" easeOfExploitation="100"/>
              <references>
                <reference name="Evaluation Gaps in Machine Learning Practice" url="https://arxiv.org/abs/2205.05256"/>
                <reference name="What Is Overfitting?" url="https://www.ibm.com/topics/overfitting"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-EVALUATION-METRICS" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-SCALABILITY-ISSUES" name="Models may be subject to failures due to scalability issues" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;The algorithm may not scale in performance in the real world, e.g., prediction time might be too important compared to the rate at which input data comes in. This might lead to denial of service situations.&lt;/p&gt;
&lt;p&gt;There are a number of challenges in the way of scaling and accelerating ML/AI workloads:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Getting feature engineering right, while especially having in mind how they could be efficiently calculated and continuously monitored in production,&lt;/li&gt;
 &lt;li&gt;The tools and setup, especially in terms of programming language, processing power (single machine or distributed computing), and deployment technologies (e.g., containerization),&amp;nbsp;&lt;/li&gt;
 &lt;li&gt;The maintenance of the model over time as it will need frequent fine-tuning and retraining, which might also be time-consuming and computationally expensive.&lt;br&gt;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;</desc>
              <riskRating confidentiality="50" integrity="50" availability="100" easeOfExploitation="100"/>
              <references>
                <reference name="How to Scale ML Projects – Lessons Learned from Experience" url="https://neptune.ai/blog/how-to-scale-ml-projects"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-ENV-SETUP" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-DATA-REPRESENTATION" name="Adversaries may exploit algorithmic leakage or data representation issues to extract data or attack the model" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Some algorithms are unsuited for processing confidential information. The choice of an algorithm should take into account the nature of the input data as well as how the algorithm represents and store the training data.&lt;/p&gt;
&lt;p&gt;On the other hand, the model initialization and data representation might have an effect on how to defend against strong adversarial attacks.&lt;/p&gt;</desc>
              <riskRating confidentiality="100" integrity="100" availability="50" easeOfExploitation="100"/>
              <references>
                <reference name="Confidential machine learning on untrusted platforms: a survey" url="https://cybersecurity.springeropen.com/articles/10.1186/s42400-021-00092-8"/>
                <reference name="ML01:2023 Input Manipulation Attack" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML01_2023-Input_Manipulation_Attack.html"/>
                <reference name="Machine Learning with Confidential Computing: A Systematization of Knowledge" url="https://arxiv.org/pdf/2208.10134.pdf"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-MODEL-CHOICE" mitigation="50"/>
                <countermeasure ref="C-ML-AI-REPRESENTATION-ROBUSTNESS" mitigation="50"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-MANIPULATE-ONLINE-SYSTEM" name="An adversary may be able to manipulate an online learning system" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Online models continue to learn during operations. This might lead to a drift from the initial purpose or use case. Attackers might be able to lead the system in the wrong direction with specially crafted data, a.k.a data poisoning.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="100" availability="100" easeOfExploitation="75"/>
              <references>
                <reference name="Concealed Data Poisoning Attacks on NLP Models" url="https://aclanthology.org/2021.naacl-main.13.pdf"/>
                <reference name="ML Attack Models: Adversarial Attacks and Data Poisoning Attacks" url="https://arxiv.org/ftp/arxiv/papers/2112/2112.02797.pdf"/>
                <reference name="ML02:2023 Data Poisoning Attack" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML02_2023-Data_Poisoning_Attack.html"/>
                <reference name="Understand model drift" url="https://datatron.com/what-is-model-drift/"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-ROBUST-LEARNING" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-MANIPULATE-PARAMETERS" name="An adversary may be able to manipulate model parameters or hyperparameters" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;ML/AI systems have a number of parameters and hyperparameters that might not be easy to understand or comprehensively test. Setting up and tuning models can be regarded as more of a black art than actual science, which make them subject to attacker influence.&lt;/p&gt;
&lt;p&gt;If an attacker can manipulate either the hyperparameters (used to control the learning process) or the parameters (learned by the model to map the input into the desired output), this will have a devastating effect on the performance and reliability of the ML/AI model.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="100" availability="50" easeOfExploitation="100"/>
              <references/>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-PARAMETERS-PROTECTION" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
          </threats>
        </usecase>
      </usecases>
    </component>
    <component uuid="38e3cbc7-7202-47fd-be9f-4e0f79ad27c9" diagramComponentId="51b9f19f-7d06-4be1-b5f4-5a22f88f36d5" ref="4783c65b-6ce7-4433-bfba-ebaba30b97ff" name="Data Preparation for Learning" desc="" library="IR-Dataflows" parentComponentRef="" componentDefinitionRef="CD-ML-AI-DATA-PREPARATION-FOR-LEARNING">
      <tags/>
      <questions/>
      <trustZones>
        <trustZone ref="private-secured"/>
      </trustZones>
      <assets/>
      <settings/>
      <weaknesses>
        <weakness ref="CWE-494" name="Download of Code Without Integrity Check" state="0" impact="100" issueId="" issueLink="">
          <desc>The product downloads source code or an executable from a remote location and executes the
                        code without sufficiently verifying the origin and integrity of the code.
                    </desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.687">
              <output/>
            </source>
            <references>
              <reference name="CWE-494: Download of Code Without Integrity Check" url="https://cwe.mitre.org/data/definitions/494.html"/>
            </references>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="CWE-359" name="Exposure of Private Information ('Privacy Violation')" state="0" impact="100" issueId="" issueLink="">
          <desc>The software does not properly prevent private data (such as credit card numbers) from being accessed by actors who either (1) are not explicitly authorized to access the data or (2) do not have the implicit consent of the people to which the data is related.</desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.687">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="CWE-284" name="Improper Access Control" state="0" impact="100" issueId="" issueLink="">
          <desc>The software does not restrict or incorrectly restricts access to a resource from an unauthorized actor.</desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.686">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="CWE-89" name="Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')" state="0" impact="100" issueId="" issueLink="">
          <desc>&lt;p&gt;The software constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&lt;br&gt;Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted and executed as SQL instead of ordinary user data. This can be leveraged to alter query logic in order to bypass security checks, or to insert additional statements that modify the back-end database, possibly including execution of system commands. SQL injection has become a common issue with database-driven web sites. The flaw is relatively easily detected, and often easily exploited, and as such, any site or software package with even a minimal user base is likely to be subject to an attempted automated attack of this kind. This flaw depends on the fact that SQL makes no real distinction between the control and data planes.&lt;/p&gt;</desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:47.152">
              <output/>
            </source>
            <references>
              <reference name="CWE-89: Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')" url="https://cwe.mitre.org/data/definitions/89.html"/>
              <reference name="OWASP Testing Guide" url="https://www.owasp.org/index.php/Testing_for_SQL_Injection_(OTG-INPVAL-005)#Standard_SQL_Injection_Testing"/>
            </references>
            <customFields/>
          </test>
        </weakness>
      </weaknesses>
      <countermeasures>
        <countermeasure ref="C-ML-AI-TROJANIZED-VERSIONS" name="Consider possible trojanized versions when acquiring shared models" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Never assume what you are downloading is safe! Malicious actors often use trojanized versions of popular applications in an effort to steal information and compromise systems.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;In this case, you might be fine-tuning a model with possibly a Trojan that includes sneaky behavior that is unanticipated.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Basic practices include carefully checking the source of the model, as well as using cryptographic methods like digital signatures and secure hashes to verify the authenticity and file integrity.&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="How to Verify Downloaded Files" url="https://www.digitalocean.com/community/tutorials/how-to-verify-downloaded-files"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.623">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-PRIVACY-TRAINING-DATA" name="Consider potential privacy issues such as de-identification early on" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Consider how sensitive the input data is and how to protect it early on in the design and model development process. Best practices include:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;De-identification and removal of PII (Personal Identifiable Information) in the training data,&lt;/li&gt;
 &lt;li&gt;Obfuscation, or other techniques that might reduce the exposure of the data later on, including randomized/shuffled data, regularization, or restricting/reducing the training sets.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;One example of data anonymization technique used to protect individual privacy in a dataset is k-anonymization. &amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Example tool: ARX Data Anonymization" url="https://github.com/arx-deidentifier/arx"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.622">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-TEST-PACKAGES" name="Consider the security of the data and model supply chain" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;In addition to inherent modelization risks, there are additional ones associated with security engineering, software security, and supply chain security. These include trojanization or backdooring of base models or other packages.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Understand and maintain the full list of dependencies and harden the verification and patching pipelines. &amp;nbsp;Consider the following:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Verification of digital signatures/ cryptographic hashes,&lt;/li&gt;
 &lt;li&gt;Use secure repos,&lt;/li&gt;
 &lt;li&gt;Keep packages up-to-date,&lt;/li&gt;
 &lt;li&gt;Use segregated virtual environments,&lt;/li&gt;
 &lt;li&gt;Perform regular code reviews of packages and libraries,&lt;/li&gt;
 &lt;li&gt;Use package verification tools, and&lt;/li&gt;
 &lt;li&gt;Educate users.&lt;/li&gt;
&lt;/ul&gt;</desc>
          <implementations/>
          <references>
            <reference name="Secure at every step: What is software supply chain security and why does it matter?" url="https://github.blog/2020-09-02-secure-your-software-supply-chain-and-protect-against-supply-chain-threats-github-blog/"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.623">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-ACCESS-CONTROL" name="Control the access to data, models, and components" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;On the back-end/ server side, there should be strong controls on who has access to the model, its code and data, and related components and materials (such as designs, or test and validation plans).&lt;/p&gt;
&lt;p&gt;On the client side, users of the model should be authenticated before the system allows them access to using the model.&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="A01:2021 – Broken Access Control - How to Prevent?" url="https://owasp.org/Top10/A01_2021-Broken_Access_Control/"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.621">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-PROPER-SLICING" name="Ensure the split data originate from similar data with careful consideration for overlaps" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Make sure that the training, validation, and test sets are 'the same' from data integrity, trustworthiness, and mathematical perspectives.&lt;/p&gt;
&lt;p&gt;Too much overlap or similarity between these sets may lead to overfitting. By contrast, when the evaluation set is too different from the eventual future inputs during operations, it will not measure the true performance.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Use best practices, including when using built-in methods within your favorite ML/AI framework.&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Splits and Slicing (TensorFlow)" url="https://www.tensorflow.org/datasets/splits"/>
            <reference name="Train Test Validation Split: How To &amp; Best Practices" url="https://www.v7labs.com/blog/train-validation-test-set"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.622">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-HARDEN-DATA-PIPELINE" name="Harden the data and feedback pipeline" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Data pipelines are at the heart of building a robust data practice. It includes the processing of the data, triggering model retraining, notifying when feedback/ metric is received/ reached, etc.&lt;/p&gt;
&lt;p&gt;When considering data and feedback collectors in a production environment, there are several aspects to ensure the security of the deployment. Best practices and considerations include:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Secure encrypted end-to-end communication,&lt;/li&gt;
 &lt;li&gt;User identity management,&lt;/li&gt;
 &lt;li&gt;Secure credentials,&lt;/li&gt;
 &lt;li&gt;Audit and log activities,&amp;nbsp;&lt;/li&gt;
 &lt;li&gt;Limit the exposure of the automation stages, e.g., scripts that perform data and feedback transformations,&lt;/li&gt;
 &lt;li&gt;Secure storage, and&lt;/li&gt;
 &lt;li&gt;Secure streaming system and pipeline.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Depending on your use case and infrastructure, you will have several options for data orchestration, including cloud-based options, such as AWS Glue or Azure Data Factory, among others.&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Building a secure data pipeline in AWS Glue" url="https://docs.aws.amazon.com/whitepapers/latest/aws-glue-best-practices-build-secure-data-pipeline/building-a-secure-data-pipeline.html"/>
            <reference name="Security considerations for data movement in Azure Data Factory" url="https://learn.microsoft.com/en-us/azure/data-factory/data-movement-security-considerations"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.622">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-INPUT-MANIPULATION" name="Protect input from manipulation and poisoned data samples" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Always ensure that the training data is validated and verified before it is used to train the model. This involves the following considerations:&amp;nbsp;&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Ensure that the data pipeline is properly managed and secure enough, e.g., versioning and security controls such as access control,&lt;/li&gt;
 &lt;li&gt;Secure storage,&lt;/li&gt;
 &lt;li&gt;Data monitoring, e.g., for changes in the data distribution, and&lt;/li&gt;
 &lt;li&gt;Logging and auditing.&lt;/li&gt;
&lt;/ul&gt;</desc>
          <implementations/>
          <references/>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.622">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="CWE-89-PREPARED" name="Use prepared statements for all database queries" issueId="" issueLink="" platform="" cost="0" risk="28" state="Recommended" owner="fscott-admin" library="IR-Dataflows" source="DATAFLOW_RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Database injection attacks, such as SQLi (SQL Injection) rely on sending tainted client-side data which is used in dynamic SQL queries on the server-side in an unsafe manner. Creating queries by concatenating strings using untrusted data may result in&amp;nbsp;vulnerable code;&amp;nbsp;for example, an attacker appending an 'OR' statement to the customerName parameter in order to bypass checks and retrieve additional data from the database:&lt;/p&gt;&lt;p&gt;
 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;String query = "SELECT user FROM users WHERE name = '"&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&amp;nbsp;+ request.getParameter("customerName")+"'";&lt;/p&gt;&lt;p&gt;The use of prepared statements with carefully controlled and validated input conditions mitigates SQLi and related attacks.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Database queries should always be executed using prepared statements or parameterized queries.&lt;/li&gt;&lt;li&gt;Queries through an Object-Relational mapper should also be treated as tainted input, and again executed using prepared statements to mitigate the threat.&lt;/li&gt;&lt;/ul&gt;</desc>
          <implementations>
            <implementation platform="C#">
              <desc>ClN0cmluZyBxdWVyeSA9ICJTRUxFQ1QgYWNjb3VudF9iYWxhbmNlIEZST00gdXNlcl9kYXRhIFdIRVJFIHVzZXJfbmFtZSA9ID8iOwp0cnkgewogCU9sZURiQ29tbWFuZCBjb21tYW5kID0gbmV3IE9sZURiQ29tbWFuZChxdWVyeSwgY29ubmVjdGlvbik7CiAJY29tbWFuZC5QYXJhbWV0ZXJzLkFkZChuZXcgT2xlRGJQYXJhbWV0ZXIoImN1c3RvbWVyTmFtZSIsIEN1c3RvbWVyTmFtZSBOYW1lLlRleHQpKTsKIAlPbGVEYkRhdGFSZWFkZXIgcmVhZGVyID0gY29tbWFuZC5FeGVjdXRlUmVhZGVyKCk7IAkKIH0gY2F0Y2ggKE9sZURiRXhjZXB0aW9uIHNlKSB7CiAJLy8gZXJyb3IgaGFuZGxpbmcKIH0g</desc>
            </implementation>
          </implementations>
          <references>
            <reference name="OSA SI-01 System And Information Integrity Policy And Procedures" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/39-08_02_SI-01"/>
            <reference name="OSA SI-10 Information Accuracy, Completeness, Validity, And Authenticity" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/48-08_02_SI-10"/>
          </references>
          <standards>
            <standard ref="1" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="12" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="12.2.1" supportedStandardRef="ISO/IEC 27002:2013"/>
            <standard ref="14" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="15" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="16" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="17" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="18" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="19" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="24" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="31" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="5.3.4" supportedStandardRef="owasp-asvs4-level-3"/>
            <standard ref="5.3.4" supportedStandardRef="owasp-asvs4-level-2"/>
            <standard ref="5.3.4" supportedStandardRef="owasp-asvs4-level-1"/>
            <standard ref="6" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="8" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="cwe-top-25" supportedStandardRef="cwe-top-25-dangerous-weaknesses"/>
          </standards>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps>&lt;p&gt;For inputs receiving data that is subsequently added to - or used - in an SQL query:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Ensure SQL meta-characters are identified and properly escaped or encoded.&lt;/li&gt;&lt;li&gt;Data should be used in the form of parameterized SQL queries, rather than dynamically generated queries.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;To test data validation:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Review the code processing potentially tainted user input to SQL queries and confirm the data is executed only in pre-prepared parameterized SQL queries.&lt;/li&gt;&lt;li&gt;Review the acceptable input criteria, and build test cases that deviate from it (invalid characters, lengths, ranges etc.)&lt;/li&gt;&lt;li&gt;Pass invalid input to the application and review error trapping and handling.&lt;/li&gt;&lt;li&gt;Where unexpected exceptions occur, the application may be vulnerable to attack.&lt;/li&gt;&lt;/ol&gt;</steps>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.977">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
      </countermeasures>
      <usecases>
        <usecase ref="general" name="General" desc="" library="IR-Machine-Learning-Artificial-Intelligence">
          <threats>
            <threat ref="T-ML-AI-BACKDOORING" name="An adversary may be able to use a base model or an underlying package as a means of attack ('backdooring')" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;If the model is not built from scratch, consider the possibility that the base models might be trojanized. Adversaries may introduce a backdoor by training the model with poisoned data, or injecting a payload into the model.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;The aim is that the model learns to associate an adversary trigger with the adversary's desired output.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="100" availability="100" easeOfExploitation="100"/>
              <references>
                <reference name="ML06:2023 Corrupted Packages" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML06_2023-Corrupted_Packages.html"/>
              </references>
              <weaknesses>
                <weakness ref="CWE-284">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-ACCESS-CONTROL" mitigation="33"/>
                  </countermeasures>
                </weakness>
                <weakness ref="CWE-494">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-TROJANIZED-VERSIONS" mitigation="34"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="C-ML-AI-ACCESS-CONTROL" mitigation="33"/>
                <countermeasure ref="C-ML-AI-TEST-PACKAGES" mitigation="33"/>
                <countermeasure ref="C-ML-AI-TROJANIZED-VERSIONS" mitigation="34"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-DATA-POISONING" name="An adversary may be able to manipulate training data" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Data is subject to poisoning whereby attackers may intentionally inject malicious data to cause ML/AI training to go awry.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Data engineers should consider what training data in their pipeline an attacker might be able to control and to what extent.&lt;/p&gt;</desc>
              <riskRating confidentiality="75" integrity="100" availability="75" easeOfExploitation="75"/>
              <references>
                <reference name="ML02:2023 Data Poisoning Attack" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML02_2023-Data_Poisoning_Attack.html"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-INPUT-MANIPULATION" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-DATA-SIMILARITY" name="(Di)similarity in training, test, and validation sets can result in overfitting, wrong evaluations, or simply erroneous models" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;If the training, validation, and test sets are not similar from a data integrity, trustworthiness, and mathematical perspective, the model cannot be properly assessed and will ultimately exhibit poor behavior when it is fielded.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="100" availability="50" easeOfExploitation="100"/>
              <references/>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-PROPER-SLICING" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-INPUT-INFERENCE" name="An adversary may be able to conduct input inference attacks later on to get information from training data" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Data engineers should keep in mind that inference attacks are one of the most widespread attacks on ML/AI systems, and plan for it early on. The goals of these attacks are espionage and confidentiality.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;There are a number of inference attack types, including input inference. These attacks extract data from the training dataset, which represents a big issue in terms of privacy and GDPR compliance for instance.&lt;/p&gt;</desc>
              <riskRating confidentiality="100" integrity="50" availability="50" easeOfExploitation="100"/>
              <references>
                <reference name="ML04:2023 Membership Inference Attack" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML04_2023-Membership_Inference_Attack.html"/>
                <reference name="Membership Inference Attacks on Machine Learning: A Survey" url="https://arxiv.org/pdf/2103.07853.pdf"/>
              </references>
              <weaknesses>
                <weakness ref="CWE-359">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-PRIVACY-TRAINING-DATA" mitigation="100"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="C-ML-AI-PRIVACY-TRAINING-DATA" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-MODEL-SKEWING" name="An adversary may be able to slowly retrain the system with malicious introductions" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;In an online system, an attacker might be able to slowly 'retrain' the system to shift its behavior in unintended directions. This might happen if an attacker compromises the feedback loop in the model lifecycle, e.g., MLOps system.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="100" availability="100" easeOfExploitation="75"/>
              <references>
                <reference name="ML08:2023 Model Skewing" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML08_2023-Model_Skewing.html"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-HARDEN-DATA-PIPELINE" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
          </threats>
        </usecase>
        <usecase ref="READ OR POST DATA" name="Read or Post data" desc="" library="IR-Dataflows">
          <threats>
            <threat ref="CAPEC-66" name="Attackers gain unauthorised access to data and/or systems through SQL Injection attacks" state="Expose" source="DATAFLOW_RULES" edited="false" owner="fscott-admin" library="IR-Dataflows" editable="true">
              <desc>&lt;p&gt;Successful SQL Injection attacks could lead to full compromise of the database or to a partial compromise of only the data visible to the application.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Typically, these types of attacks result in unauthorized disclosure of sensitive data, but can also be used to inject spurious data into the database or to drop tables and deny services to legitimate users.&lt;/p&gt;</desc>
              <riskRating confidentiality="100" integrity="100" availability="25" easeOfExploitation="25"/>
              <references/>
              <weaknesses>
                <weakness ref="CWE-89">
                  <countermeasures>
                    <countermeasure ref="CWE-89-PREPARED" mitigation="100"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="CWE-89-PREPARED" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
          </threats>
        </usecase>
      </usecases>
    </component>
    <component uuid="43a35311-e914-4abe-aac7-5981e7457586" diagramComponentId="dbebfd8f-4b75-4b12-8d82-c4a6d3b22137" ref="0f80f801-b5a4-4b8f-8687-bdb7ef3f9799" name="Data Pre-processing" desc="" library="IR-Dataflows" parentComponentRef="" componentDefinitionRef="CD-ML-AI-DATA-PRE-PROCESSING">
      <tags/>
      <questions/>
      <trustZones>
        <trustZone ref="private-secured"/>
      </trustZones>
      <assets/>
      <settings/>
      <weaknesses>
        <weakness ref="CWE-89" name="Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')" state="0" impact="100" issueId="" issueLink="">
          <desc>&lt;p&gt;The software constructs all or part of an SQL command using externally-influenced input from an upstream component, but it does not neutralize or incorrectly neutralizes special elements that could modify the intended SQL command when it is sent to a downstream component.&amp;nbsp;&lt;/p&gt;&lt;p&gt;&amp;nbsp;&amp;nbsp;&lt;br&gt;Without sufficient removal or quoting of SQL syntax in user-controllable inputs, the generated SQL query can cause those inputs to be interpreted and executed as SQL instead of ordinary user data. This can be leveraged to alter query logic in order to bypass security checks, or to insert additional statements that modify the back-end database, possibly including execution of system commands. SQL injection has become a common issue with database-driven web sites. The flaw is relatively easily detected, and often easily exploited, and as such, any site or software package with even a minimal user base is likely to be subject to an attempted automated attack of this kind. This flaw depends on the fact that SQL makes no real distinction between the control and data planes.&lt;/p&gt;</desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:48.730">
              <output/>
            </source>
            <references>
              <reference name="CWE-89: Improper Neutralization of Special Elements used in an SQL Command ('SQL Injection')" url="https://cwe.mitre.org/data/definitions/89.html"/>
              <reference name="OWASP Testing Guide" url="https://www.owasp.org/index.php/Testing_for_SQL_Injection_(OTG-INPVAL-005)#Standard_SQL_Injection_Testing"/>
            </references>
            <customFields/>
          </test>
        </weakness>
      </weaknesses>
      <countermeasures>
        <countermeasure ref="C-ML-AI-IDENTIFY-BIAS" name="Identify potential bias in the data" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Measuring and mitigating the effects of potential biases and discriminatory patterns in the data is an important step to make ML/AI models fairer, especially in socially sensitive decision processes.&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Bias Testing for Generalized Machine Learning Applications" url="https://github.com/pymetrics/audit-ai"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:48.456">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-MAINTAIN-QUALITY" name="Maintain data quality and integrity during data pre-processing" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Maintaining data quality is an essential practice when developing ML/AI models. This allows for a reliable data pipeline, including the data cleaning and feature engineering steps.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Developers might not always be aware of best practices for preparing or transforming the data for a given model type, which can lead to suboptimal representations of input features.&lt;/p&gt;
&lt;p&gt;Many data issues can be identified with some lightweight automation. Some examples include:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Numerical features on widely differing scales,&lt;/li&gt;
 &lt;li&gt;Values assigned special meanings, e.g., to represent missing data,&lt;/li&gt;
 &lt;li&gt;Malformed values of special string types, such as dates.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;This is far from being an exhaustive list. A schema or a set of constraints should be defined for this data validation step, which would depend on the initial data, how it is expected to evolve, and the model type.&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Deequ - Unit Tests for Data" url="https://github.com/awslabs/deequ"/>
            <reference name="The Data Linter: Lightweight, Automated Sanity Checking for ML Data Sets" url="http://learningsys.org/nips17/assets/papers/paper_19.pdf"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:48.457">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-MONITOR-DATA-DRIFT" name="Monitor for data drift, especially for new data and online models" issueId="" issueLink="" platform="" cost="0" risk="44" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Data drift is a type of model drift where the properties of the independent variables change, i.e., input distribution changes.&lt;/p&gt;
&lt;p&gt;There are some basic monitoring measures to consider, including the use of statistical tests over time:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Compare the cumulative distributions of the initial training data and the post-training data, e.g., Kolmogorov-Smirnov test,&amp;nbsp;&lt;/li&gt;
 &lt;li&gt;Measure how a variable distribution changes, e.g., Population stability Index,&amp;nbsp;&lt;/li&gt;
 &lt;li&gt;Compare the feature distribution between the training data and post-training data, e.g., Z-score.&lt;/li&gt;
 &lt;li&gt;Compute the observed values and their mean, compared to a threshold value, e.g., the Page-Hinkley test.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There are libraries and built-in verification and validation methods in ML/AI frameworks, such as TensorFlow for instance.&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Frouros: A Python library for drift detection" url="https://arxiv.org/pdf/2208.06868.pdf"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:48.457">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="CWE-89-PREPARED" name="Use prepared statements for all database queries" issueId="" issueLink="" platform="" cost="0" risk="28" state="Recommended" owner="fscott-admin" library="IR-Dataflows" source="DATAFLOW_RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Database injection attacks, such as SQLi (SQL Injection) rely on sending tainted client-side data which is used in dynamic SQL queries on the server-side in an unsafe manner. Creating queries by concatenating strings using untrusted data may result in&amp;nbsp;vulnerable code;&amp;nbsp;for example, an attacker appending an 'OR' statement to the customerName parameter in order to bypass checks and retrieve additional data from the database:&lt;/p&gt;&lt;p&gt;
 &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp; &amp;nbsp;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;String query = "SELECT user FROM users WHERE name = '"&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&amp;nbsp;+ request.getParameter("customerName")+"'";&lt;/p&gt;&lt;p&gt;The use of prepared statements with carefully controlled and validated input conditions mitigates SQLi and related attacks.&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Database queries should always be executed using prepared statements or parameterized queries.&lt;/li&gt;&lt;li&gt;Queries through an Object-Relational mapper should also be treated as tainted input, and again executed using prepared statements to mitigate the threat.&lt;/li&gt;&lt;/ul&gt;</desc>
          <implementations>
            <implementation platform="C#">
              <desc>ClN0cmluZyBxdWVyeSA9ICJTRUxFQ1QgYWNjb3VudF9iYWxhbmNlIEZST00gdXNlcl9kYXRhIFdIRVJFIHVzZXJfbmFtZSA9ID8iOwp0cnkgewogCU9sZURiQ29tbWFuZCBjb21tYW5kID0gbmV3IE9sZURiQ29tbWFuZChxdWVyeSwgY29ubmVjdGlvbik7CiAJY29tbWFuZC5QYXJhbWV0ZXJzLkFkZChuZXcgT2xlRGJQYXJhbWV0ZXIoImN1c3RvbWVyTmFtZSIsIEN1c3RvbWVyTmFtZSBOYW1lLlRleHQpKTsKIAlPbGVEYkRhdGFSZWFkZXIgcmVhZGVyID0gY29tbWFuZC5FeGVjdXRlUmVhZGVyKCk7IAkKIH0gY2F0Y2ggKE9sZURiRXhjZXB0aW9uIHNlKSB7CiAJLy8gZXJyb3IgaGFuZGxpbmcKIH0g</desc>
            </implementation>
          </implementations>
          <references>
            <reference name="OSA SI-01 System And Information Integrity Policy And Procedures" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/39-08_02_SI-01"/>
            <reference name="OSA SI-10 Information Accuracy, Completeness, Validity, And Authenticity" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/48-08_02_SI-10"/>
          </references>
          <standards>
            <standard ref="1" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="12" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="12.2.1" supportedStandardRef="ISO/IEC 27002:2013"/>
            <standard ref="14" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="15" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="16" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="17" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="18" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="19" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="24" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="31" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="5.3.4" supportedStandardRef="owasp-asvs4-level-3"/>
            <standard ref="5.3.4" supportedStandardRef="owasp-asvs4-level-2"/>
            <standard ref="5.3.4" supportedStandardRef="owasp-asvs4-level-1"/>
            <standard ref="6" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="8" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="cwe-top-25" supportedStandardRef="cwe-top-25-dangerous-weaknesses"/>
          </standards>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps>&lt;p&gt;For inputs receiving data that is subsequently added to - or used - in an SQL query:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Ensure SQL meta-characters are identified and properly escaped or encoded.&lt;/li&gt;&lt;li&gt;Data should be used in the form of parameterized SQL queries, rather than dynamically generated queries.&lt;/li&gt;&lt;/ol&gt;&lt;p&gt;To test data validation:&lt;/p&gt;&lt;ol&gt;&lt;li&gt;Review the code processing potentially tainted user input to SQL queries and confirm the data is executed only in pre-prepared parameterized SQL queries.&lt;/li&gt;&lt;li&gt;Review the acceptable input criteria, and build test cases that deviate from it (invalid characters, lengths, ranges etc.)&lt;/li&gt;&lt;li&gt;Pass invalid input to the application and review error trapping and handling.&lt;/li&gt;&lt;li&gt;Where unexpected exceptions occur, the application may be vulnerable to attack.&lt;/li&gt;&lt;/ol&gt;</steps>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:48.658">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
      </countermeasures>
      <usecases>
        <usecase ref="general" name="General" desc="" library="IR-Machine-Learning-Artificial-Intelligence">
          <threats>
            <threat ref="T-ML-AI-CHANGING-DATA" name="Changing data distribution and properties will affect the performance of the future model" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;How the data will evolve over time is an important aspect to consider early on, which might affect pre-processing steps, as well as the future model behavior.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="50" availability="50" easeOfExploitation="75"/>
              <references/>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-MONITOR-DATA-DRIFT" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-DATA-PRE-PROCESSING" name="Data encoding, normalization, filtering, feature selection, and annotation, may all introduce biases or affect the predictive and generalization qualities of the model" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Raw data needs to be properly pre-processed in order to be processed by a learning algorithm. It will impact both the performance and security of the future ML/AI model. Important aspects to take into consideration are:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Feature discovery/engineering: transforming raw data into features that are easier to interpret and feed into the ML/AI model. This mostly relies on the type of data.&lt;/li&gt;
 &lt;li&gt;Encoding: both data and feature encoding should be carefully considered, e.g., to avoid information loss.&lt;/li&gt;
 &lt;li&gt;Normalization: essentially to use a common scale throughout the data.&lt;/li&gt;
 &lt;li&gt;Partitioning: care must be taken when creating data partitions.&amp;nbsp;&lt;/li&gt;
 &lt;li&gt;Other data manipulation or helpers: such as data filters or randomness, which plays an important role in stochastic systems.&lt;/li&gt;
&lt;/ul&gt;</desc>
              <riskRating confidentiality="100" integrity="100" availability="100" easeOfExploitation="100"/>
              <references>
                <reference name="A Comprehensive Guide to Data Preprocessing" url="https://neptune.ai/blog/data-preprocessing-guide"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-IDENTIFY-BIAS" mitigation="50"/>
                <countermeasure ref="C-ML-AI-MAINTAIN-QUALITY" mitigation="50"/>
              </countermeasures>
              <customFields/>
            </threat>
          </threats>
        </usecase>
        <usecase ref="READ OR POST DATA" name="Read or Post data" desc="" library="IR-Dataflows">
          <threats>
            <threat ref="CAPEC-66" name="Attackers gain unauthorised access to data and/or systems through SQL Injection attacks" state="Expose" source="DATAFLOW_RULES" edited="false" owner="fscott-admin" library="IR-Dataflows" editable="true">
              <desc>&lt;p&gt;Successful SQL Injection attacks could lead to full compromise of the database or to a partial compromise of only the data visible to the application.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;Typically, these types of attacks result in unauthorized disclosure of sensitive data, but can also be used to inject spurious data into the database or to drop tables and deny services to legitimate users.&lt;/p&gt;</desc>
              <riskRating confidentiality="100" integrity="100" availability="25" easeOfExploitation="25"/>
              <references/>
              <weaknesses>
                <weakness ref="CWE-89">
                  <countermeasures>
                    <countermeasure ref="CWE-89-PREPARED" mitigation="100"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="CWE-89-PREPARED" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
          </threats>
        </usecase>
      </usecases>
    </component>
    <component uuid="d84a63c8-38b7-44dd-bb35-dd73822908bf" diagramComponentId="d7a72b37-4a1d-454d-b209-0698e506c1e2" ref="299077f7-64f3-48ed-9f2f-ea92ea0fed27" name="Deploy an Artificial Intelligence model" desc="" library="IR-Machine-Learning-Artificial-Intelligence" parentComponentRef="" componentDefinitionRef="CD-ML-AI-MODEL-DEPLOY">
      <tags/>
      <questions/>
      <trustZones>
        <trustZone ref="private-secured"/>
      </trustZones>
      <assets/>
      <settings/>
      <weaknesses>
        <weakness ref="CWE-300" name="Channel Accessible by Non-Endpoint ('Man-in-the-Middle')" state="0" impact="100" issueId="" issueLink="">
          <desc>&lt;p&gt;The product does not adequately verify the identity of actors at both ends of a communication channel, or does not adequately ensure the integrity of the channel, in a way that allows the channel to be accessed or influenced by an actor that is not an endpoint.&lt;/p&gt;&lt;p&gt;&lt;br&gt;&amp;nbsp;&lt;/p&gt;&lt;p&gt;In order to establish secure communication between two parties, it is often important to adequately verify the identity of entities at each end of the communication channel. Inadequate or inconsistent verification may result in insufficient or incorrect identification of either communicating entity. This can have negative consequences such as misplaced trust in the entity at the other end of the channel. An attacker can leverage this by interposing between the communicating entities and masquerading as the original entity. In the absence of sufficient verification of an identity, an attacker could potentially eavesdrop and modify &amp;nbsp;communication between the original entities.&lt;/p&gt;</desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.761">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="CWE-506" name="Embedded Malicious Code" state="0" impact="100" issueId="" issueLink="">
          <desc>The application contains code that appears to be malicious in
                        nature.
                    </desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.979">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="CWE-359" name="Exposure of Private Information ('Privacy Violation')" state="0" impact="100" issueId="" issueLink="">
          <desc>The software does not properly prevent private data (such as credit card numbers) from being accessed by actors who either (1) are not explicitly authorized to access the data or (2) do not have the implicit consent of the people to which the data is related.</desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.979">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="CWE-799" name="Improper Control of Interaction Frequency" state="0" impact="100" issueId="" issueLink="">
          <desc>The software does not properly limit the number or frequency of interactions that it has with
                        an actor, such as the number of incoming requests.&amp;nbsp;
                        &lt;div&gt;
                        &amp;nbsp;
                        &lt;br /&gt;This can allow the actor to perform actions more frequently than expected. The actor
                        could be a human or an automated process such as a virus or bot. This could be used to cause a
                        denial of service, compromise program logic (such as limiting humans to a single vote), or other
                        consequences. For example, an authentication routine might not limit the number of times an
                        attacker can guess a password. Or, a web site might conduct a poll but only expect humans to
                        vote a maximum of once a day.
                        &lt;/div&gt;
                    </desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:50.187">
              <output/>
            </source>
            <references>
              <reference name="CWE-799: Improper Control of Interaction Frequency" url="https://cwe.mitre.org/data/definitions/799.html"/>
            </references>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="CWE-200" name="Information Exposure" state="0" impact="100" issueId="" issueLink="">
          <desc>An information exposure is the intentional or unintentional disclosure of information to an
                        actor that is not explicitly authorized to have access to that information.
                    </desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.978">
              <output/>
            </source>
            <references>
              <reference name="CWE-200: Information Exposure" url="https://cwe.mitre.org/data/definitions/200.html"/>
            </references>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="CWE-20" name="Input to exported activities, intents or content providers is not validated" state="0" impact="100" issueId="" issueLink="">
          <desc>The product does not validate or incorrectly validates input that can affect the control flow
                        or data flow of a program.

                        When software does not validate input properly, an attacker is able to craft the input in a form
                        that is not expected by the rest of the application. This will lead to parts of the system
                        receiving unintended input, which may result in altered control flow, arbitrary control of a
                        resource, or arbitrary code execution.
                    </desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.545">
              <output/>
            </source>
            <references>
              <reference name="CWE-20: Input to exported activities, intents or content providers is not validated" url="https://cwe.mitre.org/data/definitions/20.html"/>
            </references>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="CWE-693" name="Protection Mechanism Failure" state="0" impact="100" issueId="" issueLink="">
          <desc>The product does not use or incorrectly uses a protection
                        mechanism that provides sufficient defense against directed attacks against the
                        product.
                    </desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:50.187">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </weakness>
      </weaknesses>
      <countermeasures>
        <countermeasure ref="C-ML-AI-MODEL-CHOICE" name="Carefully consider the model choice" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Models can be built using different approaches or algorithms. Overall, choosing the right algorithm depends on several factors, including business considerations, data size, accuracy, training time, parameters, and inference performance, among others.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;In general, the right choice is typically a combination of these aspects. In terms of security, however, the main algorithmic concern is about the nature of the data, where certain algorithms are more vulnerable to data leaks than others.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Depending on your privacy concerns, and/or compliance considerations, e.g., GPDR, &amp;nbsp;consider auditing the data privacy of the models under consideration, e.g., using tools like Privacy Meter.&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Privacy Meter" url="https://github.com/privacytrustlab/ml_privacy_meter"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.932">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-ADVERSARIAL-ATTACKS" name="Consider defenses against adversarial tampering such as adversarial training methods or data preprocessing-based approaches" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Adversarial attacks and defenses are an active and rapidly growing research area. Thousands of research papers have been published in recent years (1200 in 2022!) documenting both attacks and defense techniques. These, however, can be classified into the following categories of defenses:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Model-oriented, including: 
  &lt;ul&gt;
   &lt;li&gt;Robustness improvement for models, e.g., adversarial training methods,&amp;nbsp;&lt;/li&gt;
   &lt;li&gt;Adversarial attack detection, e.g., detect adversarial perturbations of commonly used attack methods, and&lt;/li&gt;
  &lt;/ul&gt;&lt;/li&gt;
 &lt;li&gt;Data-oriented: essentially preprocessing functions: e.g., input cleansing or denoising.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Depending on the domain area and the underlying ML/AI framework used, there are a number of tools that can help assess models against adversarial threats, including adversarial tampering. These include the Adversarial Robustness Toolbox, DeepRobust, or AutoAttack.&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Adversarial Attacks and Defenses in Machine Learning-Powered Networks: A Contemporary Survey" url="https://arxiv.org/abs/2303.06302"/>
            <reference name="Adversarial Robustness Toolbox (ART)" url="https://adversarial-robustness-toolbox.readthedocs.io/en/latest/"/>
            <reference name="Adversarial Robustness Tools" url="https://github.com/EthicalML/awesome-production-machine-learning#adversarial-robustness"/>
            <reference name="AutoAttack" url="https://github.com/fra31/auto-attack"/>
            <reference name="DeepRobust" url="https://github.com/DSE-MSU/DeepRobust"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.486">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-EXPLAINABILITY" name="Consider improving and providing explainability" issueId="" issueLink="" platform="" cost="0" risk="46" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Allowing users to comprehend and trust the results of the model does not only help transparency, but the overall security of the system as well.&amp;nbsp;&lt;br&gt;Certain models are explainable by nature, e.g., decision trees, but many others are not. Making inherently opaque models explainable can be done via a number of techniques, called post-hoc explainability techniques, such as:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Learning to generate text explanations,&lt;/li&gt;
 &lt;li&gt;Visualize the model's behavior vi dimensionality reduction for instance,&lt;/li&gt;
 &lt;li&gt;Segmenting the solution space into less complex solution subspaces,&lt;/li&gt;
 &lt;li&gt;Extracting representative examples of the inner relationships and correlations found by the model,&lt;/li&gt;
 &lt;li&gt;Compute the relevance score of features which would unveil the importance granted by the model to each of the variables.&lt;/li&gt;
&lt;/ul&gt;</desc>
          <implementations/>
          <references>
            <reference name="&quot;Why Should I Trust You?&quot;: Explaining the Predictions of Any Classifier" url="https://arxiv.org/abs/1602.04938"/>
            <reference name="Explainable Artificial Intelligence (XAI): Concepts, Taxonomies, Opportunities and Challenges toward Responsible AI" url="https://arxiv.org/abs/1910.10045"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.687">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-PROTECT-DEPLOYMENT" name="Consider possible defenses at the deployment stage to secure the outputs or the model" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;There is an increasing demand for privacy-preserving techniques in ML/AI, at all levels, from training to inference. While there is no silver bullet when it comes to achieving privacy in ML/AI, certain mechanisms can be used to further secure the outputs or the model, including:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Obfuscation, e.g., especially for shipped/on-device models, as opposed to remote or cloud-hosted models,&lt;/li&gt;
 &lt;li&gt;Encryption, e.g., homomorphic encryption where training, inference, and output can be based on/generate encrypted data,&lt;/li&gt;
 &lt;li&gt;Decentralizes the data, such as with federated machine learning that strengthens both input and output privacy,&lt;/li&gt;
 &lt;li&gt;Added noise or the use of regularization which has been shown to strengthen anonymization without compromising model accuracy.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A number of ML/AI tools have now integrated privacy-preserving mechanisms such as Tensorflow or PyTorch. There are also tools dedicated to model deployment that can offer security and monitoring features, such as Seldon Core or Evidently.&lt;/p&gt;
&lt;p&gt;In addition to ML/AI-specific solutions, data engineering solutions can also contribute to supporting output and model privacy. These include strong authentication, authorization, and proxy/token-based access to inference.&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Model Serving and Monitoring" url="https://github.com/EthicalML/awesome-production-machine-learning#model-serving-and-monitoring"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:50.138">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-TROJANIZED-VERSIONS" name="Consider possible trojanized versions when acquiring shared models" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Never assume what you are downloading is safe! Malicious actors often use trojanized versions of popular applications in an effort to steal information and compromise systems.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;In this case, you might be fine-tuning a model with possibly a Trojan that includes sneaky behavior that is unanticipated.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Basic practices include carefully checking the source of the model, as well as using cryptographic methods like digital signatures and secure hashes to verify the authenticity and file integrity.&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="How to Verify Downloaded Files" url="https://www.digitalocean.com/community/tutorials/how-to-verify-downloaded-files"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.933">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-PRIVACY-PRESERVING" name="Consider privacy-preserving techniques and strategies" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Privacy-preserving or privacy-enhancing techniques are designed to prevent the exposure of the model and data.&lt;/p&gt;
&lt;p&gt;First, there are two types of access an adversary might have; 1) White Box, where information about the model or its original training data is available, and 2) Black Box, where there is no knowledge about the model or data. In this case, attackers would explore the model by providing carefully crafted inputs and observing outputs.&lt;/p&gt;
&lt;p&gt;There are a number of attack types, which involve inferring information about the model and data, e.g., model inversion or membership inference.&lt;/p&gt;
&lt;p&gt;Some of the defenses aiming to protect confidentiality and privacy in ML/AI-based systems are:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Differential Privacy: one of the state-of-the-art methods for providing access to information free from inferences. There are two main privacy-preserving model-training approaches in the literature: 1) using noisy stochastic gradient descent (noisy SGD), and 2) Private Aggregation of Teacher Ensembles (PATE).&lt;/li&gt;
 &lt;li&gt;Cryptography; e.g., privacy-enhancing tools based on secure multi-party computation (SMC) and fully homomorphic encryption (FHE) have been proposed to securely train supervised machine learning models.&lt;/li&gt;
 &lt;li&gt;Trusted environments (engineering/setup solution).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other techniques in this space include dropout, weight normalization, dimensionality reduction, selective gradient sharing, etc.&lt;br&gt;&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="An Overview of Privacy in Machine Learning" url="https://arxiv.org/pdf/2005.08679.pdf"/>
            <reference name="ML03:2023 Model Inversion Attack" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML03_2023-Model_Inversion_Attack.html"/>
            <reference name="ML04:2023 Membership Inference Attack" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML04_2023-Membership_Inference_Attack.html"/>
            <reference name="ML05:2023 Model Stealing" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML05_2023-Model_Stealing.html"/>
            <reference name="Privacy Preserving ML" url="https://github.com/EthicalML/awesome-production-machine-learning#privacy-preserving-ml"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.932">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-ACCESS-CONTROL" name="Control the access to data, models, and components" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;On the back-end/ server side, there should be strong controls on who has access to the model, its code and data, and related components and materials (such as designs, or test and validation plans).&lt;/p&gt;
&lt;p&gt;On the client side, users of the model should be authenticated before the system allows them access to using the model.&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="A01:2021 – Broken Access Control - How to Prevent?" url="https://owasp.org/Top10/A01_2021-Broken_Access_Control/"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.486">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-HARDEN-DATA-PIPELINE" name="Harden the data and feedback pipeline" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Data pipelines are at the heart of building a robust data practice. It includes the processing of the data, triggering model retraining, notifying when feedback/ metric is received/ reached, etc.&lt;/p&gt;
&lt;p&gt;When considering data and feedback collectors in a production environment, there are several aspects to ensure the security of the deployment. Best practices and considerations include:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Secure encrypted end-to-end communication,&lt;/li&gt;
 &lt;li&gt;User identity management,&lt;/li&gt;
 &lt;li&gt;Secure credentials,&lt;/li&gt;
 &lt;li&gt;Audit and log activities,&amp;nbsp;&lt;/li&gt;
 &lt;li&gt;Limit the exposure of the automation stages, e.g., scripts that perform data and feedback transformations,&lt;/li&gt;
 &lt;li&gt;Secure storage, and&lt;/li&gt;
 &lt;li&gt;Secure streaming system and pipeline.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Depending on your use case and infrastructure, you will have several options for data orchestration, including cloud-based options, such as AWS Glue or Azure Data Factory, among others.&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Building a secure data pipeline in AWS Glue" url="https://docs.aws.amazon.com/whitepapers/latest/aws-glue-best-practices-build-secure-data-pipeline/building-a-secure-data-pipeline.html"/>
            <reference name="Security considerations for data movement in Azure Data Factory" url="https://learn.microsoft.com/en-us/azure/data-factory/data-movement-security-considerations"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.688">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-KEEP-LOGS" name="Keep a history of queries to the model" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Keeping a history of queries to the system is highly recommended. Logging user interaction with the model is not only a powerful tool for user research (behavior study, usability, system metrics analysis, etc.) but will also allow reviews to make sure the system is not unintentionally leaking confidential information.&amp;nbsp;&lt;br&gt;Care must be taken on how you store the logs! Logging can itself introduce a security/privacy risk.&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Top seven logging and monitoring best practices" url="https://www.synopsys.com/blogs/software-security/logging-and-monitoring-best-practices/"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.930">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-RECOVERY" name="Models in production should be able to recover to a known state, reset, or be 'cleaned' if necessary" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Make sure to have a failover mechanism and a business continuity plan. Best practices include:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Manage your code base locally, backed by a remote repository,&lt;/li&gt;
 &lt;li&gt;Backup all data, including generated versioned models, important notebooks, and any configuration files,&lt;/li&gt;
 &lt;li&gt;Automate (script out) deployments, and&lt;/li&gt;
 &lt;li&gt;Make use of geo-replication for data storage/accounts if you are using your own deployment setup on a Cloud platform.&lt;/li&gt;
&lt;/ul&gt;</desc>
          <implementations/>
          <references>
            <reference name="Disaster Recovery: Best Practices" url="https://www.cisco.com/en/US/technologies/collateral/tk869/tk769/white_paper_c11-453495.pdf"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:50.138">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-MONITOR-DRIFT" name="Monitor for model drift by continuously tracking the model's performance metrics in production" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Model drift detection is an active research field. New methods are continuously being proposed in the literature. There are, however, some basic monitoring measures to consider, including:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Continually monitoring the accuracy of the model when possible, e.g., using F-score,&lt;/li&gt;
 &lt;li&gt;Use statistical tests to: 
  &lt;ul&gt;
   &lt;li&gt;Compare the cumulative distributions of the training data and the post-training data, e.g., Kolmogorov-Smirnov test,&amp;nbsp;&lt;/li&gt;
   &lt;li&gt;Measure how a variable distribution has changed over time, e.g., Population stability Index, or&amp;nbsp;&lt;/li&gt;
   &lt;li&gt;Compare the feature distribution between the training and live data, e.g., Z-score.&lt;/li&gt;
  &lt;/ul&gt;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Addressing model drift would then involve setting up the right schedule to retrain your model.&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Drift detection in the literature - sciencedirect" url="https://www.sciencedirect.com/topics/computer-science/concept-drift-detection"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.688">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-INPUT-VALIDATION" name="Perform input validation by applying anomaly detection" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;This can be considered as part of the data preprocessing functions already mentioned in the overall defenses against adversarial tampering, except that it would attempt to validate the input without further processing.&lt;/p&gt;
&lt;p&gt;Anomaly detection can be a powerful defense mechanism in this case. It would allow to model the distribution of the clean data, i.e., expected values or patterns, and consequently allows to (potentially) reject anomalous inputs that are likely to be malicious.&lt;br&gt;&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Anomaly Detection Learning Resources" url="https://github.com/yzhao062/anomaly-detection-resources"/>
            <reference name="ML01:2023 Input Manipulation Attack" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML01_2023-Input_Manipulation_Attack.html"/>
            <reference name="Outlier and Anomaly Detection" url="https://github.com/EthicalML/awesome-production-machine-learning#outlier-and-anomaly-detection"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.487">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-PROTECT-SHARED" name="Protect data and IP when sharing or shipping models" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Models sharing or shipping can be treated the same as software delivery and protection:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Consider training data as traditional source code.&lt;/li&gt;
 &lt;li&gt;Consider the model files as compiled executables.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A number of mechanisms/options can be used, including:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Encryption, e.g., for on-device models, only unpacking in memory when the model is running,&lt;/li&gt;
 &lt;li&gt;Obfuscation, e.g., adding random noise to existing samples, or augmenting the dataset with new samples,&amp;nbsp;&lt;/li&gt;
 &lt;li&gt;Copyright traps, or&lt;/li&gt;
 &lt;li&gt;Confidential computing, e.g., built-in hardware-based security.&lt;/li&gt;
&lt;/ul&gt;</desc>
          <implementations/>
          <references>
            <reference name="ML Model Watermarking" url="https://github.com/SAP/ml-model-watermarking"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.933">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-PROTECT-DATA-STREAMS" name="Protect the data stream and consider multiple independent sensors if possible" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;By design, input data will be structured and pre-processed similarly to the training data to be ingested by the model and make predictions. Protecting this input data stream and processing is of prime importance.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Options include using a multi-modal input stream/system which will be harder to completely control. One way to carry this out might be to use multiple sensors that are not similarly designed or that do not have the same engineering failure conditions.&lt;/p&gt;</desc>
          <implementations/>
          <references/>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.487">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-RESTRICT-QUERIES" name="Restrict the number of model queries" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Define a resource access policy especially to limit the size and frequency of user queries. There are different ways that you can approach rate limiting, especially for APIs. This would involve:&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Limits focused on controlling traffic from individual sources and making sure that users are staying within their prescribed limits, and&lt;/li&gt;&lt;li&gt;Global limits in relation to all the traffic coming into the system from all sources, and ensure that the overall rate limit (e.g., tested capacity of the system or what makes sense with regard to the model's task) is not exceeded.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;Ideally, both are used to secure the system, including against denial of service attacks.&lt;/p&gt;</desc>
          <implementations/>
          <references/>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:50.138">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-SECURE-STORAGE" name="Secure data storage" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Take measures to protect storage resources and the data stored on them, both on-premises and externally, e.g., on the cloud. This involves:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Access control to regulate who can use the data. The environment should identify, verify, and authorize a user before they access the data. Additional policies can dictate where the data can be copied and under which conditions.&amp;nbsp;&lt;/li&gt;
 &lt;li&gt;Storage encryption, which can be built in by the service provider. Otherwise, on-premise encryption should be considered.&lt;/li&gt;
 &lt;li&gt;Isolate data from other systems and networks.&lt;/li&gt;
 &lt;li&gt;Immutable storage as an extra protection layer that can help prevent bad actors from tampering with data.&lt;br&gt;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;</desc>
          <implementations/>
          <references/>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:50.139">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-ISOLATE-CONTROL" name="Sufficiently isolate production models, with proper security controls and monitoring" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;ML/AI models are exposed by nature. The following should be considered when deploying models to production:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Host segregation to allow for more efficient monitoring,&lt;/li&gt;
 &lt;li&gt;Host protections, e.g., using endpoint security solutions,&lt;/li&gt;
 &lt;li&gt;Secure configuration and proper maintenance, e.g., patch/vulnerability management,&lt;/li&gt;
 &lt;li&gt;Proper access control, and&amp;nbsp;&lt;/li&gt;
 &lt;li&gt;Minimal exposure of services.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;On the other hand, the user should have a clear understanding of his/her responsibility when using third parties hosted MLOps tools / MLaaS (Machine Learning as a Service) offerings to develop and deploy ML/AI models.&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Model Serving and Monitoring" url="https://github.com/EthicalML/awesome-production-machine-learning#model-serving-and-monitoring"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:50.137">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
      </countermeasures>
      <usecases>
        <usecase ref="general" name="General" desc="" library="IR-Machine-Learning-Artificial-Intelligence">
          <threats>
            <threat ref="T-ML-AI-ADVERSARIAL-EXAMPLES" name="An adversary may input adversarial examples to the deployed model" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Probably the most prevalent attack type on ML/AI models. &amp;nbsp;Adversarial examples are specially crafted inputs with the purpose of confusing a model, resulting essentially in misclassification. Making false predictions as a result of these perturbations will render the model useless.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;In the age of large language models, this includes prompt injection, where input text may trick the system into ignoring instructions, performing unintended actions, or even executing malicious code or instructions.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="100" availability="100" easeOfExploitation="100"/>
              <references>
                <reference name="LLM01: Prompt Injection (LLM OWASP Top 10)" url="https://owasp.org/www-project-top-10-for-large-language-model-applications/assets/PDF/OWASP-Top-10-for-LLMs-2023-v1_0_1.pdf"/>
                <reference name="ML01:2023 Input Manipulation Attack" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML01_2023-Input_Manipulation_Attack.html"/>
              </references>
              <weaknesses>
                <weakness ref="CWE-20">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-INPUT-VALIDATION" mitigation="33"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="C-ML-AI-ACCESS-CONTROL" mitigation="34"/>
                <countermeasure ref="C-ML-AI-ADVERSARIAL-ATTACKS" mitigation="33"/>
                <countermeasure ref="C-ML-AI-INPUT-VALIDATION" mitigation="33"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-INPUT-STREAM" name="An adversary may take control of the input stream/pipeline" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;An ML/AI system that takes its input data from open and potentially unprotected sources may be purposefully manipulated or completely controlled by an attacker. If the data is used in retraining, the model will take in the poisoned/manipulated data and produce incorrect predictions.&lt;/p&gt;</desc>
              <riskRating confidentiality="75" integrity="100" availability="75" easeOfExploitation="75"/>
              <references>
                <reference name="ML02:2023 Data Poisoning Attack" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML02_2023-Data_Poisoning_Attack.html"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-PROTECT-DATA-STREAMS" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-MITM-MODEL-OUTPUT" name="An adversary may be able to interpose between the model output and the intended receiver" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Output integrity might be hard to verify given the black-box model most data-driven systems operate in. An attacker might be able to execute a man-in-the-middle type of attack and hijack the model output without alerting the end user or even the model publisher monitoring the system&lt;/p&gt;</desc>
              <riskRating confidentiality="75" integrity="100" availability="75" easeOfExploitation="75"/>
              <references>
                <reference name="ML09:2023 Output Integrity Attack" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML09_2023-Output_Integrity_Attack.html"/>
              </references>
              <weaknesses>
                <weakness ref="CWE-300">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-HARDEN-DATA-PIPELINE" mitigation="100"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="C-ML-AI-HARDEN-DATA-PIPELINE" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-MODEL-DRIFT" name="Model drift may occur when the data changes becoming unstable and producing unreliable predictions" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Model drift is one of the most prominent issues in ML/AI deployments. It represents the decay of the model's predictive power as a result of the changes in real-world conditions or attacks introducing malicious shifts in the distribution of training data.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="100" availability="50" easeOfExploitation="100"/>
              <references>
                <reference name="Understand model drift" url="https://datatron.com/what-is-model-drift/"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-MONITOR-DRIFT" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-OPACITY" name="Opaque systems may less accepted and make it easier for adversaries to inject erroneous outputs" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Interpretability and transparency might not be the main concerns initially, given that accuracy is what the data engineer would look for the most. This, however, makes attacks a much easier endeavor, since it will be harder to discern if anything has gone wrong. Additionally, the lack of transparency and/or the impossibility of interpretable outputs by humans is likely to increase skepticism about ML/AI systems. &amp;nbsp;&lt;/p&gt;</desc>
              <riskRating confidentiality="25" integrity="75" availability="25" easeOfExploitation="75"/>
              <references>
                <reference name="Artificial intelligence, transparency, and public decision-making" url="https://link.springer.com/article/10.1007/s00146-020-00960-w"/>
                <reference name="What is explainable AI" url="https://www.ibm.com/watson/explainable-ai"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-EXPLAINABILITY" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-PRIVACY" name="An adversary may be able to reveal sensitive information available in the model or the data used to build it" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;ML/AI models incorporate information and representations of the data ingested during training, some of which might be sensitive.&lt;br&gt;Algorithm choice might be a key factor to limit the risk, e.g., using a non-parametric method like k-nearest neighbors in a situation with sensitive medical records is probably a bad idea since exemplars will have to be stored on production hosts.&amp;nbsp;&lt;br&gt;In general, however, remember that any model is susceptible to memorizing information present within the training sets.&amp;nbsp;&lt;br&gt;You should also be aware of what outputs the model is producing and how it may reveal sensitive aspects of the training data, e.g., using monitoring.&lt;/p&gt;</desc>
              <riskRating confidentiality="100" integrity="50" availability="50" easeOfExploitation="75"/>
              <references>
                <reference name="An Overview of Privacy in Machine Learning" url="https://arxiv.org/pdf/2005.08679.pdf"/>
              </references>
              <weaknesses>
                <weakness ref="CWE-359">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-KEEP-LOGS" mitigation="33"/>
                    <countermeasure ref="C-ML-AI-MODEL-CHOICE" mitigation="33"/>
                    <countermeasure ref="C-ML-AI-PRIVACY-PRESERVING" mitigation="34"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="C-ML-AI-KEEP-LOGS" mitigation="33"/>
                <countermeasure ref="C-ML-AI-MODEL-CHOICE" mitigation="33"/>
                <countermeasure ref="C-ML-AI-PRIVACY-PRESERVING" mitigation="34"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-SHARING-TRANSFER" name="An adversary may take advantage of model sharing and/or transfer" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Models storage and transfer may lead to the possibility that what is being reused may be a Trojaned (or otherwise damaged) version of the model. This risk is more pronounced in the case of using ready-to-use models.&lt;/p&gt;</desc>
              <riskRating confidentiality="100" integrity="100" availability="50" easeOfExploitation="75"/>
              <references>
                <reference name="ML06:2023 Corrupted Packages" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML06_2023-Corrupted_Packages.html"/>
              </references>
              <weaknesses>
                <weakness ref="CWE-200">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-PROTECT-SHARED" mitigation="50"/>
                  </countermeasures>
                </weakness>
                <weakness ref="CWE-506">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-TROJANIZED-VERSIONS" mitigation="50"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="C-ML-AI-PROTECT-SHARED" mitigation="50"/>
                <countermeasure ref="C-ML-AI-TROJANIZED-VERSIONS" mitigation="50"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-ERRONEOUS-MODEL" name="An adversary may be able to manipulate online production models" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Online production systems are subject to manipulation or complete shutdown by an ill-intentioned actor, or any other error or failure. Manipulation of online models might involve data provenance and integrity that results in model drift.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="50" availability="100" easeOfExploitation="75"/>
              <references>
                <reference name="Plan for business continuity and resilience" url="https://www.ready.gov/it-disaster-recovery-plan"/>
                <reference name="Understand model drift" url="https://datatron.com/what-is-model-drift/"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-MONITOR-DRIFT" mitigation="50"/>
                <countermeasure ref="C-ML-AI-RECOVERY" mitigation="50"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-EXTRACTION" name="An adversary may be able to conduct input inference or model inversion/extraction attacks" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Extraction attacks are quite common and are one of the simplest forms of privacy leakage for ML/AI models. Also called 'inference attacks' or 'model inversion'.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;An attacker might be able to extract details of the data corpus the model was trained on, the inputs (e.g., inferring sensitive data features/input from publicly available results of a model), or the model itself by uncovering its behavior and parameters. This might have serious privacy implications.&lt;/p&gt;</desc>
              <riskRating confidentiality="100" integrity="50" availability="50" easeOfExploitation="100"/>
              <references>
                <reference name="ML03:2023 Model Inversion Attack" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML03_2023-Model_Inversion_Attack.html"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-PROTECT-DEPLOYMENT" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-GARBAGE-DATA" name="Spamming the system with garbage data" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Adversaries may spam the system with data to produce output noise. This may also affect the model behavior in the long run.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="100" availability="100" easeOfExploitation="100"/>
              <references/>
              <weaknesses>
                <weakness ref="CWE-799">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-RESTRICT-QUERIES" mitigation="100"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="C-ML-AI-RESTRICT-QUERIES" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-HOST-COMPROMISE" name="An adversary may be able to exploit the host of the ML/AI system" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Where and how is the model deployed and operating? Attackers might target host servers that are not sufficiently isolated and protected.&lt;br&gt;Note that there is a plethora of tools and platforms (MLOps tools) that are used for model development, deployment, and monitoring (cf. examples in the below reference). This will involve a shared responsibility model with the provider.&lt;/p&gt;</desc>
              <riskRating confidentiality="100" integrity="100" availability="100" easeOfExploitation="75"/>
              <references>
                <reference name="MLOps landscape" url="https://neptune.ai/blog/mlops-tools-platforms-landscape"/>
              </references>
              <weaknesses>
                <weakness ref="CWE-693">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-ISOLATE-CONTROL" mitigation="100"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="C-ML-AI-ISOLATE-CONTROL" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-MODEL-STEALING" name="An adversary may be able to steal the model" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;An attacker may be able to gain access to the model itself or the model's parameters, including via an environment breach. The attacker can use this information to recreate the model.&lt;/p&gt;</desc>
              <riskRating confidentiality="100" integrity="50" availability="50" easeOfExploitation="75"/>
              <references>
                <reference name="ML05:2023 Model Stealing" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML05_2023-Model_Stealing.html"/>
              </references>
              <weaknesses>
                <weakness ref="CWE-693">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-ACCESS-CONTROL" mitigation="50"/>
                    <countermeasure ref="C-ML-AI-SECURE-STORAGE" mitigation="50"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="C-ML-AI-ACCESS-CONTROL" mitigation="50"/>
                <countermeasure ref="C-ML-AI-SECURE-STORAGE" mitigation="50"/>
              </countermeasures>
              <customFields/>
            </threat>
          </threats>
        </usecase>
      </usecases>
    </component>
    <component uuid="5efd38c9-1264-4884-a51d-48cf67ceda2c" diagramComponentId="933dca8b-9277-4a0a-97ea-5d4fd542fbde" ref="97348d1e-36ad-4b51-85fd-e5ed8ed6907b" name="Other data store" desc="" library="CS-Default" parentComponentRef="" componentDefinitionRef="other-database">
      <tags/>
      <questions/>
      <trustZones>
        <trustZone ref="private-secured"/>
      </trustZones>
      <assets/>
      <settings/>
      <weaknesses>
        <weakness ref="CWE-250" name="Execution with Unnecessary Privileges" state="0" impact="100" issueId="" issueLink="">
          <desc>The software performs an operation at a privilege level that is higher than the minimum level
                        required, which creates new weaknesses or amplifies the consequences of other weaknesses.

                        New weaknesses can be exposed because running with extra privileges, such as root or
                        Administrator, can disable the normal security checks being performed by the operating system or
                        surrounding environment. Other pre-existing weaknesses can turn into security vulnerabilities if
                        they occur while operating at raised privileges. Privilege management functions can behave in
                        some less-than-obvious ways, and they have different quirks on different platforms. These
                        inconsistencies are particularly pronounced if you are transitioning from one non-root user to
                        another. Signal handlers and spawned processes run at the privilege of the owning process, so if
                        a process is running as root when a signal fires or a sub-process is executed, the signal
                        handler or sub-process will operate with root privileges.
                    </desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:48.191">
              <output/>
            </source>
            <references>
              <reference name="CWE-250: Execution with Unnecessary Privileges" url="https://cwe.mitre.org/data/definitions/250.html"/>
            </references>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="CWE-284" name="Improper Access Control" state="0" impact="100" issueId="" issueLink="">
          <desc>The software does not restrict or incorrectly restricts access to a resource from an unauthorized actor.</desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:47.892">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="CWE-306" name="Missing Authentication for Critical Function" state="0" impact="100" issueId="" issueLink="">
          <desc>The software does not perform any authentication for functionality that requires a provable user identity or consumes a significant amount of resources.</desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:47.893">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="EU-GDPR-MISSING-ENCRYPTION-SENSITIVE-DATA" name="Missing Encryption of Sensitive Data" state="0" impact="100" issueId="" issueLink="">
          <desc>&lt;ul&gt; 
 &lt;li&gt;Weak encryption algorithms&amp;nbsp;&lt;/li&gt; 
 &lt;li&gt;Loss of encryption keys&amp;nbsp;&lt;/li&gt; 
 &lt;li&gt;Compromised encryption keys&amp;nbsp;&lt;/li&gt; 
 &lt;li&gt;Revoked keys are still active (bad key lifecycle management)&lt;/li&gt; 
&lt;/ul&gt;</desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:47.893">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </weakness>
      </weaknesses>
      <countermeasures>
        <countermeasure ref="RESTRICT-ACCESS-DATABASE" name="Access the data store from an account with the least privileges necessary" issueId="" issueLink="" platform="" cost="1" risk="28" state="Recommended" owner="fscott-admin" library="CS-Default" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>	&lt;p&gt;Use an account with only the minimum set of permissions required to access the data store. The account should not be able to perform operations that are not explicitly required by the component that performs these operations. For example, if a web application needs to read data from certain tables and insert and update data from others, then a database account with only those specific permissions should be used by the application server.&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="OSA AC-03 Access Enforcement" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/25-08_02_AC-03"/>
            <reference name="OSA AC-06 Least Privilege" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/28-08_02_AC-06"/>
            <reference name="OWASP Juice Shop: Injection" url="https://pwning.owasp-juice.shop/part2/injection.html"/>
            <reference name="[C3] OWASP Proactive Controls" url="https://www.owasp.org/index.php/OWASP_Proactive_Controls"/>
          </references>
          <standards>
            <standard ref="1.2.1" supportedStandardRef="owasp-asvs4-level-3"/>
            <standard ref="1.2.1" supportedStandardRef="owasp-asvs4-level-2"/>
            <standard ref="13.2.1" supportedStandardRef="ISO/IEC 27002:2013"/>
            <standard ref="5.14" supportedStandardRef="iso-27002-2022"/>
            <standard ref="A03:2021-Injection" supportedStandardRef="owasp-top-10-2021"/>
            <standard ref="A1:2017-Injection" supportedStandardRef="owasp-top-10-2017"/>
          </standards>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps>&lt;p&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Identify the user account used by external components to access this data store.&lt;/li&gt; 
 &lt;li&gt;Check which permissions are assigned to this user account.&lt;/li&gt; 
 &lt;li&gt;Verify that they are the minimum set of permissions necessary and no unnecessary permissions are assigned.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;/p&gt;</steps>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:48.088">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="PATCH-SERVICE" name="Apply required security patches to the service" issueId="" issueLink="" platform="" cost="1" risk="56" state="Recommended" owner="fscott-admin" library="CS-Default" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Vendors and other maintainers of software release patches in response to security flaws and other bugs in their products.&amp;nbsp; The longer a system is exposed with a known security vulnerability, the easier to compromise it. &amp;nbsp;As the exploit enters the public domain, they get included in automated exploitation suites like Metasploit and a wider less skilled miscreant is able to leverage them.&amp;nbsp; &amp;nbsp;&lt;/p&gt;&lt;ul&gt;&lt;li&gt;Apply patches and other software updates in a timely manner to prevent unexpected failures or exploitation.&lt;/li&gt;&lt;li&gt;Clearly define an approach for testing and applying patches, in particular security patches, with expected timescales. &amp;nbsp;There is often a small window between the release of a patch, and potentially malicious actors reverse-engineering the patch to identify and exploit the flaw.&lt;/li&gt;&lt;li&gt;Use a threat intelligence, vulnerability scanning, or other alerting services to ensure the project team is promptly aware of issues within the project or its components.&lt;/li&gt;&lt;/ul&gt;&lt;p&gt;&lt;br&gt;&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="OSA CM-01 Configuration Management Policy And Procedures" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/152-08_02_CM-01"/>
          </references>
          <standards>
            <standard ref="1" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="10" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="14.1.3" supportedStandardRef="owasp-asvs4-level-3"/>
            <standard ref="14.1.3" supportedStandardRef="owasp-asvs4-level-2"/>
            <standard ref="17" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="18" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="19" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="3" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="31" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="5" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="API1:2023-Broken Object Level Authorization" supportedStandardRef="owasp-api-security-top-10"/>
          </standards>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps>&lt;br /&gt;
                            &lt;ol&gt;
                            &lt;li&gt;Check with the software or service vendor whether security vulnerabilities and
                            their associated patches are available for the version deployed.&lt;/li&gt;
                            &lt;li&gt;Evaluate the criticality of the vulnerability and schedule a fix accordingly.&lt;/li&gt;
                            &lt;/ol&gt;
                        </steps>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:47.740">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="EU-GDPR-ENCRYPT-PERSONAL-DATA" name="Encrypt personal data" issueId="" issueLink="" platform="" cost="0" risk="27" state="Recommended" owner="fscott-admin" library="CS-Default" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>Implement encryption at rest (see guidance below) or give a risk-based explanation as to why encryption was not implemented.&lt;br /&gt;&lt;br /&gt;Use well-known encryption libraries, taking into account the data use, and do not invent your own. 
          &lt;br /&gt;
              &lt;ul&gt;
              &lt;li&gt;personal data must be encrypted&lt;/li&gt;
              &lt;li&gt;data that is not used by the application (e.g. passwords, ...) should be hashed so they cannot be recovered easily&lt;/li&gt;
              &lt;/ul&gt;
          </desc>
          <implementations/>
          <references>
            <reference name="EU GDPR - EU Data Protection" url="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:02016R0679-20160504"/>
            <reference name="OWASP Proactive controls [C8]" url="https://www.owasp.org/images/b/bc/OWASP_Top_10_Proactive_Controls_V3.pdf"/>
          </references>
          <standards>
            <standard ref="2.4.12.2" supportedStandardRef="iotsf-class-1"/>
            <standard ref="2.4.12.2" supportedStandardRef="iotsf-class-2"/>
            <standard ref="8.3.7" supportedStandardRef="owasp-asvs4-level-2"/>
            <standard ref="8.3.7" supportedStandardRef="owasp-asvs4-level-3"/>
            <standard ref="Art.32" supportedStandardRef="EU-GDPR"/>
          </standards>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps>&lt;div&gt;
                &lt;ol&gt;
                &lt;li&gt;Verify that users' personal data processed by the application is encrypted at rest (e.g. databases).&lt;/li&gt;
                &lt;li&gt;If encryption is not being leveraged, verify that there is documentation outlining the reasoning for that decision.&lt;/li&gt;
                &lt;/ol&gt;
                &lt;/div&gt;
            </steps>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:47.739">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="CWE-306-SERVICE" name="Require authentication before presenting restricted data" issueId="" issueLink="" platform="" cost="0" risk="28" state="Recommended" owner="fscott-admin" library="CS-Default" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;The application should ensure users have undergone an Identification and Verification (ID&amp;amp;V) process before allowing access to secret, sensitive or otherwise restricted data. For less sensitive but still restricted data, simple verification of the location of the user may suffice (e.g. IP restrictions).&lt;/p&gt;&lt;ul&gt;&lt;li&gt;For non-sensitive but non-public data, access could be restricted by IP address, limiting access to internal networks, workstations, or gateways&lt;/li&gt;&lt;li&gt;For more sensitive data, TLS client-side certificates may be appropriate&lt;/li&gt;&lt;li&gt;Where secret or other sensitive data is handled, a full authentication process to identify and validate users with single or multi-factor authentication may be required&lt;/li&gt;&lt;/ul&gt;</desc>
          <implementations/>
          <references>
            <reference name="EU GDPR - EU Data Protection" url="https://eur-lex.europa.eu/legal-content/EN/TXT/?uri=CELEX:02016R0679-20160504"/>
            <reference name="OSA AC-14 Permitted Actions Without Identification Or Authentication" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/38-08_02_AC-14"/>
            <reference name="OSA IA-01 Identification And Authentication Policy And Procedures" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/181-08_02_IA-01"/>
            <reference name="OSA SI-04 Information System Monitoring Tools And Techniques" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/42-08_02_SI-04"/>
            <reference name="OSA SI-12 Information Output Handling And Retention" url="https://www.opensecurityarchitecture.org/cms/library/08_02_control-catalogue/50-08_02_SI-12"/>
            <reference name="OWASP Juice Shop: Broken Authentication" url="https://pwning.owasp-juice.shop/part2/broken-authentication.html"/>
            <reference name="[C3] OWASP Proactive Controls" url="https://www.owasp.org/index.php/OWASP_Proactive_Controls"/>
          </references>
          <standards>
            <standard ref="1.2.2" supportedStandardRef="owasp-asvs4-level-3"/>
            <standard ref="1.2.2" supportedStandardRef="owasp-asvs4-level-2"/>
            <standard ref="11.1.3" supportedStandardRef="owasp-asvs4-level-3"/>
            <standard ref="11.1.3" supportedStandardRef="owasp-asvs4-level-1"/>
            <standard ref="11.1.3" supportedStandardRef="owasp-asvs4-level-2"/>
            <standard ref="A07:2021-Identification and Authentication Failures" supportedStandardRef="owasp-top-10-2021"/>
            <standard ref="A2:2017-Broken Authentication" supportedStandardRef="owasp-top-10-2017"/>
            <standard ref="AC-22" supportedStandardRef="NIST 800-53"/>
            <standard ref="AC-22" supportedStandardRef="fedramp-moderate-baseline"/>
            <standard ref="AC-22" supportedStandardRef="fedramp-low-baseline"/>
            <standard ref="AC-22" supportedStandardRef="fedramp-high-baseline"/>
            <standard ref="Art.32" supportedStandardRef="EU-GDPR"/>
            <standard ref="cwe-top-25" supportedStandardRef="cwe-top-25-dangerous-weaknesses"/>
          </standards>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps>&lt;ol&gt;
                &lt;li&gt;Identify which parts of the service present sensitive data.&amp;nbsp;&lt;/li&gt;
                &lt;li&gt;Try to obtain access to this information without any type of authentication, for example, attempt to navigate directly to URLs that present sensitive data.&lt;/li&gt;
                &lt;/ol&gt;
            </steps>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:47.738">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="RESTRICT-SERVICE" name="Restrict access to the service at the network layer to reduce exposure" issueId="" issueLink="" platform="" cost="1" risk="56" state="Recommended" owner="fscott-admin" library="CS-Default" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Access to services should be restricted to expected sources, limiting the exposure of the service and its attack surface; and the likelihood of a malicious actor gaining access to the system.&amp;nbsp;&lt;/p&gt; 
&lt;p&gt;Apply network layer security controls so that only the necessary and expected IP addresses are permitted access to connect to the service.&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="OWASP Juice Shop: Broken Authentication" url="https://pwning.owasp-juice.shop/part2/broken-authentication.html"/>
            <reference name="OWASP Juice Shop: Insecure Deserialization" url="https://pwning.owasp-juice.shop/part2/insecure-deserialization.html"/>
          </references>
          <standards>
            <standard ref="1" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="10" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="17" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="18" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="19" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="2.4.7.18" supportedStandardRef="iotsf-class-1"/>
            <standard ref="2.4.7.18" supportedStandardRef="iotsf-class-2"/>
            <standard ref="3" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="31" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="5" supportedStandardRef="csa-api-security-guidelines"/>
            <standard ref="A07:2021-Identification and Authentication Failures" supportedStandardRef="owasp-top-10-2021"/>
            <standard ref="A08:2021-Software and Data Integrity Failures" supportedStandardRef="owasp-top-10-2021"/>
            <standard ref="A2:2017-Broken Authentication" supportedStandardRef="owasp-top-10-2017"/>
            <standard ref="A8:2017-Insecure Deserialization" supportedStandardRef="owasp-top-10-2017"/>
            <standard ref="API1:2023-Broken Object Level Authorization" supportedStandardRef="owasp-api-security-top-10"/>
          </standards>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps>&lt;p&gt;&lt;/p&gt; 
&lt;ul&gt; 
 &lt;li&gt;Check that the network access to the service is only allowed for the components which need it.&lt;/li&gt; 
 &lt;li&gt;There should be a firewall protecting the network segment in which the service is installed.&lt;/li&gt; 
 &lt;li&gt;Check that the firewall restricts all access and only permits the minimum connections required using the principal of least privilege.&lt;/li&gt; 
&lt;/ul&gt; 
&lt;p&gt;&lt;/p&gt;</steps>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:47.740">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
      </countermeasures>
      <usecases>
        <usecase ref="ACCESS SERVICE" name="Access service" desc="" library="CS-Default">
          <threats>
            <threat ref="EU-GDPR-DATA_LEAKAGE-UNAUTHZ-PARTIES" name="Data leakage or disclosure to unauthorized parties" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="CS-Default" editable="true">
              <desc>&lt;p&gt;An unauthorized party might access/breach the personal data of a data subject.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="50" availability="75" easeOfExploitation="25"/>
              <references/>
              <weaknesses>
                <weakness ref="EU-GDPR-MISSING-ENCRYPTION-SENSITIVE-DATA">
                  <countermeasures>
                    <countermeasure ref="EU-GDPR-ENCRYPT-PERSONAL-DATA" mitigation="100"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="EU-GDPR-ENCRYPT-PERSONAL-DATA" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="CAPEC-115" name="Authentication Bypass" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="CS-Default" editable="true">
              <desc>&lt;p&gt;An attacker gains access to the application, service, or device with the privileges of an authorized or privileged user by evading - or circumventing - an authentication mechanism. The attacker is therefore able to access protected data without authentication ever having taken place. This refers to an attacker gaining access equivalent to an authenticated user without ever going through an authentication procedure. &lt;/p&gt; This is usually the result of the attacker using an unexpected access procedure that does not go through the proper checkpoints where authentication should occur. For example, a web site might assume that all users will click through a given link in order to access secure material and simply authenticate everyone that clicks the link. However, an attacker might be able to reach secured web content by explicitly entering the path to the content rather than clicking through the authentication link, thereby avoiding the check entirely. This attack pattern differs from other authentication attacks in that attacks of this pattern avoid authentication entirely, rather than faking authentication by exploiting flaws or by stealing credentials from legitimate users.</desc>
              <riskRating confidentiality="100" integrity="100" availability="100" easeOfExploitation="25"/>
              <references>
                <reference name="Mitre ATT&amp;CK Technique T1548.001" url="https://attack.mitre.org/techniques/T1548/001"/>
                <reference name="Mitre ATT&amp;CK Technique T1548.002" url="https://attack.mitre.org/techniques/T1548/002"/>
                <reference name="Mitre ATT&amp;CK Technique T1548.003" url="https://attack.mitre.org/techniques/T1548/003"/>
                <reference name="Mitre ATT&amp;CK Technique T1548.004" url="https://attack.mitre.org/techniques/T1548/004"/>
              </references>
              <weaknesses>
                <weakness ref="CWE-306">
                  <countermeasures>
                    <countermeasure ref="CWE-306-SERVICE" mitigation="100"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="CWE-306-SERVICE" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="VULN-SERVICE" name="Attackers gain access to unauthorised data by exploiting vulnerabilities in the service" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="CS-Default" editable="true">
              <desc>Attackers exploit vulnerabilities in the service and gain access to data, or to
                                services for which they are not authorized.
                            </desc>
              <riskRating confidentiality="100" integrity="100" availability="100" easeOfExploitation="100"/>
              <references>
                <reference name="Mitre ATT&amp;CK Technique T1190" url="https://attack.mitre.org/techniques/T1190"/>
              </references>
              <weaknesses>
                <weakness ref="CWE-284">
                  <countermeasures>
                    <countermeasure ref="PATCH-SERVICE" mitigation="50"/>
                    <countermeasure ref="RESTRICT-SERVICE" mitigation="50"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="PATCH-SERVICE" mitigation="50"/>
                <countermeasure ref="RESTRICT-SERVICE" mitigation="50"/>
              </countermeasures>
              <customFields/>
            </threat>
          </threats>
        </usecase>
        <usecase ref="authentication" name="Authentication" desc="" library="CS-Default">
          <threats>
            <threat ref="AUTH-DATASTORE-LEAST-PRIV" name="Attackers who compromise the application or application server could directly access and modify the data store" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="CS-Default" editable="true">
              <desc>If attackers gain access to the application or the application server, then they could
                                directly access the data store using the privilege assigned to the application.
                                &lt;div&gt;
                                If the data store user account used by the application has elevated privileges then this
                                could allow attackers to perform unauthorized operations such as dropping tables,
                                modifying the database schema or modifying data.
                                &lt;/div&gt;
                            </desc>
              <riskRating confidentiality="1" integrity="100" availability="100" easeOfExploitation="25"/>
              <references/>
              <weaknesses>
                <weakness ref="CWE-250">
                  <countermeasures>
                    <countermeasure ref="RESTRICT-ACCESS-DATABASE" mitigation="100"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="RESTRICT-ACCESS-DATABASE" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
          </threats>
        </usecase>
      </usecases>
    </component>
    <component uuid="cea3da2f-6814-4242-85ed-cb73efb432e1" diagramComponentId="bd3eee1b-37cb-4b9a-bb34-cd16a716588f" ref="a84f8551-e8e8-474c-9b0e-f8e39393dcc4" name="Raw Data Collection" desc="" library="IR-Machine-Learning-Artificial-Intelligence" parentComponentRef="" componentDefinitionRef="CD-ML-AI-RAW-DATA-COLLECTION">
      <tags/>
      <questions/>
      <trustZones>
        <trustZone ref="private-secured"/>
      </trustZones>
      <assets/>
      <settings/>
      <weaknesses>
        <weakness ref="CWE-922" name="Information Exposure Through Local Storage" state="0" impact="100" issueId="" issueLink="">
          <desc>The software stores sensitive information without properly limiting read or write access by
                        unauthorized actors.
                        If read access is not properly restricted, then attackers can steal the sensitive information.
                        If write access is not properly restricted, then attackers can modify and possibly delete the
                        data, causing incorrect results and possibly a denial of service.
                    </desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.277">
              <output/>
            </source>
            <references>
              <reference name="CWE-922: Insecure Storage of Sensitive Information" url="https://cwe.mitre.org/data/definitions/922.html"/>
            </references>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="CWE-311" name="Missing Encryption of Sensitive Data" state="0" impact="100" issueId="" issueLink="">
          <desc>The software does not encrypt sensitive or critical information
                        before storage or transmission.
                    </desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.276">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </weakness>
      </weaknesses>
      <countermeasures>
        <countermeasure ref="C-ML-AI-AVOID-DIRECT-LOOPING" name="Avoid direct loops in the data pipeline" issueId="" issueLink="" platform="" cost="0" risk="46" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;A beneficial feedback loop typically involves bringing in unbiased, external information into your system, which would ensure that, over time, the models do not degrade due to being retrained on biased data that is strongly correlated to the outputs and does not reflect the true data distribution.&lt;/p&gt;
&lt;p&gt;Human-in-the-loop, or human-on-the-loop (only holding overall control over the system), are generally good ideas in ML/AI systems.&amp;nbsp;&amp;nbsp;&lt;br&gt;&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Human-in-the-Loop in Machine Learning: A Handful of Arguments in Favor" url="https://labelyourdata.com/articles/human-in-the-loop-in-machine-learning"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.194">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-REDUNDANCY" name="Build and maintain redundant data streams" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Avoid central points of failure or compromise for all critical components of your system including the data stream.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Internally, this involves network pathway, power, geographic, and data redundancy, and can even be extended to individuals, i.e., key staff looking after the data pipeline!&lt;/p&gt;
&lt;p&gt;In addition, when possible, avoid 'vendor lock-in' and consider alternative data sources.&lt;br&gt;&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references/>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.196">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-PROTECT-SOURCES" name="Carefully select and protect data sources when possible" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;When selecting data sources, their security (availability and integrity, and possibly confidentiality) is of paramount importance. This involves:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Analyze the source, e.g., reputation,&lt;/li&gt;
 &lt;li&gt;Evaluate the data quality,&lt;/li&gt;
 &lt;li&gt;Verify the data formats and accessibility options,&lt;/li&gt;
 &lt;li&gt;Verify the reliability of the data, including in terms of their security aspects, as well as legitimacy and biases,&lt;/li&gt;
 &lt;li&gt;Verify the security of the data source, e.g., compliance with security standards and regulations.&lt;/li&gt;
&lt;/ul&gt;</desc>
          <implementations/>
          <references>
            <reference name="How to find reliable data sources" url="https://earlygrowthfinancialservices.com/blog/how-to-find-reliable-data-sources/"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.196">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-MODEL-SELECTION" name="Consider model selection techniques for more reliable and reproducible inference and predictions" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Model selection in ML/AI is the process of selecting the best algorithm and model architecture for a specific job and/or data. It entails assessing and contrasting various models to identify the one that best fits the data at hand and produces the best results.&lt;/p&gt;
&lt;p&gt;This involves some testing with modeling, as opposed to only relying on the characteristics of the data itself. Selection techniques include model performance comparisons with measurements such as the F-score, accuracy, precision, recall, or the area beneath the receiver's operating characteristic curve (AUC-ROC). Several model performance and validation strategies are used to assess each model's generalization, including sampling and splitting techniques.&lt;/p&gt;
&lt;p&gt;Note that the selection step is a must even though it increases the computational cost and time, such as cross-validation for instance, which requires training and testing multiple times with varied parameters.&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Model selection for machine learning" url="https://www.scholarhat.com/tutorial/machinelearning/model-selection-for-machine-learning"/>
            <reference name="The Ultimate Guide to Evaluation and Selection of Models in Machine Learning" url="https://neptune.ai/blog/ml-model-evaluation-and-selection"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.196">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-DATA-VERSION-CONTROL" name="Consider using version control to manage datasets" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Data versioning should be treated in the same manner as code versioning.&lt;/p&gt;
&lt;p&gt;There exist several data version control libraries such as DVC which is dedicated to ML/AI data and systems or lakeFS which provides a Git-like functionality for data using S3 or Google Cloud Storage (GCS).&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="(Not Just) Data Version Control" url="https://dvc.org/"/>
            <reference name="Best 7 Data Version Control Tools That Improve Your Workflow With Machine Learning Projects" url="https://neptune.ai/blog/best-data-version-control-tools"/>
            <reference name="Model and Data Versioning" url="https://github.com/EthicalML/awesome-production-machine-learning#model-and-data-versioning"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.195">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-CONSISTENT-REPRESENTATION" name="Reviews of data representations, periodic validation, as well as future featurization options for the data being collected so that it is consistently represented" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Experimentations in the literature have revealed a strong correlation between data consistency and the performance of the ML/AI model. The following is helpful to maintain a consistent representation of the data throughout the ML/AI system lifecycle:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Measure the consistency of the labels assigned to similar items,&lt;/li&gt;
 &lt;li&gt;Detect issues such as low-quality/ambiguous examples or non-IID (independent and identically distributed) sampling,&lt;/li&gt;
 &lt;li&gt;Periodic automated validation to detect labeling errors, duplicates, outliers, etc.,&lt;/li&gt;
 &lt;li&gt;Account for possible changes in the data and alternatives on how to featurize it, and&lt;/li&gt;
 &lt;li&gt;Ensure data integrity while processing and migrating the data, over the network or into a new schema, for instance.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;There exist tools around aspects like labeling or validation that can help further automate and speed up the process. Additionaly, consider the use of frameworks for creating portable and production-ready ML/AI pipelines.&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Argilla - MLOps for NLP: from data labeling to model monitoring" url="https://github.com/argilla-io/argilla"/>
            <reference name="Cleanlab: clean data and labels by automatically detecting issues in a ML dataset" url="https://github.com/cleanlab/cleanlab"/>
            <reference name="Model Training Orchestration" url="https://github.com/EthicalML/awesome-production-machine-learning#model-training-orchestration"/>
            <reference name="TensorFlow Data Validation" url="https://github.com/tensorflow/data-validation"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.195">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-SECURE-STORAGE" name="Secure data storage" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Take measures to protect storage resources and the data stored on them, both on-premises and externally, e.g., on the cloud. This involves:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Access control to regulate who can use the data. The environment should identify, verify, and authorize a user before they access the data. Additional policies can dictate where the data can be copied and under which conditions.&amp;nbsp;&lt;/li&gt;
 &lt;li&gt;Storage encryption, which can be built in by the service provider. Otherwise, on-premise encryption should be considered.&lt;/li&gt;
 &lt;li&gt;Isolate data from other systems and networks.&lt;/li&gt;
 &lt;li&gt;Immutable storage as an extra protection layer that can help prevent bad actors from tampering with data.&lt;br&gt;&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;</desc>
          <implementations/>
          <references/>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.196">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-SECURE-TRANSFER" name="Secure data transfer" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;When data transfer is needed, various secure methods can be employed, including:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;SSH&lt;/li&gt;
 &lt;li&gt;SSL/TLS&lt;/li&gt;
 &lt;li&gt;VPN&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Also, do not forget to have a backup of your data before starting any migration process.&lt;/p&gt;</desc>
          <implementations/>
          <references/>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.196">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-VALIDATE-DATA" name="Validate data algorithmically before feeding it into the data pipeline" issueId="" issueLink="" platform="" cost="0" risk="56" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Maintaining data quality is an essential practice when developing ML/AI models.&lt;/p&gt;
&lt;p&gt;Best practices for monitoring and validating the data include:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Consistent scale across the data variables,&lt;/li&gt;
 &lt;li&gt;Data balancing to remove bias,&lt;/li&gt;
 &lt;li&gt;Data sensitivity tests,&lt;/li&gt;
 &lt;li&gt;Outliers recognition,&lt;/li&gt;
 &lt;li&gt;Common statistical checks such as data sparseness or variance, etc. including for assessing the data representativeness for the target application,&lt;/li&gt;
 &lt;li&gt;Checking against the restrictions as to how the acquired data can be used,&lt;/li&gt;
 &lt;li&gt;Others, such as the validity of characters in text, or the correlation between variables, etc.&amp;nbsp;&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;These might also depend on the domain area and target modeling.&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Data Validation Tool" url="https://github.com/GoogleCloudPlatform/professional-services-data-validator"/>
            <reference name="TensorFlow Data Validation" url="https://github.com/tensorflow/data-validation"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:49.197">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
      </countermeasures>
      <usecases>
        <usecase ref="general" name="General" desc="" library="IR-Machine-Learning-Artificial-Intelligence">
          <threats>
            <threat ref="T-ML-AI-DISABLED-DATA-SOURCES" name="An adversary may target and disable data sources that the system depends on" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Beyond data integrity, confidentiality, and quality; availability is of critical importance, especially when it comes to public data sources.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Data sources, which might be used both during training and post-deployment, can be vulnerable to attacks, including denial of service type of attacks.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="50" availability="100" easeOfExploitation="75"/>
              <references>
                <reference name="Understanding Denial-of-Service Attacks" url="https://www.us-cert.gov/ncas/tips/ST04-015"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-REDUNDANCY" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-FEATURE-ENGINEERING" name="Consider how feature engineering and data representations and encodings might impact the future system output" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Data transformation into features, i.e., in a form that can be easily digested and interpreted by the ML/AI algorithm, is a key aspect in building performant and secure models.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;This involves a number of steps, including feature extraction and encoding, and a number of transformations such as dealing with missing values. Care must be taken early on to build a featurization pipeline that is likely to result in more robust systems.&lt;/p&gt;</desc>
              <riskRating confidentiality="75" integrity="100" availability="50" easeOfExploitation="100"/>
              <references>
                <reference name="Feature Engineering" url="https://www.kaggle.com/learn/feature-engineering"/>
                <reference name="Robust Feature Alignment (an example from image classification)" url="https://github.com/safreita1/unmask"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-CONSISTENT-REPRESENTATION" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-INSECURE-STORE-TRANSFER" name="Data confidentiality and integrity can be jeopardized by an adversary taking advantage of data that is insecurely stored or managed" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Data sources that are not sufficiently protected might be manipulated or poisoned, whether at rest or during transfers.&lt;/p&gt;</desc>
              <riskRating confidentiality="100" integrity="100" availability="25" easeOfExploitation="75"/>
              <references/>
              <weaknesses>
                <weakness ref="CWE-311">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-SECURE-TRANSFER" mitigation="50"/>
                  </countermeasures>
                </weakness>
                <weakness ref="CWE-922">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-SECURE-STORAGE" mitigation="50"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="C-ML-AI-SECURE-STORAGE" mitigation="50"/>
                <countermeasure ref="C-ML-AI-SECURE-TRANSFER" mitigation="50"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-LACK-VERSION-CONTROL" name="Insufficient or lack of version control of the data can disrupt the system and lead to inconsistencies and failures" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Insufficient or lack of version control carries multiple risks including working from the wrong data, losing previous states and creating rework, &amp;nbsp;and increasing the risk of inconsistencies, errors, or even total failures of the system without the possibility of rolling back to older data/versions.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="100" availability="100" easeOfExploitation="100"/>
              <references>
                <reference name="Why Use a Version Control System?" url="https://www.git-tower.com/learn/git/ebook/en/desktop-gui/basics/why-use-version-control"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-DATA-VERSION-CONTROL" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-LOOPING" name="For future online systems, they may be exposed to using their own output as input data" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;There may be unexpected behavior if data output from the future model is directly used as input, which is referred to as looping.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="75" availability="50" easeOfExploitation="75"/>
              <references/>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-AVOID-DIRECT-LOOPING" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-RAW-DATA-PROPERTIES" name="Raw data properties affect the design, selection, and outputs of ML/AI models" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Have you properly vetted the data being considered, including how it matches the algorithm under consideration? Data properties and quality should be a major consideration in order to reduce the impact of 'garbage in, garbage out'. Is the data fit to serve its purpose in the context of future modeling, i.e., the considered ML/AI technique?&lt;/p&gt;
&lt;p&gt;'Summarized' data via its statistical characteristics, as well as visualization, can be useful to select appropriate cleaning, feature processing, and modeling techniques.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="100" availability="50" easeOfExploitation="75"/>
              <references/>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-MODEL-SELECTION" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-RAW-POISONING" name="Data sources may be unreliable due to data poisoning" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Data poisoning is one of the main concerns in adversarial ML/AI. It involves the intentional supply of wrong or misleading data to impact the quality of ML/AI models. The manipulation of the data is aimed at controlling the behavior of the model and delivering false results.&lt;/p&gt;</desc>
              <riskRating confidentiality="50" integrity="100" availability="50" easeOfExploitation="100"/>
              <references>
                <reference name="ML02:2023 Data Poisoning Attack" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML02_2023-Data_Poisoning_Attack.html"/>
              </references>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-PROTECT-SOURCES" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-UNRELIABLE-SOURCES" name="Data sources may be biased, unbalanced, not representative or properly encoded, or even include copyrighted or illegal material" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Multiple issues can occur with raw input data, including:&amp;nbsp;&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Bias, which can have ethical and moral implications,&lt;/li&gt;
 &lt;li&gt;Imbalance, which reflects unequal distributions of classes within the dataset and will likely result in unstable models and poor performance (biased towards the overrepresented class),&lt;/li&gt;
 &lt;li&gt;Encoding and transformation, which might result in information loss, errors, or inconsistencies,&lt;/li&gt;
 &lt;li&gt;Representativity, which might not have proper coverage or distribution of the target population, and&lt;/li&gt;
 &lt;li&gt;Legality, such as copyrighted material or non-compliance with regulation, e.g., GDPR.&lt;/li&gt;
&lt;/ul&gt;</desc>
              <riskRating confidentiality="50" integrity="100" availability="50" easeOfExploitation="100"/>
              <references/>
              <weaknesses/>
              <countermeasures>
                <countermeasure ref="C-ML-AI-VALIDATE-DATA" mitigation="100"/>
              </countermeasures>
              <customFields/>
            </threat>
          </threats>
        </usecase>
      </usecases>
    </component>
    <component uuid="0c50a029-aa98-4c49-bae9-efbb94747db1" diagramComponentId="09f3011d-d0ef-4690-8b9a-dca5132e3d21" ref="6d8c6a95-3951-4011-b931-fffdf5f1923b" name="Trained Model" desc="" library="IR-Machine-Learning-Artificial-Intelligence" parentComponentRef="" componentDefinitionRef="CD-ML-AI-TRAINED-MODEL">
      <tags/>
      <questions/>
      <trustZones>
        <trustZone ref="private-secured"/>
      </trustZones>
      <assets/>
      <settings/>
      <weaknesses>
        <weakness ref="CWE-506" name="Embedded Malicious Code" state="0" impact="100" issueId="" issueLink="">
          <desc>The application contains code that appears to be malicious in
                        nature.
                    </desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.394">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="CWE-359" name="Exposure of Private Information ('Privacy Violation')" state="0" impact="100" issueId="" issueLink="">
          <desc>The software does not properly prevent private data (such as credit card numbers) from being accessed by actors who either (1) are not explicitly authorized to access the data or (2) do not have the implicit consent of the people to which the data is related.</desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.394">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </weakness>
        <weakness ref="CWE-200" name="Information Exposure" state="0" impact="100" issueId="" issueLink="">
          <desc>An information exposure is the intentional or unintentional disclosure of information to an
                        actor that is not explicitly authorized to have access to that information.
                    </desc>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.393">
              <output/>
            </source>
            <references>
              <reference name="CWE-200: Information Exposure" url="https://cwe.mitre.org/data/definitions/200.html"/>
            </references>
            <customFields/>
          </test>
        </weakness>
      </weaknesses>
      <countermeasures>
        <countermeasure ref="C-ML-AI-MODEL-CHOICE" name="Carefully consider the model choice" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Models can be built using different approaches or algorithms. Overall, choosing the right algorithm depends on several factors, including business considerations, data size, accuracy, training time, parameters, and inference performance, among others.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;In general, the right choice is typically a combination of these aspects. In terms of security, however, the main algorithmic concern is about the nature of the data, where certain algorithms are more vulnerable to data leaks than others.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Depending on your privacy concerns, and/or compliance considerations, e.g., GPDR, &amp;nbsp;consider auditing the data privacy of the models under consideration, e.g., using tools like Privacy Meter.&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Privacy Meter" url="https://github.com/privacytrustlab/ml_privacy_meter"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.259">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-TROJANIZED-VERSIONS" name="Consider possible trojanized versions when acquiring shared models" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Never assume what you are downloading is safe! Malicious actors often use trojanized versions of popular applications in an effort to steal information and compromise systems.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;In this case, you might be fine-tuning a model with possibly a Trojan that includes sneaky behavior that is unanticipated.&amp;nbsp;&lt;/p&gt;
&lt;p&gt;Basic practices include carefully checking the source of the model, as well as using cryptographic methods like digital signatures and secure hashes to verify the authenticity and file integrity.&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="How to Verify Downloaded Files" url="https://www.digitalocean.com/community/tutorials/how-to-verify-downloaded-files"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.260">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-PRIVACY-PRESERVING" name="Consider privacy-preserving techniques and strategies" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Privacy-preserving or privacy-enhancing techniques are designed to prevent the exposure of the model and data.&lt;/p&gt;
&lt;p&gt;First, there are two types of access an adversary might have; 1) White Box, where information about the model or its original training data is available, and 2) Black Box, where there is no knowledge about the model or data. In this case, attackers would explore the model by providing carefully crafted inputs and observing outputs.&lt;/p&gt;
&lt;p&gt;There are a number of attack types, which involve inferring information about the model and data, e.g., model inversion or membership inference.&lt;/p&gt;
&lt;p&gt;Some of the defenses aiming to protect confidentiality and privacy in ML/AI-based systems are:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Differential Privacy: one of the state-of-the-art methods for providing access to information free from inferences. There are two main privacy-preserving model-training approaches in the literature: 1) using noisy stochastic gradient descent (noisy SGD), and 2) Private Aggregation of Teacher Ensembles (PATE).&lt;/li&gt;
 &lt;li&gt;Cryptography; e.g., privacy-enhancing tools based on secure multi-party computation (SMC) and fully homomorphic encryption (FHE) have been proposed to securely train supervised machine learning models.&lt;/li&gt;
 &lt;li&gt;Trusted environments (engineering/setup solution).&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;Other techniques in this space include dropout, weight normalization, dimensionality reduction, selective gradient sharing, etc.&lt;br&gt;&amp;nbsp;&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="An Overview of Privacy in Machine Learning" url="https://arxiv.org/pdf/2005.08679.pdf"/>
            <reference name="ML03:2023 Model Inversion Attack" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML03_2023-Model_Inversion_Attack.html"/>
            <reference name="ML04:2023 Membership Inference Attack" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML04_2023-Membership_Inference_Attack.html"/>
            <reference name="ML05:2023 Model Stealing" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML05_2023-Model_Stealing.html"/>
            <reference name="Privacy Preserving ML" url="https://github.com/EthicalML/awesome-production-machine-learning#privacy-preserving-ml"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.259">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-KEEP-LOGS" name="Keep a history of queries to the model" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Keeping a history of queries to the system is highly recommended. Logging user interaction with the model is not only a powerful tool for user research (behavior study, usability, system metrics analysis, etc.) but will also allow reviews to make sure the system is not unintentionally leaking confidential information.&amp;nbsp;&lt;br&gt;Care must be taken on how you store the logs! Logging can itself introduce a security/privacy risk.&lt;/p&gt;</desc>
          <implementations/>
          <references>
            <reference name="Top seven logging and monitoring best practices" url="https://www.synopsys.com/blogs/software-security/logging-and-monitoring-best-practices/"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.258">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
        <countermeasure ref="C-ML-AI-PROTECT-SHARED" name="Protect data and IP when sharing or shipping models" issueId="" issueLink="" platform="" cost="0" risk="49" state="Recommended" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" source="RULES" edited="false" stateChangeSource="" priority="">
          <desc>&lt;p&gt;Models sharing or shipping can be treated the same as software delivery and protection:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Consider training data as traditional source code.&lt;/li&gt;
 &lt;li&gt;Consider the model files as compiled executables.&lt;/li&gt;
&lt;/ul&gt;
&lt;p&gt;A number of mechanisms/options can be used, including:&lt;/p&gt;
&lt;ul&gt;
 &lt;li&gt;Encryption, e.g., for on-device models, only unpacking in memory when the model is running,&lt;/li&gt;
 &lt;li&gt;Obfuscation, e.g., adding random noise to existing samples, or augmenting the dataset with new samples,&amp;nbsp;&lt;/li&gt;
 &lt;li&gt;Copyright traps, or&lt;/li&gt;
 &lt;li&gt;Confidential computing, e.g., built-in hardware-based security.&lt;/li&gt;
&lt;/ul&gt;</desc>
          <implementations/>
          <references>
            <reference name="ML Model Watermarking" url="https://github.com/SAP/ml-model-watermarking"/>
          </references>
          <standards/>
          <customFields/>
          <test expiryDate="" expiryPeriod="0">
            <steps/>
            <notes/>
            <source filename="" args="" type="Manual" result="Not Tested" enabled="true" timestamp="2024-04-09T13:26:46.260">
              <output/>
            </source>
            <references/>
            <customFields/>
          </test>
        </countermeasure>
      </countermeasures>
      <usecases>
        <usecase ref="general" name="General" desc="" library="IR-Machine-Learning-Artificial-Intelligence">
          <threats>
            <threat ref="T-ML-AI-PRIVACY" name="An adversary may be able to reveal sensitive information available in the model or the data used to build it" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;ML/AI models incorporate information and representations of the data ingested during training, some of which might be sensitive.&lt;br&gt;Algorithm choice might be a key factor to limit the risk, e.g., using a non-parametric method like k-nearest neighbors in a situation with sensitive medical records is probably a bad idea since exemplars will have to be stored on production hosts.&amp;nbsp;&lt;br&gt;In general, however, remember that any model is susceptible to memorizing information present within the training sets.&amp;nbsp;&lt;br&gt;You should also be aware of what outputs the model is producing and how it may reveal sensitive aspects of the training data, e.g., using monitoring.&lt;/p&gt;</desc>
              <riskRating confidentiality="100" integrity="50" availability="50" easeOfExploitation="75"/>
              <references>
                <reference name="An Overview of Privacy in Machine Learning" url="https://arxiv.org/pdf/2005.08679.pdf"/>
              </references>
              <weaknesses>
                <weakness ref="CWE-359">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-KEEP-LOGS" mitigation="33"/>
                    <countermeasure ref="C-ML-AI-MODEL-CHOICE" mitigation="33"/>
                    <countermeasure ref="C-ML-AI-PRIVACY-PRESERVING" mitigation="34"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="C-ML-AI-KEEP-LOGS" mitigation="33"/>
                <countermeasure ref="C-ML-AI-MODEL-CHOICE" mitigation="33"/>
                <countermeasure ref="C-ML-AI-PRIVACY-PRESERVING" mitigation="34"/>
              </countermeasures>
              <customFields/>
            </threat>
            <threat ref="T-ML-AI-SHARING-TRANSFER" name="An adversary may take advantage of model sharing and/or transfer" state="Expose" source="RULES" edited="false" owner="fscott-admin" library="IR-Machine-Learning-Artificial-Intelligence" editable="true">
              <desc>&lt;p&gt;Models storage and transfer may lead to the possibility that what is being reused may be a Trojaned (or otherwise damaged) version of the model. This risk is more pronounced in the case of using ready-to-use models.&lt;/p&gt;</desc>
              <riskRating confidentiality="100" integrity="100" availability="50" easeOfExploitation="75"/>
              <references>
                <reference name="ML06:2023 Corrupted Packages" url="https://owasp.org/www-project-machine-learning-security-top-10/docs/ML06_2023-Corrupted_Packages.html"/>
              </references>
              <weaknesses>
                <weakness ref="CWE-200">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-PROTECT-SHARED" mitigation="50"/>
                  </countermeasures>
                </weakness>
                <weakness ref="CWE-506">
                  <countermeasures>
                    <countermeasure ref="C-ML-AI-TROJANIZED-VERSIONS" mitigation="50"/>
                  </countermeasures>
                </weakness>
              </weaknesses>
              <countermeasures>
                <countermeasure ref="C-ML-AI-PROTECT-SHARED" mitigation="50"/>
                <countermeasure ref="C-ML-AI-TROJANIZED-VERSIONS" mitigation="50"/>
              </countermeasures>
              <customFields/>
            </threat>
          </threats>
        </usecase>
      </usecases>
    </component>
  </components>
</template>
